{
    "docs": [
        {
            "location": "/",
            "text": "Essential skills for reproducible research computing\n\n\nA four-day, intensive, hands-on workshop on the foundational skills that everyone using computers in the pursuit of scientific research should have.\n\n\nOffered for the first time 3\u20136 January 2017, at Universidad T\u00e9cnica Federico Santa Mar\u00eda, Valpara\u00edso, Chile.\n\n\nInstructors:\n Lorena A. Barba, Natalia C. Clementi, Gilbert F. Forsyth\n\n\nA \nrendered course website\n is also available.\n\n\n\n\nEvery student today (graduate or undergraduate) is using computers on a daily basis.\nBut to be proficient with computers as a tool for science, you need professional-grade skills.\nThis workshop teaches the computing skills that we\u2014as a research group in computational science and engineering\u2014think everyone should have.\n\n\nIn recent years, the concern for reproducibility in computational science has gained traction.\nOur research group has been pushing for several years the adoption of better standards for reproducible research.\nThese standards include treating scientific software as a core intellectual product, adding automation to our data handling and analysis, and the open sharing of our digital objects.\n\n\nThis course provides an introduction to the tools and techniques that we consider fundamental for responsible use of computers in scientific research. They include the following:\n\n\n\n\ncommand line utilities in Unix/Linux \n\n\nan open-source scientific software ecosystem (our favorite is Python's)\n\n\nsoftware version control (we advocate the distributed kind: our favorite is git)\n\n\ngood practices for scientific software development: code hygiene and testing\n\n\nknowledge of licensing options for sharing software \n\n\n\n\nReproducible Computational Research\n\n\nThe weeklong workshop also includes a presentation and discussion of the \n\"Barba-group Reproducibility Syllabus\"\n\u2014an annotated bibliography of our Top-10 readings on the topic of reproducibility.\nNumber 7 in the list of readings is an article titled \"Ten simple rules for reproducible research\" (Sandve et al., 2013).\nTwo unifying ideas run through the \"ten simple rules\":\n(1) that automation is a key device for reproducibility, and\n(2) that version control is the core technology for dealing with software as a living, changing thing.\nThese ideas justify insisting that \ncommand-line skills are a must.\n\n\nThe skeleton for our practice of reproducible research is the pledge \n\"Reproducibility PI Manifesto\"\n (2012).\nIt consists of a commitment to:\n(1) teach group members about reproducibility; \n(2) keep all code and writing under version-control; \n(3) complete code verification and validation, and publish openly the results; \n(4) for main results in a publication, share data, plotting scripts, and figures under CC-BY; \n(5) upload preprints to \narXiv\n at the time of submission of a paper; \n(6) release code no later than the time of submission of a paper; \n(7) add a \"Reproducibility\" statement to each publication; \n(8) keep an up-to-date web presence.\n\n\nWith this workshop, we propel the first commitment beyond our research group: \nwe take responsibility for not only teaching our group members, but broadly disseminating the know-how to our community. \nThe second commitment helps for what University of Washington professor Randall LeVeque\n1\n called \n\"private reproducibility\"\n: \nwe can rebuild our own past research results from the precise version of the code that was used to create them.\nPrivate reproducibility also demands fully automating the analysis and visualization of data.\nStanford professor Jon Claerbout\n2\n said:\n\n\u201cI\u2019ve learned that interactive programs are slavery (unless they include the ability to arrive in any previous state by means of a script).\u201d\n\nWith this in mind, two technologies are enemies of reproducible research:\nGUI-based image manipulation, and spreadsheets.\nFigures that visualize data, or image processing applied to photographs, can only be reproducible if made with scripts.\nAnd spreadsheets impair reproducibility because they conflate input, output, code and presentation\u2014as noted by Berkeley professor Philip Stark.\n3\n\nThis situation calls for adopting a scientific software stack for programmatic analysis and visualization.\nOur favorite environment to accomplish this is Scientific Python.\n\n\nGenuine reproducible research is not only privately reproducible, but publicly so.\nThat's why all our subsequent reproducibility commitments deal with open access to data, code and publications.\nThe American Physical Society issued a policy statement under the Ethics and Values category, titled \n\"What is Science?\"\n (1999). \nIt reads:\n\n\"The success and credibility of science are anchored in the willingness of scientists to [\u2026] Expose their ideas and results to independent testing and replication by others. This requires the open exchange of data, procedures and materials.\"\n\nIn computational research, we and many others maintain that this tenet calls for open-source software, and open data.\n\n\nOpen-source software and open data\n\n\nThe Yale Law School Roundtable on Data and Code Sharing\n4\n (November 2009) made an unambiguous statement urging for more transparency in computational sciences. \nTheir recommendations include: assigning a unique identifier to every version of the data and code, describing within each publication the computing environment used, using open licenses and non-proprietary formats, and publishing under open-access conditions (or posting pre-prints).\n\n\nSubscribing to the recommendations of the Yale roundtable means we need to learn about software licensing and data management.\nThis workshop will present and discuss the essential material and tools that open scientists need command of.\n\n\nDefinitions\n\n\nReproducible research:\n  Authors provide all the necessary data and the computer codes to run the analysis again, re-creating the results.\n\n\nReplication:\n Arriving at the same scientific findings as another study, collecting new data (possibly with different methods) and completing new analyses. A full replication study is sometimes impossible to do, but reproducible research is only limited by the time and effort we are willing to invest.\n\n\nReferences\n\n\n1\n LeVeque, Randall J. (2012), \"Issues in Reproducibility,\" talk given at the ICERM Workshop on Reproducibility in Computational and Experimental Mathematics, Brown University; \nslides PDF\n.\n\n\n2\n Claerbout, Jon; cited in Fomel, S. and Claerbout, J.F. (2009), Guest Editors' Introduction: Reproducible Research, \nComputing in Science and Engineering\n, Vol. 11(1):5\u20137, \ndoi:10.1109/MCSE.2009.14\n\n\n3\n Stark, Philip B. (2016), \"A noob's guide to reproducibility and open science,\" talk given at the Nuclear Engineering/BIDS/BITSS joint seminar, UC Berkeley, 25 January 2016; \nslides HTML\n and \ntalk video\n.\n\n\n4\n CiSE Reproducible Research By the Yale Law School Roundtable on Data and Code Sharing, \nComputing in Science and Engineering\n, Vol. 12(5): 8\u201313 (Sept.-Oct. 2010), \ndoi:10.1109/mcse.2010.113",
            "title": "Home"
        },
        {
            "location": "/#essential-skills-for-reproducible-research-computing",
            "text": "A four-day, intensive, hands-on workshop on the foundational skills that everyone using computers in the pursuit of scientific research should have.  Offered for the first time 3\u20136 January 2017, at Universidad T\u00e9cnica Federico Santa Mar\u00eda, Valpara\u00edso, Chile.  Instructors:  Lorena A. Barba, Natalia C. Clementi, Gilbert F. Forsyth  A  rendered course website  is also available.   Every student today (graduate or undergraduate) is using computers on a daily basis.\nBut to be proficient with computers as a tool for science, you need professional-grade skills.\nThis workshop teaches the computing skills that we\u2014as a research group in computational science and engineering\u2014think everyone should have.  In recent years, the concern for reproducibility in computational science has gained traction.\nOur research group has been pushing for several years the adoption of better standards for reproducible research.\nThese standards include treating scientific software as a core intellectual product, adding automation to our data handling and analysis, and the open sharing of our digital objects.  This course provides an introduction to the tools and techniques that we consider fundamental for responsible use of computers in scientific research. They include the following:   command line utilities in Unix/Linux   an open-source scientific software ecosystem (our favorite is Python's)  software version control (we advocate the distributed kind: our favorite is git)  good practices for scientific software development: code hygiene and testing  knowledge of licensing options for sharing software",
            "title": "Essential skills for reproducible research computing"
        },
        {
            "location": "/#reproducible-computational-research",
            "text": "The weeklong workshop also includes a presentation and discussion of the  \"Barba-group Reproducibility Syllabus\" \u2014an annotated bibliography of our Top-10 readings on the topic of reproducibility.\nNumber 7 in the list of readings is an article titled \"Ten simple rules for reproducible research\" (Sandve et al., 2013).\nTwo unifying ideas run through the \"ten simple rules\":\n(1) that automation is a key device for reproducibility, and\n(2) that version control is the core technology for dealing with software as a living, changing thing.\nThese ideas justify insisting that  command-line skills are a must.  The skeleton for our practice of reproducible research is the pledge  \"Reproducibility PI Manifesto\"  (2012).\nIt consists of a commitment to:\n(1) teach group members about reproducibility; \n(2) keep all code and writing under version-control; \n(3) complete code verification and validation, and publish openly the results; \n(4) for main results in a publication, share data, plotting scripts, and figures under CC-BY; \n(5) upload preprints to  arXiv  at the time of submission of a paper; \n(6) release code no later than the time of submission of a paper; \n(7) add a \"Reproducibility\" statement to each publication; \n(8) keep an up-to-date web presence.  With this workshop, we propel the first commitment beyond our research group: \nwe take responsibility for not only teaching our group members, but broadly disseminating the know-how to our community. \nThe second commitment helps for what University of Washington professor Randall LeVeque 1  called  \"private reproducibility\" : \nwe can rebuild our own past research results from the precise version of the code that was used to create them.\nPrivate reproducibility also demands fully automating the analysis and visualization of data.\nStanford professor Jon Claerbout 2  said: \u201cI\u2019ve learned that interactive programs are slavery (unless they include the ability to arrive in any previous state by means of a script).\u201d \nWith this in mind, two technologies are enemies of reproducible research:\nGUI-based image manipulation, and spreadsheets.\nFigures that visualize data, or image processing applied to photographs, can only be reproducible if made with scripts.\nAnd spreadsheets impair reproducibility because they conflate input, output, code and presentation\u2014as noted by Berkeley professor Philip Stark. 3 \nThis situation calls for adopting a scientific software stack for programmatic analysis and visualization.\nOur favorite environment to accomplish this is Scientific Python.  Genuine reproducible research is not only privately reproducible, but publicly so.\nThat's why all our subsequent reproducibility commitments deal with open access to data, code and publications.\nThe American Physical Society issued a policy statement under the Ethics and Values category, titled  \"What is Science?\"  (1999). \nIt reads: \"The success and credibility of science are anchored in the willingness of scientists to [\u2026] Expose their ideas and results to independent testing and replication by others. This requires the open exchange of data, procedures and materials.\" \nIn computational research, we and many others maintain that this tenet calls for open-source software, and open data.",
            "title": "Reproducible Computational Research"
        },
        {
            "location": "/#open-source-software-and-open-data",
            "text": "The Yale Law School Roundtable on Data and Code Sharing 4  (November 2009) made an unambiguous statement urging for more transparency in computational sciences. \nTheir recommendations include: assigning a unique identifier to every version of the data and code, describing within each publication the computing environment used, using open licenses and non-proprietary formats, and publishing under open-access conditions (or posting pre-prints).  Subscribing to the recommendations of the Yale roundtable means we need to learn about software licensing and data management.\nThis workshop will present and discuss the essential material and tools that open scientists need command of.",
            "title": "Open-source software and open data"
        },
        {
            "location": "/#definitions",
            "text": "Reproducible research:   Authors provide all the necessary data and the computer codes to run the analysis again, re-creating the results.  Replication:  Arriving at the same scientific findings as another study, collecting new data (possibly with different methods) and completing new analyses. A full replication study is sometimes impossible to do, but reproducible research is only limited by the time and effort we are willing to invest.",
            "title": "Definitions"
        },
        {
            "location": "/#references",
            "text": "1  LeVeque, Randall J. (2012), \"Issues in Reproducibility,\" talk given at the ICERM Workshop on Reproducibility in Computational and Experimental Mathematics, Brown University;  slides PDF .  2  Claerbout, Jon; cited in Fomel, S. and Claerbout, J.F. (2009), Guest Editors' Introduction: Reproducible Research,  Computing in Science and Engineering , Vol. 11(1):5\u20137,  doi:10.1109/MCSE.2009.14  3  Stark, Philip B. (2016), \"A noob's guide to reproducibility and open science,\" talk given at the Nuclear Engineering/BIDS/BITSS joint seminar, UC Berkeley, 25 January 2016;  slides HTML  and  talk video .  4  CiSE Reproducible Research By the Yale Law School Roundtable on Data and Code Sharing,  Computing in Science and Engineering , Vol. 12(5): 8\u201313 (Sept.-Oct. 2010),  doi:10.1109/mcse.2010.113",
            "title": "References"
        },
        {
            "location": "/nix/setup/",
            "text": "Workshop files\n\n\nBefore we start, you should download the following zip file:\n\n\nhttps://github.com/gforsyth/uber-trip-data/archive/master.zip\n\n\nDon't worry about waiting for it to finish, we just want to get the download\nstarted so that it's ready for later.\n\n\nOpen a terminal\n\n\nWe are going to be using the terminal A LOT. That's because the terminal is\nawesome.\n\n\nOn Ubuntu, you can open a terminal using the shortcut \nCtrl-Alt-T\n.\n\n\nOn OSX, open a finder window and browse to \nApplications->Utilities->Terminal\n\n(you can also download and install \niTerm2\n which is a\nreally nice terminal client written for OSX)\n\n\nIf you are using Windows, you will need to follow\nthe \ninstructions provided by Software Carpentry\n for configuring your machine\nto use UNIX-like tools.\n\n\nHistory\n\n\nFor almost all of this tutorial we will explain every command as we come across\nit with this one exception. For now, please copy and paste the following command\ninto your terminal -- we'll talk more about it later.\n\n\nexport HISTFILE=~/terminal_history_$(date -I)\n\n\n\n\nOk, on with the show!",
            "title": "Setup"
        },
        {
            "location": "/nix/setup/#workshop-files",
            "text": "Before we start, you should download the following zip file:  https://github.com/gforsyth/uber-trip-data/archive/master.zip  Don't worry about waiting for it to finish, we just want to get the download\nstarted so that it's ready for later.",
            "title": "Workshop files"
        },
        {
            "location": "/nix/setup/#open-a-terminal",
            "text": "We are going to be using the terminal A LOT. That's because the terminal is\nawesome.  On Ubuntu, you can open a terminal using the shortcut  Ctrl-Alt-T .  On OSX, open a finder window and browse to  Applications->Utilities->Terminal \n(you can also download and install  iTerm2  which is a\nreally nice terminal client written for OSX)  If you are using Windows, you will need to follow\nthe  instructions provided by Software Carpentry  for configuring your machine\nto use UNIX-like tools.",
            "title": "Open a terminal"
        },
        {
            "location": "/nix/setup/#history",
            "text": "For almost all of this tutorial we will explain every command as we come across\nit with this one exception. For now, please copy and paste the following command\ninto your terminal -- we'll talk more about it later.  export HISTFILE=~/terminal_history_$(date -I)  Ok, on with the show!",
            "title": "History"
        },
        {
            "location": "/nix/navigation/",
            "text": "Navigating the Command Line\n\n\nWelcome!\n\n\nThe command line can be a little intimidating at first, but it is a powerful and\nefficient way of interacting with your computer. It's also the lingua franca\nwhen dealing with computing clusters and remote machines.\n\n\nwhoami\n\n\nBefore we do anything else, let's figure out who we are. We can ask on the\ncommand line:\n\n\n$ whoami\n\n\n\n\ngil\n\n\n\n\nThat's my username!\n\n\npwd\n\n\nNow that we know who we are, time to figure out \nwhere\n we are.  To do that, we ask the terminal to \nprint working directory\n or \npwd\n\n\n$ pwd\n\n\n\n\n/home/gil/\n\n\n\n\nWe're in the \"home directory\" for our user. \n\n\nls\n\n\nLet's look around\n\n\n$ ls\n\n\n\n\nDesktop  Documents  Downloads  Music  Pictures  Public  Templates  Videos\n\n\n\n\nI think those are folders?  How can we tell? Use the \n-F\n flag\n\n\n$ ls -F\n\n\n\n\nDesktop/  Documents/  Downloads/  Music/  Pictures/  Public/  Templates/  Videos/\n\n\n\n\nThey all have a trailing slash, so they're all folders.\nWhat other options does \nls\n have?\n\n\n$ ls --help\n\n\n\n\nUsage: ls [OPTION]... [FILE]...\nList information about the FILEs (the current directory by default).\nSort entries alphabetically if none of -cftuvSUX nor --sort is specified.\n\nMandatory arguments to long options are mandatory for short options too.\n  -a, --all                  do not ignore entries starting with .\n  -A, --almost-all           do not list implied . and ..\n      --author               with -l, print the author of each file\n  -b, --escape               print C-style escapes for nongraphic characters\n      --block-size=SIZE      scale sizes by SIZE before printing them; e.g.,\n                               '--block-size=M' prints sizes in units of\n                               1,048,576 bytes; see SIZE format below\n  -B, --ignore-backups       do not list implied entries ending with ~\n  -c                         with -lt: sort by, and show, ctime (time of last\n                               modification of file status information);\n                               with -l: show ctime and sort by name;\n                               otherwise: sort by ctime, newest first\n  -C                         list entries by columns\n      --color[=WHEN]         colorize the output; WHEN can be 'always' (default\n                               if omitted), 'auto', or 'never'; more info below\n  -d, --directory            list directories themselves, not their contents\n  -D, --dired                generate output designed for Emacs' dired mode\n  -f                         do not sort, enable -aU, disable -ls --color\n  -F, --classify             append indicator (one of */=>@|) to entries\n      --file-type            likewise, except do not append '*'\n      --format=WORD          across -x, commas -m, horizontal -x, long -l,\n                               single-column -1, verbose -l, vertical -C\n      --full-time            like -l --time-style=full-iso\n  -g                         like -l, but do not list owner\n      --group-directories-first\n                             group directories before files;\n                               can be augmented with a --sort option, but any\n                               use of --sort=none (-U) disables grouping\n  -G, --no-group             in a long listing, don't print group names\n  -h, --human-readable       with -l and/or -s, print human readable sizes\n     recommonmark                          (e.g., 1K 234M 2G)\n      --si                   likewise, but use powers of 1000 not 1024\n  -H, --dereference-command-line\n                             follow symbolic links listed on the command line\n      --dereference-command-line-symlink-to-dir\n                             follow each command line symbolic link\n                               that points to a directory\n      --hide=PATTERN         do not list implied entries matching shell PATTERN\n                               (overridden by -a or -A)\n      --indicator-style=WORD  append indicator with style WORD to entry names:\n                               none (default), slash (-p),\n                               file-type (--file-type), classify (-F)\n  -i, --inode                print the index number of each file\n  -I, --ignore=PATTERN       do not list implied entries matching shell PATTERN\n  -k, --kibibytes            default to 1024-byte blocks for disk usage\n  -l                         use a long listing format\n  -L, --dereference          when showing file information for a symbolic\n                               link, show information for the file the link\n                               references rather than for the link itself\n  -m                         fill width with a comma separated list of entries\n  -n, --numeric-uid-gid      like -l, but list numeric user and group IDs\n  -N, --literal              print raw entry names (don't treat e.g. control\n                               characters specially)\n  -o                         like -l, but do not list group information\n  -p, --indicator-style=slash\n                             append / indicator to directories\n  -q, --hide-control-chars   print ? instead of nongraphic characters\n      --show-control-chars   show nongraphic characters as-is (the default,\n                               unless program is 'ls' and output is a terminal)\n  -Q, --quote-name           enclose entry names in double quotes\n      --quoting-style=WORD   use quoting style WORD for entry names:\n                               literal, locale, shell, shell-always,\n                               shell-escape, shell-escape-always, c, escape\n  -r, --reverse              reverse order while sorting\n  -R, --recursive            list subdirectories recursively\n  -s, --size                 print the allocated size of each file, in blocks\n  -S                         sort by file size, largest first\n      --sort=WORD            sort by WORD instead of name: none (-U), size (-S),\n                               time (-t), version (-v), extension (-X)\n      --time=WORD            with -l, show time as WORD instead of default\n                               modification time: atime or access or use (-u);\n                               ctime or status (-c); also use specified time\n                               as sort key if --sort=time (newest first)\n      --time-style=STYLE     with -l, show times using style STYLE:\n                               full-iso, long-iso, iso, locale, or +FORMAT;\n                               FORMAT is interpreted like in 'date'; if FORMAT\n                               is FORMAT1<newline>FORMAT2, then FORMAT1 applies\n                               to non-recent files and FORMAT2 to recent files;\n                               if STYLE is prefixed with 'posix-', STYLE\n                               takes effect only outside the POSIX locale\n  -t                         sort by modification time, newest first\n  -T, --tabsize=COLS         assume tab stops at each COLS instead of 8\n  -u                         with -lt: sort by, and show, access time;\n                               with -l: show access time and sort by name;\n                               otherwise: sort by access time, newest first\n  -U                         do not sort; list entries in directory order\n  -v                         natural sort of (version) numbers within text\n  -w, --width=COLS           set output width to COLS.  0 means no limit\n  -x                         list entries by lines instead of by columns\n  -X                         sort alphabetically by entry extension\n  -Z, --context              print any security context of each file\n  -1                         list one file per line.  Avoid '\\n' with -q or -b\n      --help     display this help and exit\n      --version  output version information and exit\n\nThe SIZE argument is an integer and optional unit (example: 10K is 10*1024).\nUnits are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).\n\nUsing color to distinguish file types is disabled both by default and\nwith --color=never.  With --color=auto, ls emits color codes only when\nstandard output is connected to a terminal.  The LS_COLORS environment\nvariable can change the settings.  Use the dircolors command to set it.\n\nExit status:\n 0  if OK,\n 1  if minor problems (e.g., cannot access subdirectory),\n 2  if serious trouble (e.g., cannot access command-line argument).\n\nGNU coreutils online help: <http://www.gnu.org/software/coreutils/>\nFull documentation at: <http://www.gnu.org/software/coreutils/ls>\nor available locally via: info '(coreutils) ls invocation'\n\n\n\n\nOh...  lots... but we aren't going to worry about that.\n\n\nFor now, let's look inside the \nDownloads/\n folder where we downloaded that zip\nfile at the beginning of the workshop.\n\n\nIf we want to use \nls\n on a different folder than the \ncurrent\n folder, just\npass the name of the folder you want to look in:\n\n\n$ ls -F Downloads/\n\n\n\n\nuber-trip-data-master.zip\n\n\n\n\nThere it is!\n\n\ncd\n\n\nOk, we know where the zip file is, time to \nchange directory\n to the folder \nDownloads/\n.  To do this, we use the \ncd\n command:\n\n\n$ cd Downloads\n\n\n\n\nNow let's check in with \npwd\n again:\n\n\n$ pwd\n\n\n\n\n/home/gil/Downloads\n\n\n\n\nOk! Cool! We moved! Now if we run \nls\n we should see the zip file in here.\n\n\n$ ls -F\n\n\n\n\nuber-trip-data-master.zip\n\n\n\n\nAnd there it is! Ok. We'll come back here in a second, but first let's explore a\nlittle more. Let's go back to the \"home directory\".\n\n\nHow do we do that...?\n\n\nThe home directory has the same name as our username. Let's try that!\n\n\n$ cd gil\n\n\n\n\ncd: no such file or directory: gil\n\n\n\n\nThat doesn't work. We're at the end of a branch of the tree that makes up the\nfilesystem. There has to be a way to go back -- what are we missing?\n\n\nLet's use \nls\n again, but this time add in the \n-a\n flag for \"show all\"\n\n\n$ ls -a\n\n\n\n\n.  ..  uber-trip-data-master.zip\n\n\n\n\nAHA! There are two more entries that we didn't see before: \n.\n and \n..\n\nWhat are those? Learn by doing, I say:\n\n\n$ cd .\n\n\n\n\n$ pwd\n\n\n\n\n/home/gil/Downloads\n\n\n\n\nWe're in the same spot. The \n.\n directory is a special directory in every folder\non the filesystem and it points to the current working directory.\n\n\n$ cd ..\n\n\n\n\n$ pwd\n\n\n\n\n/home/gil/\n\n\n\n\nWe made it back \nhome\n! The \n..\n directory is another special directory, but this one always points to the \nparent\n of the current directory.\n\n\nLet's try moving up a few more times!\n\n\n$ cd ..\n\n\n\n\n$ pwd\n\n\n\n\n/home\n\n\n\n\n$ cd ..\n\n\n\n\n$ pwd\n\n\n\n\n/\n\n\n\n\n$ cd ..\n\n\n\n\n$ pwd\n\n\n\n\n/\n\n\n\n\nWe can't go back any further because we are at the \nroot\n of the file \ntree\n.\n\n\nLet's take a brief moment to look at how the file system is organized.\n\n\n\n\nFigure 1. The inverted tree filesystem\n\n\n\nThe filesystem is an inverted tree. From \nroot\n we can see every branch below\n(which is everything). From \ngil\n, all of the folders in my home directory are\nvisible, but to move \nup\n the tree, we need to either know the folder path we\nwant to change to, or use the \n..\n shortcut.\n\n\nNow that we've had a look around, time to go back to the home directory.\nLet's use a little shortcut:\n\n\n$ cd\n\n\n\n\n$ pwd\n\n\n\n\n/home/gil/\n\n\n\n\nIf you don't pass a target to \ncd\n it will always take you back to your home\ndirectory by default. This is a nice option if you're looking around in a very\ndeep directory tree.\n\n\nAbsolute vs. relative paths\n\n\nAll of the navigation so far has been \nrelative\n. We are in the home directory,\nwe want to go to \nDesktop\n and so we type \ncd Desktop\n. This wouldn't work if we\nwere in a different directory.\n\n\nOne option when you need to jump around is to use \nabsolute\n paths, like this:\n\n\n$ cd /home/gil/Desktop\n\n\n\n\n$ pwd\n\n\n\n\n/home/gil/Desktop\n\n\n\n\nThe benefit of an absolute path is that it will work no matter where you start\nfrom, which can be helpful if you are deep in a directory tree.\n\n\nOne useful shortcut when typing out absolute paths is the \n~\n. The \n~\n is a\nshortcut for your home directory, so you don't need to explicitly write out\n\n/home/<username>/\n all the time.\n\n\n$ cd ~/Desktop\n\n\n\n\n$ pwd\n\n\n\n\n/home/gil/Desktop\n\n\n\n\nTab completion\n\n\nBefore we go any further, let's take a look at one of the most useful features\nof the *nix command line: tab completion\n\n\nReturn to the home directory if you aren't there already.\n\n\n$ cd\n\n\n\n\nType\n\n\n$ cd T\n\n\n\n\nthen hit the TAB key. Pretty cool, huh?\n\n\nWhenever you hit the TAB key, the shell will try to complete the remainder of\nthe line for you! It can't read minds, though. Since \nTemplates\n is the only\ndirectory beginning with \nT\n, the shell knew what to do. Let's try a different\nexample.\n\n\nType\n\n\n$ cd Do\n\n\n\n\nthen hit the TAB key.\n\n\nNothing. But hit it again\n\n\nDocuments/  Downloads/\n\n\n\n\nThere are two possible answers based on a prefix \nDo\n. In this case, tab\ncompletion will only complete up to the common prefix, which is just \nDo\n. It\nneeds a little more information to finish the completion. Try adding a \nc\n and\nhitting TAB again.\n\n\n$ cd Doc\n\n\n\n\n$ cd Documents/",
            "title": "Navigation"
        },
        {
            "location": "/nix/navigation/#navigating-the-command-line",
            "text": "Welcome!  The command line can be a little intimidating at first, but it is a powerful and\nefficient way of interacting with your computer. It's also the lingua franca\nwhen dealing with computing clusters and remote machines.",
            "title": "Navigating the Command Line"
        },
        {
            "location": "/nix/navigation/#whoami",
            "text": "Before we do anything else, let's figure out who we are. We can ask on the\ncommand line:  $ whoami  gil  That's my username!",
            "title": "whoami"
        },
        {
            "location": "/nix/navigation/#pwd",
            "text": "Now that we know who we are, time to figure out  where  we are.  To do that, we ask the terminal to  print working directory  or  pwd  $ pwd  /home/gil/  We're in the \"home directory\" for our user.",
            "title": "pwd"
        },
        {
            "location": "/nix/navigation/#ls",
            "text": "Let's look around  $ ls  Desktop  Documents  Downloads  Music  Pictures  Public  Templates  Videos  I think those are folders?  How can we tell? Use the  -F  flag  $ ls -F  Desktop/  Documents/  Downloads/  Music/  Pictures/  Public/  Templates/  Videos/  They all have a trailing slash, so they're all folders.\nWhat other options does  ls  have?  $ ls --help  Usage: ls [OPTION]... [FILE]...\nList information about the FILEs (the current directory by default).\nSort entries alphabetically if none of -cftuvSUX nor --sort is specified.\n\nMandatory arguments to long options are mandatory for short options too.\n  -a, --all                  do not ignore entries starting with .\n  -A, --almost-all           do not list implied . and ..\n      --author               with -l, print the author of each file\n  -b, --escape               print C-style escapes for nongraphic characters\n      --block-size=SIZE      scale sizes by SIZE before printing them; e.g.,\n                               '--block-size=M' prints sizes in units of\n                               1,048,576 bytes; see SIZE format below\n  -B, --ignore-backups       do not list implied entries ending with ~\n  -c                         with -lt: sort by, and show, ctime (time of last\n                               modification of file status information);\n                               with -l: show ctime and sort by name;\n                               otherwise: sort by ctime, newest first\n  -C                         list entries by columns\n      --color[=WHEN]         colorize the output; WHEN can be 'always' (default\n                               if omitted), 'auto', or 'never'; more info below\n  -d, --directory            list directories themselves, not their contents\n  -D, --dired                generate output designed for Emacs' dired mode\n  -f                         do not sort, enable -aU, disable -ls --color\n  -F, --classify             append indicator (one of */=>@|) to entries\n      --file-type            likewise, except do not append '*'\n      --format=WORD          across -x, commas -m, horizontal -x, long -l,\n                               single-column -1, verbose -l, vertical -C\n      --full-time            like -l --time-style=full-iso\n  -g                         like -l, but do not list owner\n      --group-directories-first\n                             group directories before files;\n                               can be augmented with a --sort option, but any\n                               use of --sort=none (-U) disables grouping\n  -G, --no-group             in a long listing, don't print group names\n  -h, --human-readable       with -l and/or -s, print human readable sizes\n     recommonmark                          (e.g., 1K 234M 2G)\n      --si                   likewise, but use powers of 1000 not 1024\n  -H, --dereference-command-line\n                             follow symbolic links listed on the command line\n      --dereference-command-line-symlink-to-dir\n                             follow each command line symbolic link\n                               that points to a directory\n      --hide=PATTERN         do not list implied entries matching shell PATTERN\n                               (overridden by -a or -A)\n      --indicator-style=WORD  append indicator with style WORD to entry names:\n                               none (default), slash (-p),\n                               file-type (--file-type), classify (-F)\n  -i, --inode                print the index number of each file\n  -I, --ignore=PATTERN       do not list implied entries matching shell PATTERN\n  -k, --kibibytes            default to 1024-byte blocks for disk usage\n  -l                         use a long listing format\n  -L, --dereference          when showing file information for a symbolic\n                               link, show information for the file the link\n                               references rather than for the link itself\n  -m                         fill width with a comma separated list of entries\n  -n, --numeric-uid-gid      like -l, but list numeric user and group IDs\n  -N, --literal              print raw entry names (don't treat e.g. control\n                               characters specially)\n  -o                         like -l, but do not list group information\n  -p, --indicator-style=slash\n                             append / indicator to directories\n  -q, --hide-control-chars   print ? instead of nongraphic characters\n      --show-control-chars   show nongraphic characters as-is (the default,\n                               unless program is 'ls' and output is a terminal)\n  -Q, --quote-name           enclose entry names in double quotes\n      --quoting-style=WORD   use quoting style WORD for entry names:\n                               literal, locale, shell, shell-always,\n                               shell-escape, shell-escape-always, c, escape\n  -r, --reverse              reverse order while sorting\n  -R, --recursive            list subdirectories recursively\n  -s, --size                 print the allocated size of each file, in blocks\n  -S                         sort by file size, largest first\n      --sort=WORD            sort by WORD instead of name: none (-U), size (-S),\n                               time (-t), version (-v), extension (-X)\n      --time=WORD            with -l, show time as WORD instead of default\n                               modification time: atime or access or use (-u);\n                               ctime or status (-c); also use specified time\n                               as sort key if --sort=time (newest first)\n      --time-style=STYLE     with -l, show times using style STYLE:\n                               full-iso, long-iso, iso, locale, or +FORMAT;\n                               FORMAT is interpreted like in 'date'; if FORMAT\n                               is FORMAT1<newline>FORMAT2, then FORMAT1 applies\n                               to non-recent files and FORMAT2 to recent files;\n                               if STYLE is prefixed with 'posix-', STYLE\n                               takes effect only outside the POSIX locale\n  -t                         sort by modification time, newest first\n  -T, --tabsize=COLS         assume tab stops at each COLS instead of 8\n  -u                         with -lt: sort by, and show, access time;\n                               with -l: show access time and sort by name;\n                               otherwise: sort by access time, newest first\n  -U                         do not sort; list entries in directory order\n  -v                         natural sort of (version) numbers within text\n  -w, --width=COLS           set output width to COLS.  0 means no limit\n  -x                         list entries by lines instead of by columns\n  -X                         sort alphabetically by entry extension\n  -Z, --context              print any security context of each file\n  -1                         list one file per line.  Avoid '\\n' with -q or -b\n      --help     display this help and exit\n      --version  output version information and exit\n\nThe SIZE argument is an integer and optional unit (example: 10K is 10*1024).\nUnits are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).\n\nUsing color to distinguish file types is disabled both by default and\nwith --color=never.  With --color=auto, ls emits color codes only when\nstandard output is connected to a terminal.  The LS_COLORS environment\nvariable can change the settings.  Use the dircolors command to set it.\n\nExit status:\n 0  if OK,\n 1  if minor problems (e.g., cannot access subdirectory),\n 2  if serious trouble (e.g., cannot access command-line argument).\n\nGNU coreutils online help: <http://www.gnu.org/software/coreutils/>\nFull documentation at: <http://www.gnu.org/software/coreutils/ls>\nor available locally via: info '(coreutils) ls invocation'  Oh...  lots... but we aren't going to worry about that.  For now, let's look inside the  Downloads/  folder where we downloaded that zip\nfile at the beginning of the workshop.  If we want to use  ls  on a different folder than the  current  folder, just\npass the name of the folder you want to look in:  $ ls -F Downloads/  uber-trip-data-master.zip  There it is!",
            "title": "ls"
        },
        {
            "location": "/nix/navigation/#cd",
            "text": "Ok, we know where the zip file is, time to  change directory  to the folder  Downloads/ .  To do this, we use the  cd  command:  $ cd Downloads  Now let's check in with  pwd  again:  $ pwd  /home/gil/Downloads  Ok! Cool! We moved! Now if we run  ls  we should see the zip file in here.  $ ls -F  uber-trip-data-master.zip  And there it is! Ok. We'll come back here in a second, but first let's explore a\nlittle more. Let's go back to the \"home directory\".  How do we do that...?  The home directory has the same name as our username. Let's try that!  $ cd gil  cd: no such file or directory: gil  That doesn't work. We're at the end of a branch of the tree that makes up the\nfilesystem. There has to be a way to go back -- what are we missing?  Let's use  ls  again, but this time add in the  -a  flag for \"show all\"  $ ls -a  .  ..  uber-trip-data-master.zip  AHA! There are two more entries that we didn't see before:  .  and  .. \nWhat are those? Learn by doing, I say:  $ cd .  $ pwd  /home/gil/Downloads  We're in the same spot. The  .  directory is a special directory in every folder\non the filesystem and it points to the current working directory.  $ cd ..  $ pwd  /home/gil/  We made it back  home ! The  ..  directory is another special directory, but this one always points to the  parent  of the current directory.  Let's try moving up a few more times!  $ cd ..  $ pwd  /home  $ cd ..  $ pwd  /  $ cd ..  $ pwd  /  We can't go back any further because we are at the  root  of the file  tree .  Let's take a brief moment to look at how the file system is organized.   Figure 1. The inverted tree filesystem  The filesystem is an inverted tree. From  root  we can see every branch below\n(which is everything). From  gil , all of the folders in my home directory are\nvisible, but to move  up  the tree, we need to either know the folder path we\nwant to change to, or use the  ..  shortcut.  Now that we've had a look around, time to go back to the home directory.\nLet's use a little shortcut:  $ cd  $ pwd  /home/gil/  If you don't pass a target to  cd  it will always take you back to your home\ndirectory by default. This is a nice option if you're looking around in a very\ndeep directory tree.",
            "title": "cd"
        },
        {
            "location": "/nix/navigation/#absolute-vs-relative-paths",
            "text": "All of the navigation so far has been  relative . We are in the home directory,\nwe want to go to  Desktop  and so we type  cd Desktop . This wouldn't work if we\nwere in a different directory.  One option when you need to jump around is to use  absolute  paths, like this:  $ cd /home/gil/Desktop  $ pwd  /home/gil/Desktop  The benefit of an absolute path is that it will work no matter where you start\nfrom, which can be helpful if you are deep in a directory tree.  One useful shortcut when typing out absolute paths is the  ~ . The  ~  is a\nshortcut for your home directory, so you don't need to explicitly write out /home/<username>/  all the time.  $ cd ~/Desktop  $ pwd  /home/gil/Desktop",
            "title": "Absolute vs. relative paths"
        },
        {
            "location": "/nix/navigation/#tab-completion",
            "text": "Before we go any further, let's take a look at one of the most useful features\nof the *nix command line: tab completion  Return to the home directory if you aren't there already.  $ cd  Type  $ cd T  then hit the TAB key. Pretty cool, huh?  Whenever you hit the TAB key, the shell will try to complete the remainder of\nthe line for you! It can't read minds, though. Since  Templates  is the only\ndirectory beginning with  T , the shell knew what to do. Let's try a different\nexample.  Type  $ cd Do  then hit the TAB key.  Nothing. But hit it again  Documents/  Downloads/  There are two possible answers based on a prefix  Do . In this case, tab\ncompletion will only complete up to the common prefix, which is just  Do . It\nneeds a little more information to finish the completion. Try adding a  c  and\nhitting TAB again.  $ cd Doc  $ cd Documents/",
            "title": "Tab completion"
        },
        {
            "location": "/nix/file_handling/",
            "text": "Creating, editing, removing files\n\n\nUse a text editor\n\n\nThere are a \nlot\n of different text editors and a \nlot\n of strong\nfeelings about which of them is the best.\n\n\nYou can use any editor you like, but you \nmust\n know how to use at least one\nterminal-friendly editor. In this workshop we are going to use \nnano\n. It's\nsimple and easy to use.\n\n\nMake sure you're in your home directory (use \ncd\n and \npwd\n to confirm) then type\n\n\n$ nano\n\n\n\n\nThis is a no-frills editor. Type something! How about a TODO list?\n\n\nTODO\n* [x] Learn how to navigate using the terminal\n* [ ] Learn how to create files\n* [ ] Learn about pipes and redirects\n\n\n\n\nAt the bottom you'll notice a bunch of different options but we are concerned\nwith only two of them: \nWrite Out\n (save) and \nExit\n.\n\n\nThe caret (\n^\n) means the Control key. To save the TODO list, hit \nCtrl+o\n, type\nin a name (how about \"TODO\") and then hit \nEnter\n. \nnano\n will report that it\nwrote some number of lines.\n\n\nNow exit \nnano\n by typing \nCtrl+x\n.\n\n\nUse \nls\n to see what happened:\n\n\n$ ls -F\n\n\n\n\nDesktop/  Documents/  Downloads/  Music/  Pictures/  Public/  TODO  Templates/  Videos/\n\n\n\n\nThere's the TODO list! If you want to edit the todo list, you can open it up in\n\nnano\n (you can use tab completion for the filename, too!)\n\n\n$ nano TODO\n\n\n\n\nCheck off the second item on the todo list and then save and exit \nnano\n. Notice\nthat when you hit \nCtrl+o\n to save an existing file, \nnano\n will automatically\nfill in the name of the existing file. If you wanted to \"Save As...\", you can\nsimply change the name in the \nWrite Out\n bar.\n\n\nCreate an empty file\n\n\nThere are a few ways to create files on the command line. If you want to create\nan \"empty\" file, you can use \ntouch\n. Try it!\n\n\n$ touch newfile\n\n\n\n\n$ ls -F\n\n\n\n\nDesktop/    Downloads/  Pictures/  TODO        Videos/\nDocuments/  Music/      Public/    Templates/  newfile\n\n\n\n\nYou can open \nnewfile\n in \nnano\n to confirm that it's empty. Then just exit out\nusing \nCtrl+x\n since there's nothing to save!\n\n\nCreate a directory\n\n\nTo create a new directory, use the \nmkdir\n command. We can create a \nResearch/\n\nfolder in the home directory.\n\n\n$ mkdir Research\n\n\n\n\n$ ls -F\n\n\n\n\nDesktop/    Downloads/  Pictures/  Research/  Templates/  newfile\nDocuments/  Music/      Public/    TODO       Videos/\n\n\n\n\nRemove a file\n\n\nWe don't need that empty file sitting around, we can remove it. To remove a\nfile, use the \nrm\n command:\n\n\n$ rm newfile\n\n\n\n\nDid anything happen?\n\n\n$ ls -F\n\n\n\n\nDesktop/    Downloads/  Pictures/  Research/  Templates/\nDocuments/  Music/      Public/    TODO       Videos/\n\n\n\n\nYes, \nnewfile\n is gone. And this is something to be aware of: there is no\n\"Recycle Bin\". There is no \"Undo\". That file is gone.\n\n\nRemove a directory\n\n\nLet's try to remove the \nResearch\n directory we created earlier.\n\n\n$ rm Research\n\n\n\n\nrm: cannot remove 'Research': Is a directory\n\n\n\n\nrm\n only works with files by default. If you want to remove the directory you\nneed to use the \n-r\n flag to specify a \nrecursive\n removal.\n\n\nThis will delete the directory and ALL of its contents. BE CAREFUL WHEN USING\nTHIS\n\n\n$ rm -r Research\n\n\n\n\n$ ls -F\n\n\n\n\nDesktop/    Downloads/  Pictures/  TODO        Videos/\nDocuments/  Music/      Public/    Templates/\n\n\n\n\nMove/Rename a file\n\n\nWe know how to create and delete files and folders now. What about renaming a\nfile?\n\n\nTo rename a file, we use the \nmv\n command, which is short for \"move\". This may\nseem a little bit odd at first, but renaming a file is the same as moving it to\na different location.\n\n\nTo start, let's make the file \nTODO\n lowercase. The syntax is \nmv <old location>\n<new location>\n\n\n$ mv TODO todo\n\n\n\n\n$ ls -F\n\n\n\n\nDesktop/    Downloads/  Pictures/  Templates/  todo\nDocuments/  Music/      Public/    Videos/\n\n\n\n\nWe \nmoved\n the file \nTODO\n from \n/home/<user>/TODO\n to a new location, called\n\n/home/<user>/todo\n. Since the directory doesn't change, the result is a\n\nrenamed\n file. \n\n\nWe can also move the \ntodo\n list to a different folder:\n\n\n$ mv todo Desktop/\n\n\n\n\n$ ls -F\n\n\n\n\nDesktop/  Documents/  Downloads/  Music/  Pictures/  Public/  Templates/  Videos/\n\n\n\n\nWe specified \nDesktop/\n as the \n<new location>\n in the \nmv\n command. Since\n\nDesktop/\n is a folder, \ntodo\n will move inside that folder. \n\n\n$ ls -F Desktop/\n\n\n\n\ntodo workshop_data.zip\n\n\n\n\nNote:\n As we see, if \n<new location>\n is a folder, then the file is moved\ninside the folder. However, if \n<new location>\n is an existing \nfile\n, then that\nfile will be overwritten.",
            "title": "File Creation and Editing"
        },
        {
            "location": "/nix/file_handling/#creating-editing-removing-files",
            "text": "",
            "title": "Creating, editing, removing files"
        },
        {
            "location": "/nix/file_handling/#use-a-text-editor",
            "text": "There are a  lot  of different text editors and a  lot  of strong\nfeelings about which of them is the best.  You can use any editor you like, but you  must  know how to use at least one\nterminal-friendly editor. In this workshop we are going to use  nano . It's\nsimple and easy to use.  Make sure you're in your home directory (use  cd  and  pwd  to confirm) then type  $ nano  This is a no-frills editor. Type something! How about a TODO list?  TODO\n* [x] Learn how to navigate using the terminal\n* [ ] Learn how to create files\n* [ ] Learn about pipes and redirects  At the bottom you'll notice a bunch of different options but we are concerned\nwith only two of them:  Write Out  (save) and  Exit .  The caret ( ^ ) means the Control key. To save the TODO list, hit  Ctrl+o , type\nin a name (how about \"TODO\") and then hit  Enter .  nano  will report that it\nwrote some number of lines.  Now exit  nano  by typing  Ctrl+x .  Use  ls  to see what happened:  $ ls -F  Desktop/  Documents/  Downloads/  Music/  Pictures/  Public/  TODO  Templates/  Videos/  There's the TODO list! If you want to edit the todo list, you can open it up in nano  (you can use tab completion for the filename, too!)  $ nano TODO  Check off the second item on the todo list and then save and exit  nano . Notice\nthat when you hit  Ctrl+o  to save an existing file,  nano  will automatically\nfill in the name of the existing file. If you wanted to \"Save As...\", you can\nsimply change the name in the  Write Out  bar.",
            "title": "Use a text editor"
        },
        {
            "location": "/nix/file_handling/#create-an-empty-file",
            "text": "There are a few ways to create files on the command line. If you want to create\nan \"empty\" file, you can use  touch . Try it!  $ touch newfile  $ ls -F  Desktop/    Downloads/  Pictures/  TODO        Videos/\nDocuments/  Music/      Public/    Templates/  newfile  You can open  newfile  in  nano  to confirm that it's empty. Then just exit out\nusing  Ctrl+x  since there's nothing to save!",
            "title": "Create an empty file"
        },
        {
            "location": "/nix/file_handling/#create-a-directory",
            "text": "To create a new directory, use the  mkdir  command. We can create a  Research/ \nfolder in the home directory.  $ mkdir Research  $ ls -F  Desktop/    Downloads/  Pictures/  Research/  Templates/  newfile\nDocuments/  Music/      Public/    TODO       Videos/",
            "title": "Create a directory"
        },
        {
            "location": "/nix/file_handling/#remove-a-file",
            "text": "We don't need that empty file sitting around, we can remove it. To remove a\nfile, use the  rm  command:  $ rm newfile  Did anything happen?  $ ls -F  Desktop/    Downloads/  Pictures/  Research/  Templates/\nDocuments/  Music/      Public/    TODO       Videos/  Yes,  newfile  is gone. And this is something to be aware of: there is no\n\"Recycle Bin\". There is no \"Undo\". That file is gone.",
            "title": "Remove a file"
        },
        {
            "location": "/nix/file_handling/#remove-a-directory",
            "text": "Let's try to remove the  Research  directory we created earlier.  $ rm Research  rm: cannot remove 'Research': Is a directory  rm  only works with files by default. If you want to remove the directory you\nneed to use the  -r  flag to specify a  recursive  removal.  This will delete the directory and ALL of its contents. BE CAREFUL WHEN USING\nTHIS  $ rm -r Research  $ ls -F  Desktop/    Downloads/  Pictures/  TODO        Videos/\nDocuments/  Music/      Public/    Templates/",
            "title": "Remove a directory"
        },
        {
            "location": "/nix/file_handling/#moverename-a-file",
            "text": "We know how to create and delete files and folders now. What about renaming a\nfile?  To rename a file, we use the  mv  command, which is short for \"move\". This may\nseem a little bit odd at first, but renaming a file is the same as moving it to\na different location.  To start, let's make the file  TODO  lowercase. The syntax is  mv <old location>\n<new location>  $ mv TODO todo  $ ls -F  Desktop/    Downloads/  Pictures/  Templates/  todo\nDocuments/  Music/      Public/    Videos/  We  moved  the file  TODO  from  /home/<user>/TODO  to a new location, called /home/<user>/todo . Since the directory doesn't change, the result is a renamed  file.   We can also move the  todo  list to a different folder:  $ mv todo Desktop/  $ ls -F  Desktop/  Documents/  Downloads/  Music/  Pictures/  Public/  Templates/  Videos/  We specified  Desktop/  as the  <new location>  in the  mv  command. Since Desktop/  is a folder,  todo  will move inside that folder.   $ ls -F Desktop/  todo workshop_data.zip  Note:  As we see, if  <new location>  is a folder, then the file is moved\ninside the folder. However, if  <new location>  is an existing  file , then that\nfile will be overwritten.",
            "title": "Move/Rename a file"
        },
        {
            "location": "/nix/redirection/",
            "text": "Redirection\n\n\nWhat happens when we run \nls\n? This isn't a trick question. \n\n\n$ ls -F\n\n\n\n\nDesktop/  Documents/  Downloads/  Music/  Pictures/  Public/  Templates/  Videos/\n\n\n\n\nWe get a list of the files and folders in the current directory \nprinted\n to the\nscreen. But what if we didn't want it to print to the screen but instead wanted\nto save it to a file somewhere?\n\n\nEasy! \n\n\nBy default, commands run on the command line print to \nstdout\n (standard\noutput). If we want to specify a different location to print to, we use the \n>\n\nsymbol.\n\n\nTry it out:\n\n\n$ ls -F > filelist\n\n\n\n\nNo output appeared on the screen (\nstdout\n). Let's check what's in \nfilelist\n\n\n$ nano filelist\n\n\n\n\nThere it is! \n\n\nNow, what do we do with it...? How about we try counting the number of lines in\n\nfilelist\n? \n\n\nImagine we have a MUCH bigger directory and we want to know how many files and\nfolders are in it. When we redirect \nls\n to a file, every file and folder is\nwritten to a separate line; if we count the number of lines, we know how many\nfiles there are!\n\n\nHow do we count the number of lines? We use \nwc\n (wordcount)\n\n\nwc\n\n\nYes, it's \nword_count, but it counts lines, words and characters. We'll\nplay with \nwc\n more later, but for now, let's just count the number of lines in\n\nfilelist\n. To specify that we want the number of _lines\n in a file we use the\n\n-l\n flag.\n\n\n$ wc -l filelist\n\n\n\n\n8 filelist\n\n\n\n\nCool! \nwc\n tells us that \nfilelist\n has 8 lines, which means we have 8\nfiles/folders in the HOME directory. (Yes, we already knew that since we can\ncount, but still...)",
            "title": "Redirection"
        },
        {
            "location": "/nix/redirection/#redirection",
            "text": "What happens when we run  ls ? This isn't a trick question.   $ ls -F  Desktop/  Documents/  Downloads/  Music/  Pictures/  Public/  Templates/  Videos/  We get a list of the files and folders in the current directory  printed  to the\nscreen. But what if we didn't want it to print to the screen but instead wanted\nto save it to a file somewhere?  Easy!   By default, commands run on the command line print to  stdout  (standard\noutput). If we want to specify a different location to print to, we use the  > \nsymbol.  Try it out:  $ ls -F > filelist  No output appeared on the screen ( stdout ). Let's check what's in  filelist  $ nano filelist  There it is!   Now, what do we do with it...? How about we try counting the number of lines in filelist ?   Imagine we have a MUCH bigger directory and we want to know how many files and\nfolders are in it. When we redirect  ls  to a file, every file and folder is\nwritten to a separate line; if we count the number of lines, we know how many\nfiles there are!  How do we count the number of lines? We use  wc  (wordcount)",
            "title": "Redirection"
        },
        {
            "location": "/nix/redirection/#wc",
            "text": "Yes, it's  word_count, but it counts lines, words and characters. We'll\nplay with  wc  more later, but for now, let's just count the number of lines in filelist . To specify that we want the number of _lines  in a file we use the -l  flag.  $ wc -l filelist  8 filelist  Cool!  wc  tells us that  filelist  has 8 lines, which means we have 8\nfiles/folders in the HOME directory. (Yes, we already knew that since we can\ncount, but still...)",
            "title": "wc"
        },
        {
            "location": "/nix/pipes/",
            "text": "Pipes and intro to unix utilities\n\n\nWe just learned about redirecting output to files using the \n>\n operator. In\naddition to redirecting a data stream to a file, we can also \nintercept\n that\nstream of information and perform another operation on it. \n\n\nTo do this we use the \n|\n operator which we call a pipe. \n\n\nPipes allow a user to string together a series of commands, a \"command\npipeline\", and there are many useful utilites that are commonly installed on\nUNIX systems. \n\n\nThe use of these many small programs is only clear when we use it in concert\nwith pipes, so we're going to learn about them at the same time.\n\n\ncat\n\n\nIn the redirection exercise we wrote the contents of the command \nls -F\n into a\nfile called \nfilelist\n. When we checked to see if it worked, we opened the file\nup in \nnano\n. That didn't take very long, but it can be a pain if you need to\nlook through the contents of a number of files.\n\n\nNow, we didn't need to \nedit\n \nfilelist\n, right? We just wanted to look at it. \nThis is the perfect job for \ncat\n!\n\n\ncat\n dumps the contents of a file into \nstdout\n (by default). \n\n\nTry it out on \nfilelist\n to see what happens.\n\n\n$ cat filelist\n\n\n\n\nDesktop/\nDocuments/\nDownloads/\nMusic/\nPictures/\nPublic/\nTemplates/\nVideos/\n\n\n\n\nTime to pipe!\n\n\nRemember \nwc -l\n? We used it to count the lines in \nfilelist\n. We did:\n\n\n$ wc -l filelist\n\n\n\n\n8 filelist\n\n\n\n\nBut instead of doing it this way, we can also \npipe\n the \ncontents\n of\n\nfilelist\n to \nwc\n.\n\n\nTry it out!\n\n\n$ cat filelist | wc -l\n\n\n\n\n8\n\n\n\n\nWhat just happened? \n\n\nWe used \ncat\n to dump the contents of \nfilelist\n to the screen (\nstdout\n). But\nthen, instead of printing the contents, we intercepted them with the pipe and\ninstead fed them into \nwc\n. \n\n\nSkip \nfilelist\n\n\nWe used \n>\n to redirect the contents of \nls -F\n, then used \ncat\n to dump the\ncontents of \nfilelist\n and then piped those contents to \nwc\n. Are all of these\nsteps necessary?\n\n\nNo! How about: \n\n\nls -F | wc -l\n\n\n\n\n9\n\n\n\n\nAny output can be piped to (nearly) any other program. \n\n\ngrep\n\n\ngrep\n is your best friend, you just don't know it yet. \ngrep\n does stand for\nsomething, but it's long and confusing, so just accept that \ngrep\n is \ngrep\n. \n\n\ngrep\n searches through text files and streams for matches. It is one of the\nmost powerful tools in the UNIX toolbox. It's also \n42 years old\n. And we still\nuse it. It's that good.\n\n\nTry it out by piping the contents of \nls -F\n and \ngrep\nping for \"Do\"\n\n\n$ ls -F | grep Do\n\n\n\n\nDocuments/\nDownloads/\n\n\n\n\nExercise\n\n\nThere are obviously two files/folders that contain \nDo\n that \ngrep\n has matched.\nBut what if there were hundreds? How can we count the number of results from a\n\ngrep\n? \n\n\nUse \nls\n, \ngrep\n and any tools we've already learned about to get the command\nline to spit out the number of files/folders that contain \nDo\n in their title.\n\n\nsort\n\n\nIn order to learn about \nsort\n, we need \nsomething\n to sort. We could download a\nfile using the web browser, but why would we? Simpler to use \nwget\n on the\ncommand line!\n\n\nwget https://raw.githubusercontent.com/barbagroup/essential_skills_RRC/master/resources/copa_america_goals\n\n\n\n\nAfter \nwget\n finishes, use \nls\n to check and make sure that the file has\ndownloaded.\n\n\nOk! We have downloaded a list of goals scored in the 2016 Copa America, let's\ntake a look at what the file contains:\n\n\n$ cat copa_america_goals \n\n\n\n\n1 Miku\n1 Neymar\n1 Robinho\n3 Sergio Aguero\n2 Charles Aranguiz\n3 Lucas Barrios\n1 Edgar Benitez\n2 Miller Bolanos\n1 Andrew Carrillo\n1 Douglas Costa\n1 Christian Cueva\n2 Angel Di Maria\n1 Roberto Firmino\n1 Jose Gimenez\n1 Derlis Gonzalez\n4 Paolo Guerrero\n1 Nelson Haedo Valdez\n2 Gonzalo Higuain\n1 Mauricio Isla\n2 Raul Jimenez\n2 Marcelo Martins Moreno\n1 Gary Medel\n1 Lionel Messi\n1 Jeison Murillo\n1 Javier pastore\n1 Claudio Pizarro\n1 Ronald Raldes\n1 Cristian Rodriguez\n1 Marcos Rojo\n1 Salomon Rondon\n1 Alexis Sanchez\n1 Thiago Silva\n1 Martis Smedberg-Dalence\n2 Enner Valencia\n4 Eduardo Vargas\n3 Arturo Vidal\n2 Matias Vuoso\n\n\n\n\nThe first column is goals, then first names, then last names. And of course,\nsome players only have one name. How many players scored 4 goals? We can \ngrep\n\nfor that, which will definitely work, but we can also sort the list easily using\nthe \nsort\n command.  Try it out!\n\n\n$ cat copa_america_goals | sort\n\n\n\n\n1 Alexis Sanchez\n1 Andrew Carrillo\n1 Christian Cueva\n1 Claudio Pizarro\n1 Cristian Rodriguez\n1 Derlis Gonzalez\n1 Douglas Costa\n1 Edgar Benitez\n1 Gary Medel\n1 Javier pastore\n1 Jeison Murillo\n1 Jose Gimenez\n1 Lionel Messi\n1 Marcos Rojo\n1 Martis Smedberg-Dalence\n1 Mauricio Isla\n1 Miku\n1 Nelson Haedo Valdez\n1 Neymar\n1 Roberto Firmino\n1 Robinho\n1 Ronald Raldes\n1 Salomon Rondon\n1 Thiago Silva\n2 Angel Di Maria\n2 Charles Aranguiz\n2 Enner Valencia\n2 Gonzalo Higuain\n2 Marcelo Martins Moreno\n2 Matias Vuoso\n2 Miller Bolanos\n2 Raul Jimenez\n3 Arturo Vidal\n3 Lucas Barrios\n3 Sergio Aguero\n4 Eduardo Vargas\n4 Paolo Guerrero\n\n\n\n\nAnd we see that at the bottom of the sorted list there are two players who\nscored 4 goals in the Copa. \n\n\nNow, sorting goal scorers by last name seems a little strange if we care about\nthe number of goals scored. Let's save the list of goals but sort it by the\nnumber of goals. How should we do that?\n\n\n$ cat copa_america_goals | sort > copa_goals_sorted\n\n\n\n\nAnd remember, there's no output to the screen (\nstdout\n) because we \nredirected\n\nit to a new file. We can \ncat\n the new file to make sure it worked as we expect.\n\n\n$ cat copa_goals_sorted \n\n\n\n\n1 Alexis Sanchez\n1 Andrew Carrillo\n1 Christian Cueva\n1 Claudio Pizarro\n1 Cristian Rodriguez\n1 Derlis Gonzalez\n1 Douglas Costa\n1 Edgar Benitez\n1 Gary Medel\n1 Javier pastore\n1 Jeison Murillo\n1 Jose Gimenez\n1 Lionel Messi\n1 Marcos Rojo\n1 Martis Smedberg-Dalence\n1 Mauricio Isla\n1 Miku\n1 Nelson Haedo Valdez\n1 Neymar\n1 Roberto Firmino\n1 Robinho\n1 Ronald Raldes\n1 Salomon Rondon\n1 Thiago Silva\n2 Angel Di Maria\n2 Charles Aranguiz\n2 Enner Valencia\n2 Gonzalo Higuain\n2 Marcelo Martins Moreno\n2 Matias Vuoso\n2 Miller Bolanos\n2 Raul Jimenez\n3 Arturo Vidal\n3 Lucas Barrios\n3 Sergio Aguero\n4 Eduardo Vargas\n4 Paolo Guerrero\n\n\n\n\n$ cat copa_goals_sorted | grep Alexis\n\n\n\n\n1 Alexis Sanchez",
            "title": "Piping"
        },
        {
            "location": "/nix/pipes/#pipes-and-intro-to-unix-utilities",
            "text": "We just learned about redirecting output to files using the  >  operator. In\naddition to redirecting a data stream to a file, we can also  intercept  that\nstream of information and perform another operation on it.   To do this we use the  |  operator which we call a pipe.   Pipes allow a user to string together a series of commands, a \"command\npipeline\", and there are many useful utilites that are commonly installed on\nUNIX systems.   The use of these many small programs is only clear when we use it in concert\nwith pipes, so we're going to learn about them at the same time.",
            "title": "Pipes and intro to unix utilities"
        },
        {
            "location": "/nix/pipes/#cat",
            "text": "In the redirection exercise we wrote the contents of the command  ls -F  into a\nfile called  filelist . When we checked to see if it worked, we opened the file\nup in  nano . That didn't take very long, but it can be a pain if you need to\nlook through the contents of a number of files.  Now, we didn't need to  edit   filelist , right? We just wanted to look at it. \nThis is the perfect job for  cat !  cat  dumps the contents of a file into  stdout  (by default).   Try it out on  filelist  to see what happens.  $ cat filelist  Desktop/\nDocuments/\nDownloads/\nMusic/\nPictures/\nPublic/\nTemplates/\nVideos/",
            "title": "cat"
        },
        {
            "location": "/nix/pipes/#time-to-pipe",
            "text": "Remember  wc -l ? We used it to count the lines in  filelist . We did:  $ wc -l filelist  8 filelist  But instead of doing it this way, we can also  pipe  the  contents  of filelist  to  wc .  Try it out!  $ cat filelist | wc -l  8  What just happened?   We used  cat  to dump the contents of  filelist  to the screen ( stdout ). But\nthen, instead of printing the contents, we intercepted them with the pipe and\ninstead fed them into  wc .",
            "title": "Time to pipe!"
        },
        {
            "location": "/nix/pipes/#skip-filelist",
            "text": "We used  >  to redirect the contents of  ls -F , then used  cat  to dump the\ncontents of  filelist  and then piped those contents to  wc . Are all of these\nsteps necessary?  No! How about:   ls -F | wc -l  9  Any output can be piped to (nearly) any other program.",
            "title": "Skip filelist"
        },
        {
            "location": "/nix/pipes/#grep",
            "text": "grep  is your best friend, you just don't know it yet.  grep  does stand for\nsomething, but it's long and confusing, so just accept that  grep  is  grep .   grep  searches through text files and streams for matches. It is one of the\nmost powerful tools in the UNIX toolbox. It's also  42 years old . And we still\nuse it. It's that good.  Try it out by piping the contents of  ls -F  and  grep ping for \"Do\"  $ ls -F | grep Do  Documents/\nDownloads/",
            "title": "grep"
        },
        {
            "location": "/nix/pipes/#exercise",
            "text": "There are obviously two files/folders that contain  Do  that  grep  has matched.\nBut what if there were hundreds? How can we count the number of results from a grep ?   Use  ls ,  grep  and any tools we've already learned about to get the command\nline to spit out the number of files/folders that contain  Do  in their title.",
            "title": "Exercise"
        },
        {
            "location": "/nix/pipes/#sort",
            "text": "In order to learn about  sort , we need  something  to sort. We could download a\nfile using the web browser, but why would we? Simpler to use  wget  on the\ncommand line!  wget https://raw.githubusercontent.com/barbagroup/essential_skills_RRC/master/resources/copa_america_goals  After  wget  finishes, use  ls  to check and make sure that the file has\ndownloaded.  Ok! We have downloaded a list of goals scored in the 2016 Copa America, let's\ntake a look at what the file contains:  $ cat copa_america_goals   1 Miku\n1 Neymar\n1 Robinho\n3 Sergio Aguero\n2 Charles Aranguiz\n3 Lucas Barrios\n1 Edgar Benitez\n2 Miller Bolanos\n1 Andrew Carrillo\n1 Douglas Costa\n1 Christian Cueva\n2 Angel Di Maria\n1 Roberto Firmino\n1 Jose Gimenez\n1 Derlis Gonzalez\n4 Paolo Guerrero\n1 Nelson Haedo Valdez\n2 Gonzalo Higuain\n1 Mauricio Isla\n2 Raul Jimenez\n2 Marcelo Martins Moreno\n1 Gary Medel\n1 Lionel Messi\n1 Jeison Murillo\n1 Javier pastore\n1 Claudio Pizarro\n1 Ronald Raldes\n1 Cristian Rodriguez\n1 Marcos Rojo\n1 Salomon Rondon\n1 Alexis Sanchez\n1 Thiago Silva\n1 Martis Smedberg-Dalence\n2 Enner Valencia\n4 Eduardo Vargas\n3 Arturo Vidal\n2 Matias Vuoso  The first column is goals, then first names, then last names. And of course,\nsome players only have one name. How many players scored 4 goals? We can  grep \nfor that, which will definitely work, but we can also sort the list easily using\nthe  sort  command.  Try it out!  $ cat copa_america_goals | sort  1 Alexis Sanchez\n1 Andrew Carrillo\n1 Christian Cueva\n1 Claudio Pizarro\n1 Cristian Rodriguez\n1 Derlis Gonzalez\n1 Douglas Costa\n1 Edgar Benitez\n1 Gary Medel\n1 Javier pastore\n1 Jeison Murillo\n1 Jose Gimenez\n1 Lionel Messi\n1 Marcos Rojo\n1 Martis Smedberg-Dalence\n1 Mauricio Isla\n1 Miku\n1 Nelson Haedo Valdez\n1 Neymar\n1 Roberto Firmino\n1 Robinho\n1 Ronald Raldes\n1 Salomon Rondon\n1 Thiago Silva\n2 Angel Di Maria\n2 Charles Aranguiz\n2 Enner Valencia\n2 Gonzalo Higuain\n2 Marcelo Martins Moreno\n2 Matias Vuoso\n2 Miller Bolanos\n2 Raul Jimenez\n3 Arturo Vidal\n3 Lucas Barrios\n3 Sergio Aguero\n4 Eduardo Vargas\n4 Paolo Guerrero  And we see that at the bottom of the sorted list there are two players who\nscored 4 goals in the Copa.   Now, sorting goal scorers by last name seems a little strange if we care about\nthe number of goals scored. Let's save the list of goals but sort it by the\nnumber of goals. How should we do that?  $ cat copa_america_goals | sort > copa_goals_sorted  And remember, there's no output to the screen ( stdout ) because we  redirected \nit to a new file. We can  cat  the new file to make sure it worked as we expect.  $ cat copa_goals_sorted   1 Alexis Sanchez\n1 Andrew Carrillo\n1 Christian Cueva\n1 Claudio Pizarro\n1 Cristian Rodriguez\n1 Derlis Gonzalez\n1 Douglas Costa\n1 Edgar Benitez\n1 Gary Medel\n1 Javier pastore\n1 Jeison Murillo\n1 Jose Gimenez\n1 Lionel Messi\n1 Marcos Rojo\n1 Martis Smedberg-Dalence\n1 Mauricio Isla\n1 Miku\n1 Nelson Haedo Valdez\n1 Neymar\n1 Roberto Firmino\n1 Robinho\n1 Ronald Raldes\n1 Salomon Rondon\n1 Thiago Silva\n2 Angel Di Maria\n2 Charles Aranguiz\n2 Enner Valencia\n2 Gonzalo Higuain\n2 Marcelo Martins Moreno\n2 Matias Vuoso\n2 Miller Bolanos\n2 Raul Jimenez\n3 Arturo Vidal\n3 Lucas Barrios\n3 Sergio Aguero\n4 Eduardo Vargas\n4 Paolo Guerrero  $ cat copa_goals_sorted | grep Alexis  1 Alexis Sanchez",
            "title": "sort"
        },
        {
            "location": "/nix/man_and_less/",
            "text": "man\n pages\n\n\nEarlier we used the command \nls --help\n to display the long list of options that\n\nls\n supports. The more common way to look up how to use a program is to look at\nits \"man page\", short for manual.\n\n\nTry opening the \nman\n page for \nls\n\n\nman ls\n\n\n\n\nNotice that instead of dumping all of that information to the screen (STDOUT),\nwe instead end up in something called a \"pager\". You can move up and down in the\n\nman\n page using the arrow keys. When you want to quit the \nman\n page, just hit\n\nq\n.\n\n\nless\n\n\nWe just used \nless\n. When you open a \nman\n page, it opens up in \nless\n, which is\na \"pager\".\n\n\nA pager is a program used to view but not edit a text file or text stream. Why\nuse a pager instead of a text editor? Sometimes you don't \nwant\n to be able to\nedit a file (changing \nman\n pages is a bad idea). It's also much faster when\ndealing with a very large file. A pager can begin to display the beginning of a\nfile while the rest of it is being loaded in to memory. A text editor has to\nfirst load an entire file before it can display any part of it.\n\n\nWe'll look at a few more ways that \nless\n can help you in the exercises in the next section. First, let's take a quick pass through the movement commands that \nless\n supports.\n\n\nTo do that, we can open up another \nman\n page. How about \nsort\n?\n\n\nman sort\n\n\n\n\nvi\n movement commands\n\n\nvi\n or \nvim\n is a popular and powerful command line text editor. It's also\nnotoriously difficult for beginners. It's too much to try to learn \nvi\n on top\nof everything else we're going to look at, but we do need to look at a few \nvi\n\ncommands.\n\n\nWhy? Because a lot of *nix programs inherited parts of their interface from \nvi\n\nand you'll need to know how to interact with them. \nless\n is just one of those\nprograms.\n\n\n\n\n\n\n\n\nCommand\n\n\nAction\n\n\n\n\n\n\n\n\n\n\nj or Down Arrow\n\n\nDown\n\n\n\n\n\n\nk or Up Arrow\n\n\nUp\n\n\n\n\n\n\nq\n\n\nQuit\n\n\n\n\n\n\ng or <\n\n\nGo to top\n\n\n\n\n\n\nG or >\n\n\nGo to bottom\n\n\n\n\n\n\n/\n\n\nSearch for",
            "title": "man and less"
        },
        {
            "location": "/nix/man_and_less/#man-pages",
            "text": "Earlier we used the command  ls --help  to display the long list of options that ls  supports. The more common way to look up how to use a program is to look at\nits \"man page\", short for manual.  Try opening the  man  page for  ls  man ls  Notice that instead of dumping all of that information to the screen (STDOUT),\nwe instead end up in something called a \"pager\". You can move up and down in the man  page using the arrow keys. When you want to quit the  man  page, just hit q .",
            "title": "man pages"
        },
        {
            "location": "/nix/man_and_less/#less",
            "text": "We just used  less . When you open a  man  page, it opens up in  less , which is\na \"pager\".  A pager is a program used to view but not edit a text file or text stream. Why\nuse a pager instead of a text editor? Sometimes you don't  want  to be able to\nedit a file (changing  man  pages is a bad idea). It's also much faster when\ndealing with a very large file. A pager can begin to display the beginning of a\nfile while the rest of it is being loaded in to memory. A text editor has to\nfirst load an entire file before it can display any part of it.  We'll look at a few more ways that  less  can help you in the exercises in the next section. First, let's take a quick pass through the movement commands that  less  supports.  To do that, we can open up another  man  page. How about  sort ?  man sort",
            "title": "less"
        },
        {
            "location": "/nix/man_and_less/#vi-movement-commands",
            "text": "vi  or  vim  is a popular and powerful command line text editor. It's also\nnotoriously difficult for beginners. It's too much to try to learn  vi  on top\nof everything else we're going to look at, but we do need to look at a few  vi \ncommands.  Why? Because a lot of *nix programs inherited parts of their interface from  vi \nand you'll need to know how to interact with them.  less  is just one of those\nprograms.     Command  Action      j or Down Arrow  Down    k or Up Arrow  Up    q  Quit    g or <  Go to top    G or >  Go to bottom    /  Search for",
            "title": "vi movement commands"
        },
        {
            "location": "/nix/uber/",
            "text": "uber example\n\n\nYou downloaded a zip file at the beginning of the workshop. Time to use it!\n\n\nFirst, navigate to the \nDownloads\n folder.\n\n\nWe need to unzip the zip file -- any guesses about what command unzips files?\n\n\nunzip uber-trip-data-master.zip\n\n\n\n\nBy default, \nunzip\n will create a folder with the same name as the \nzip\n file\n(without the \n.zip\n part). If you want to unzip the file to a different\nlocation, you can use the \n-d\n flag.\n\n\nFor example:\n\n\nunzip uber-trip-data-master.zip -d ~/uber-trip-data\n\n\n\n\nNow you should have a folder called \nuber-trip-data\n or something similar,\ndepending on where and how you unzipped it. \ncd\n to that directory and take a\nlook.\n\n\n$ cd uber-trip-data/\n\n\n\n\n$ ls\n\n\n\n\ntaxi-zone-lookup.csv     uber-raw-data-janjune-15.csv.zip  uber-raw-data-may14.csv\nuber-raw-data-apr14.csv  uber-raw-data-jul14.csv           uber-raw-data-sep14.csv\nuber-raw-data-aug14.csv  uber-raw-data-jun14.csv\n\n\n\n\nWe have several raw data files (so named). \ncsv\n files divided by month and also\na zip file. We'll leave the zip file alone for the moment. What questions can we\nanswer about this dataset from the command line?\n\n\nFirst, it would help to know what these files contain. We can use \nhead\n to\ndisplay the first ten lines of one of the raw files.\n\n\n$ head uber-raw-data-apr14.csv\n\n\n\n\n\"Date/Time\",\"Lat\",\"Lon\",\"Base\"\n\"4/1/2014 0:11:00\",40.769,-73.9549,\"B02512\"\n\"4/1/2014 0:17:00\",40.7267,-74.0345,\"B02512\"\n\"4/1/2014 0:21:00\",40.7316,-73.9873,\"B02512\"\n\"4/1/2014 0:28:00\",40.7588,-73.9776,\"B02512\"\n\"4/1/2014 0:33:00\",40.7594,-73.9722,\"B02512\"\n\"4/1/2014 0:33:00\",40.7383,-74.0403,\"B02512\"\n\"4/1/2014 0:39:00\",40.7223,-73.9887,\"B02512\"\n\"4/1/2014 0:45:00\",40.762,-73.979,\"B02512\"\n\"4/1/2014 0:55:00\",40.7524,-73.996,\"B02512\"\n\n\n\n\nThey weren't kidding when they said \"raw\" data. What do we have here? Looks like\na date and time (these are pickup times), the lat-long of the pickup location\nand some \"Base\" id that we don't care about right now.\n\n\nWhat can we take away from this?\n\n\nFor one, that the number of lines in this file is how many pickups Uber made\nduring the month of April, 2014. That sounds like interesting information. We\ncan use \nwc\n or \nwordcount\n to count the number of \nlines\n in a file by using\nthe \n-l\n flag.\n\n\n$ wc -l uber-raw-data-apr14.csv\n\n\n\n\n564517 uber-raw-data-apr14.csv\n\n\n\n\nWe know now that Uber provided more than half a million rides in New York City\nin April of 2014. Let's take a look at how that figure changes month-to-month!\n\n\nWe can run the same \nwc\n command but now use the \n*\n wildcard to get the\nlinecount of every file in the directory.\n\n\n$ wc -l *\n\n\n\n\n        0 taxi-zone-lookup.csv\n   564517 uber-raw-data-apr14.csv\n   829276 uber-raw-data-aug14.csv\n   284595 uber-raw-data-janjune-15.csv.zip\n   796122 uber-raw-data-jul14.csv\n   663845 uber-raw-data-jun14.csv\n   652436 uber-raw-data-may14.csv\n  1028137 uber-raw-data-sep14.csv\n  4818928 total\n\n\n\n\nCool, but the linecount of a zip file doesn't really make any sense. To be a\nlittle more specific, restrict \nwc\n to only look at \ncsv\n files.\n\n\n$ wc -l *.csv\n\n\n\n\n        0 taxi-zone-lookup.csv\n   564517 uber-raw-data-apr14.csv\n   829276 uber-raw-data-aug14.csv\n   796122 uber-raw-data-jul14.csv\n   663845 uber-raw-data-jun14.csv\n   652436 uber-raw-data-may14.csv\n  1028137 uber-raw-data-sep14.csv\n  4534333 total\n\n\n\n\nBetter. \nwc\n outputs files in the order it gets them, which in this case is the\norder they exist in the directory. And that's alphabetical order. The names of\nmonths aren't hugely useful when sorting alphabetically.\n\n\nLet's use the \nsort\n command to sort the results from \nwc\n. We can pipe the\noutput of \nwc\n to \nsort\n using the \n|\n character. Remember, the pipe takes the\noutput from the previous command and hands it off to the following command.\n\n\n$ wc -l *.csv | sort\n\n\n\n\n        0 taxi-zone-lookup.csv\n  1028137 uber-raw-data-sep14.csv\n  4534333 total\n   564517 uber-raw-data-apr14.csv\n   652436 uber-raw-data-may14.csv\n   663845 uber-raw-data-jun14.csv\n   796122 uber-raw-data-jul14.csv\n   829276 uber-raw-data-aug14.csv\n\n\n\n\nHmmm.  That looks a little funny.  Can you see what \nsort\n did?\n\n\nYeah, it sorted things alphanumerically, which isn't helpful since it only looks\nat leading digits. We want to use the number of lines in each file as the\nsorting criteria.\n\n\nTo do this, we can use the \n-n\n flag with \nsort\n to specify a \"numerical\" sort.\n\n\nThis may start to look a little confusing, but remember, we're just building up\na command using smaller commands. We use the \n-l\n flag with \nwc\n to count the\nnumber of lines, then pipe that output to \nsort\n where we use the \n-n\n flag to\nrequire numerical sorting.\n\n\n$ wc -l *.csv | sort -n\n\n\n\n\n        0 taxi-zone-lookup.csv\n   564517 uber-raw-data-apr14.csv\n   652436 uber-raw-data-may14.csv\n   663845 uber-raw-data-jun14.csv\n   796122 uber-raw-data-jul14.csv\n   829276 uber-raw-data-aug14.csv\n  1028137 uber-raw-data-sep14.csv\n  4534333 total\n\n\n\n\nOk! Now we have the raw data files sorted by number of lines, which we know is\nequivalent to number of rides. And look, now the months are in the correct\norder. That wasn't necessarily expected, but looking at this output we can see\nthat Uber is expanding at a pretty fast pace in 2014; they nearly doubled their\nusage numbers in 5 months!\n\n\nThis data set is missing the last quarter of 2014, but we have the first half of\n2015 available, so we can check if the trend continues (is there a ceiling for\nUber requests in NYC?)\n\n\nFirst, unzip the file containing the 2015 data.\n\n\n$ unzip uber-raw-data-janjune-15.csv.zip\n\n\n\n\nArchive:  uber-raw-data-janjune-15.csv.zip\n  inflating: uber-raw-data-janjune-15.csv\n   creating: __MACOSX/\n  inflating: __MACOSX/._uber-raw-data-janjune-15.csv\n\n\n\n\nNow we know that whoever created this zip file uses a Mac. But that's not really\nimportant. Let's take another look at the line counts.\n\n\n$ wc -l *.csv | sort -n\n\n\n\n\n        0 taxi-zone-lookup.csv\n   564517 uber-raw-data-apr14.csv\n   652436 uber-raw-data-may14.csv\n   663845 uber-raw-data-jun14.csv\n   796122 uber-raw-data-jul14.csv\n   829276 uber-raw-data-aug14.csv\n  1028137 uber-raw-data-sep14.csv\n 14270480 uber-raw-data-janjune-15.csv\n 18804813 total\n\n\n\n\nWow! 14+ million rides! Impressive! But this data layout is different from the\n2014 data. The six months are all in the same file. Not cool. So what now?\n\n\nFirst, let's see what the data looks like in the combined file.\n\n\n$ head uber-raw-data-janjune-15.csv\n\n\n\n\nDispatching_base_num,Pickup_date,Affiliated_base_num,locationID\nB02617,2015-05-17 09:47:00,B02617,141\nB02617,2015-05-17 09:47:00,B02617,65\nB02617,2015-05-17 09:47:00,B02617,100\nB02617,2015-05-17 09:47:00,B02774,80\nB02617,2015-05-17 09:47:00,B02617,90\nB02617,2015-05-17 09:47:00,B02617,228\nB02617,2015-05-17 09:47:00,B02617,7\nB02617,2015-05-17 09:47:00,B02764,74\nB02617,2015-05-17 09:47:00,B02617,249\n\n\n\n\nVery uncool. First, the data is in a different format than the previous files we\nlooked at. Worse, the first pickup listed is in May? Either the file is\nmislabeled (bad) or it isn't even sorted (bad).\n\n\nLet's look at a few more lines to see if we can figure out which bad scenario we\nhave.\n\n\nWe can use \ntail\n to peek at the \nlast\n 10 lines in the file. How do those look?\n\n\n$ tail uber-raw-data-janjune-15.csv\n\n\n\n\nB02765,2015-05-08 15:42:00,B02764,79\nB02765,2015-05-08 15:42:00,B02765,37\nB02765,2015-05-08 15:42:00,B02765,161\nB02765,2015-05-08 15:42:00,B02765,7\nB02765,2015-05-08 15:43:00,B02711,25\nB02765,2015-05-08 15:43:00,B02765,186\nB02765,2015-05-08 15:43:00,B02765,263\nB02765,2015-05-08 15:43:00,B02765,90\nB02765,2015-05-08 15:44:00,B01899,45\nB02765,2015-05-08 15:44:00,B02682,144\n\n\n\n\nNot looking good.  Is this all just in May?  Let's look through a larger number of lines using \nhead\n and see if we can find a ride that wasn't in May.  Use the \n-n\n flag with \nhead\n to specify the number of lines to show (the default is 10).\n\n\n$ head -n 500 uber-raw-data-janjune-15.csv\n\n\n\n\n[snip]\nB02598,2015-01-18 11:06:58,B02598,7\nB02598,2015-01-18 18:55:46,B02598,141\nB02598,2015-01-18 14:54:28,B02598,249\nB02598,2015-01-18 20:48:57,B02598,90\nB02598,2015-01-18 09:28:20,B02682,234\nB02598,2015-01-18 19:31:14,B02764,13\nB02598,2015-01-18 14:13:38,B02598,163\nB02598,2015-01-18 22:53:57,B02598,90\nB02598,2015-01-18 19:13:00,B02617,246\nB02598,2015-01-18 14:53:36,B02598,161\nB02598,2015-01-18 02:37:00,B02598,114\nB02598,2015-01-18 18:47:01,B02598,113\nB02598,2015-01-18 16:06:11,B02598,233\nB02598,2015-01-18 15:36:12,B02598,162\nB02598,2015-01-18 02:10:39,B02598,50\nB02598,2015-01-18 12:18:57,B02764,142\nB02598,2015-01-18 14:03:01,B02598,37\n\n\n\n\nOk. 500 lines in, we can see some January pickups. It looks like we have bad\noption #2. The data is labeled correctly but isn't sorted. Time to sort it!\n\n\nSorting can be expensive, so rather than trying to sort the whole file at once,\nlet's copy a portion of the big file into a separate file.\n\n\nUse the same \nhead\n command we just used, but now, instead of writing it to the\nscreen, we can \nredirect\n that output to another file using \n>\n. We'll just call\nthat file \ntest.csv\n.\n\n\n$ head -n 500 uber-raw-data-janjune-15.csv > test.csv\n\n\n\n\nNow it's time to figure out how to sort this data. We can use \nsort\n the way we\ndid with \nwc\n because the information we want to use as the sort key (the date\nand time) are embedded in the middle of every line.\n\n\nHere's one line from \ntest.csv\n:\n\n\nB02598,2015-01-18 14:03:01,B02598,37\n\n\nWe already looked at using \nsort\n with \nfields\n and the \n-k\n flag. Let's try it\nhere:\n\n\nWe're going to \ncat\n all of \ntest.csv\n, pipe that into \nsort\n and then use the\n\n-k2\n flag, which will sort the lines of \ntest.csv\n based on the first character\nof the \nsecond\n field/column.\n\n\n$ cat test.csv | sort -k2 | less\n\n\n\n\nThat... didn't work. The default field delimiter in \nsort\n is whitespace, so the\nprevious command sorted everything based on pickup time, but ignored pickup\ndate.\n\n\nHow can we change the delimiter character that \nsort\n uses? Let's check the\n\nman\n page.\n\n\n$ man sort\n\n\n\n\n$ cat test.csv | sort -t \",\" -k2 | less\n\n\n\n\n$ cat test.csv | sort -t \",\" -k2 > test_sort.csv\n\n\n\n\n$ head test_sort.csv\n\n\n\n\nB02598,2015-01-18 00:02:54,,144\n\nB02598,2015-01-18 00:05:05,B02598,50\n\nB02598,2015-01-18 00:06:19,B02598,107\n\nB02598,2015-01-18 00:08:14,B02598,142\n\nB02598,2015-01-18 00:16:58,B02598,107\n\nB02598,2015-01-18 00:30:59,B02598,50\n\nB02598,2015-01-18 00:36:16,B02598,211\n\nB02598,2015-01-18 00:37:16,B02774,141\n\nB02598,2015-01-18 00:45:16,,48\n\nB02598,2015-01-18 00:47:08,B02617,68",
            "title": "Uber exercise"
        },
        {
            "location": "/nix/uber/#uber-example",
            "text": "You downloaded a zip file at the beginning of the workshop. Time to use it!  First, navigate to the  Downloads  folder.  We need to unzip the zip file -- any guesses about what command unzips files?  unzip uber-trip-data-master.zip  By default,  unzip  will create a folder with the same name as the  zip  file\n(without the  .zip  part). If you want to unzip the file to a different\nlocation, you can use the  -d  flag.  For example:  unzip uber-trip-data-master.zip -d ~/uber-trip-data  Now you should have a folder called  uber-trip-data  or something similar,\ndepending on where and how you unzipped it.  cd  to that directory and take a\nlook.  $ cd uber-trip-data/  $ ls  taxi-zone-lookup.csv     uber-raw-data-janjune-15.csv.zip  uber-raw-data-may14.csv\nuber-raw-data-apr14.csv  uber-raw-data-jul14.csv           uber-raw-data-sep14.csv\nuber-raw-data-aug14.csv  uber-raw-data-jun14.csv  We have several raw data files (so named).  csv  files divided by month and also\na zip file. We'll leave the zip file alone for the moment. What questions can we\nanswer about this dataset from the command line?  First, it would help to know what these files contain. We can use  head  to\ndisplay the first ten lines of one of the raw files.  $ head uber-raw-data-apr14.csv  \"Date/Time\",\"Lat\",\"Lon\",\"Base\"\n\"4/1/2014 0:11:00\",40.769,-73.9549,\"B02512\"\n\"4/1/2014 0:17:00\",40.7267,-74.0345,\"B02512\"\n\"4/1/2014 0:21:00\",40.7316,-73.9873,\"B02512\"\n\"4/1/2014 0:28:00\",40.7588,-73.9776,\"B02512\"\n\"4/1/2014 0:33:00\",40.7594,-73.9722,\"B02512\"\n\"4/1/2014 0:33:00\",40.7383,-74.0403,\"B02512\"\n\"4/1/2014 0:39:00\",40.7223,-73.9887,\"B02512\"\n\"4/1/2014 0:45:00\",40.762,-73.979,\"B02512\"\n\"4/1/2014 0:55:00\",40.7524,-73.996,\"B02512\"  They weren't kidding when they said \"raw\" data. What do we have here? Looks like\na date and time (these are pickup times), the lat-long of the pickup location\nand some \"Base\" id that we don't care about right now.  What can we take away from this?  For one, that the number of lines in this file is how many pickups Uber made\nduring the month of April, 2014. That sounds like interesting information. We\ncan use  wc  or  wordcount  to count the number of  lines  in a file by using\nthe  -l  flag.  $ wc -l uber-raw-data-apr14.csv  564517 uber-raw-data-apr14.csv  We know now that Uber provided more than half a million rides in New York City\nin April of 2014. Let's take a look at how that figure changes month-to-month!  We can run the same  wc  command but now use the  *  wildcard to get the\nlinecount of every file in the directory.  $ wc -l *          0 taxi-zone-lookup.csv\n   564517 uber-raw-data-apr14.csv\n   829276 uber-raw-data-aug14.csv\n   284595 uber-raw-data-janjune-15.csv.zip\n   796122 uber-raw-data-jul14.csv\n   663845 uber-raw-data-jun14.csv\n   652436 uber-raw-data-may14.csv\n  1028137 uber-raw-data-sep14.csv\n  4818928 total  Cool, but the linecount of a zip file doesn't really make any sense. To be a\nlittle more specific, restrict  wc  to only look at  csv  files.  $ wc -l *.csv          0 taxi-zone-lookup.csv\n   564517 uber-raw-data-apr14.csv\n   829276 uber-raw-data-aug14.csv\n   796122 uber-raw-data-jul14.csv\n   663845 uber-raw-data-jun14.csv\n   652436 uber-raw-data-may14.csv\n  1028137 uber-raw-data-sep14.csv\n  4534333 total  Better.  wc  outputs files in the order it gets them, which in this case is the\norder they exist in the directory. And that's alphabetical order. The names of\nmonths aren't hugely useful when sorting alphabetically.  Let's use the  sort  command to sort the results from  wc . We can pipe the\noutput of  wc  to  sort  using the  |  character. Remember, the pipe takes the\noutput from the previous command and hands it off to the following command.  $ wc -l *.csv | sort          0 taxi-zone-lookup.csv\n  1028137 uber-raw-data-sep14.csv\n  4534333 total\n   564517 uber-raw-data-apr14.csv\n   652436 uber-raw-data-may14.csv\n   663845 uber-raw-data-jun14.csv\n   796122 uber-raw-data-jul14.csv\n   829276 uber-raw-data-aug14.csv  Hmmm.  That looks a little funny.  Can you see what  sort  did?  Yeah, it sorted things alphanumerically, which isn't helpful since it only looks\nat leading digits. We want to use the number of lines in each file as the\nsorting criteria.  To do this, we can use the  -n  flag with  sort  to specify a \"numerical\" sort.  This may start to look a little confusing, but remember, we're just building up\na command using smaller commands. We use the  -l  flag with  wc  to count the\nnumber of lines, then pipe that output to  sort  where we use the  -n  flag to\nrequire numerical sorting.  $ wc -l *.csv | sort -n          0 taxi-zone-lookup.csv\n   564517 uber-raw-data-apr14.csv\n   652436 uber-raw-data-may14.csv\n   663845 uber-raw-data-jun14.csv\n   796122 uber-raw-data-jul14.csv\n   829276 uber-raw-data-aug14.csv\n  1028137 uber-raw-data-sep14.csv\n  4534333 total  Ok! Now we have the raw data files sorted by number of lines, which we know is\nequivalent to number of rides. And look, now the months are in the correct\norder. That wasn't necessarily expected, but looking at this output we can see\nthat Uber is expanding at a pretty fast pace in 2014; they nearly doubled their\nusage numbers in 5 months!  This data set is missing the last quarter of 2014, but we have the first half of\n2015 available, so we can check if the trend continues (is there a ceiling for\nUber requests in NYC?)  First, unzip the file containing the 2015 data.  $ unzip uber-raw-data-janjune-15.csv.zip  Archive:  uber-raw-data-janjune-15.csv.zip\n  inflating: uber-raw-data-janjune-15.csv\n   creating: __MACOSX/\n  inflating: __MACOSX/._uber-raw-data-janjune-15.csv  Now we know that whoever created this zip file uses a Mac. But that's not really\nimportant. Let's take another look at the line counts.  $ wc -l *.csv | sort -n          0 taxi-zone-lookup.csv\n   564517 uber-raw-data-apr14.csv\n   652436 uber-raw-data-may14.csv\n   663845 uber-raw-data-jun14.csv\n   796122 uber-raw-data-jul14.csv\n   829276 uber-raw-data-aug14.csv\n  1028137 uber-raw-data-sep14.csv\n 14270480 uber-raw-data-janjune-15.csv\n 18804813 total  Wow! 14+ million rides! Impressive! But this data layout is different from the\n2014 data. The six months are all in the same file. Not cool. So what now?  First, let's see what the data looks like in the combined file.  $ head uber-raw-data-janjune-15.csv  Dispatching_base_num,Pickup_date,Affiliated_base_num,locationID\nB02617,2015-05-17 09:47:00,B02617,141\nB02617,2015-05-17 09:47:00,B02617,65\nB02617,2015-05-17 09:47:00,B02617,100\nB02617,2015-05-17 09:47:00,B02774,80\nB02617,2015-05-17 09:47:00,B02617,90\nB02617,2015-05-17 09:47:00,B02617,228\nB02617,2015-05-17 09:47:00,B02617,7\nB02617,2015-05-17 09:47:00,B02764,74\nB02617,2015-05-17 09:47:00,B02617,249  Very uncool. First, the data is in a different format than the previous files we\nlooked at. Worse, the first pickup listed is in May? Either the file is\nmislabeled (bad) or it isn't even sorted (bad).  Let's look at a few more lines to see if we can figure out which bad scenario we\nhave.  We can use  tail  to peek at the  last  10 lines in the file. How do those look?  $ tail uber-raw-data-janjune-15.csv  B02765,2015-05-08 15:42:00,B02764,79\nB02765,2015-05-08 15:42:00,B02765,37\nB02765,2015-05-08 15:42:00,B02765,161\nB02765,2015-05-08 15:42:00,B02765,7\nB02765,2015-05-08 15:43:00,B02711,25\nB02765,2015-05-08 15:43:00,B02765,186\nB02765,2015-05-08 15:43:00,B02765,263\nB02765,2015-05-08 15:43:00,B02765,90\nB02765,2015-05-08 15:44:00,B01899,45\nB02765,2015-05-08 15:44:00,B02682,144  Not looking good.  Is this all just in May?  Let's look through a larger number of lines using  head  and see if we can find a ride that wasn't in May.  Use the  -n  flag with  head  to specify the number of lines to show (the default is 10).  $ head -n 500 uber-raw-data-janjune-15.csv  [snip]\nB02598,2015-01-18 11:06:58,B02598,7\nB02598,2015-01-18 18:55:46,B02598,141\nB02598,2015-01-18 14:54:28,B02598,249\nB02598,2015-01-18 20:48:57,B02598,90\nB02598,2015-01-18 09:28:20,B02682,234\nB02598,2015-01-18 19:31:14,B02764,13\nB02598,2015-01-18 14:13:38,B02598,163\nB02598,2015-01-18 22:53:57,B02598,90\nB02598,2015-01-18 19:13:00,B02617,246\nB02598,2015-01-18 14:53:36,B02598,161\nB02598,2015-01-18 02:37:00,B02598,114\nB02598,2015-01-18 18:47:01,B02598,113\nB02598,2015-01-18 16:06:11,B02598,233\nB02598,2015-01-18 15:36:12,B02598,162\nB02598,2015-01-18 02:10:39,B02598,50\nB02598,2015-01-18 12:18:57,B02764,142\nB02598,2015-01-18 14:03:01,B02598,37  Ok. 500 lines in, we can see some January pickups. It looks like we have bad\noption #2. The data is labeled correctly but isn't sorted. Time to sort it!  Sorting can be expensive, so rather than trying to sort the whole file at once,\nlet's copy a portion of the big file into a separate file.  Use the same  head  command we just used, but now, instead of writing it to the\nscreen, we can  redirect  that output to another file using  > . We'll just call\nthat file  test.csv .  $ head -n 500 uber-raw-data-janjune-15.csv > test.csv  Now it's time to figure out how to sort this data. We can use  sort  the way we\ndid with  wc  because the information we want to use as the sort key (the date\nand time) are embedded in the middle of every line.  Here's one line from  test.csv :  B02598,2015-01-18 14:03:01,B02598,37  We already looked at using  sort  with  fields  and the  -k  flag. Let's try it\nhere:  We're going to  cat  all of  test.csv , pipe that into  sort  and then use the -k2  flag, which will sort the lines of  test.csv  based on the first character\nof the  second  field/column.  $ cat test.csv | sort -k2 | less  That... didn't work. The default field delimiter in  sort  is whitespace, so the\nprevious command sorted everything based on pickup time, but ignored pickup\ndate.  How can we change the delimiter character that  sort  uses? Let's check the man  page.  $ man sort  $ cat test.csv | sort -t \",\" -k2 | less  $ cat test.csv | sort -t \",\" -k2 > test_sort.csv  $ head test_sort.csv  B02598,2015-01-18 00:02:54,,144\n\nB02598,2015-01-18 00:05:05,B02598,50\n\nB02598,2015-01-18 00:06:19,B02598,107\n\nB02598,2015-01-18 00:08:14,B02598,142\n\nB02598,2015-01-18 00:16:58,B02598,107\n\nB02598,2015-01-18 00:30:59,B02598,50\n\nB02598,2015-01-18 00:36:16,B02598,211\n\nB02598,2015-01-18 00:37:16,B02774,141\n\nB02598,2015-01-18 00:45:16,,48\n\nB02598,2015-01-18 00:47:08,B02617,68",
            "title": "uber example"
        },
        {
            "location": "/nix/unix_cheat_sheet/",
            "text": "Command\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nwhoami\n\n\nwho you are logged in as\n\n\n\n\n\n\npwd\n\n\nprint working directory\n\n\n\n\n\n\nls\n\n\nlist items in current directory\n\n\n\n\n\n\nls -F\n\n\nlist all items in current directory and show directories with a slash and executables with a star\n\n\n\n\n\n\nls -a\n\n\nlist all items in current directory, including hidden files\n\n\n\n\n\n\ncd\n dir\n\n\nchange directory to \ndir\n\n\n\n\n\n\ncd ..\n\n\ngo up one directory\n\n\n\n\n\n\ncd\n\n\ngo to your home directory\n\n\n\n\n\n\ncd ~\n\n\ngo to your home directory\n\n\n\n\n\n\ncd /\n\n\ngo to  your root directory\n\n\n\n\n\n\nnano\n\n\nopen a text editor. See ribbon at bottom for help. ^ means CTRL\n\n\n\n\n\n\nnano\n new.txt\n\n\nopen nano to edit a file called \nnew.txt\n\n\n\n\n\n\ntouch\n newfile\n\n\ncreate or updates \nnewfile\n\n\n\n\n\n\nmkdir\n newdir\n\n\nmake directory \nnewdir\n\n\n\n\n\n\nrm\n file\n\n\nremove \nfile\n\n\n\n\n\n\nrmdir\n dir\n\n\nremove directory \ndir\n if it is empty\n\n\n\n\n\n\nrm -r\n dir\n\n\nremove directory \ndir\n recursively\n\n\n\n\n\n\nmv\n file newname\n\n\nrename \nfile\n to \nnewname\n. If a file called \nnewname\n exists, this will overwrite it!\n\n\n\n\n\n\nmv\n file newlocdir\n\n\nmove \nfile\n into the destination directory \nnewlocdir\n\n\n\n\n\n\ncommand \n>\n file\n\n\nredirect the output of \ncommand\n writing it to \nfile\n\n\n\n\n\n\nwc\n file\n\n\ncount lines, words and characters in \nfile\n\n\n\n\n\n\nwc -l\n file\n\n\ncount lines in \nfile\n\n\n\n\n\n\nwc -w\n file\n\n\ncount words in \nfile\n\n\n\n\n\n\nwc -c\n file\n\n\ncount characters in \nfile\n\n\n\n\n\n\ncat\n file\n\n\ndisplays the content of \nfile\n\n\n\n\n\n\ncommand1 \n|\n command2\n\n\n\"pipe\" the output of \ncommand1\n to \ncommand2\n\n\n\n\n\n\ngrep\n pattern files\n\n\nsearch for \npattern\n in \nfiles\n\n\n\n\n\n\nwget\n file\n\n\ndownload \nfile\n\n\n\n\n\n\nsort\n file\n\n\nprint the lines of its input or concatenation of all files listed in its argument list in sorted order\n\n\n\n\n\n\nman\n command\n\n\nshow the manual for command\n\n\n\n\n\n\nless\n file\n\n\nview \nfile\n with page navigation\n\n\n\n\n\n\nhead\n file\n\n\noutput the first 10 lines of \nfile\n\n\n\n\n\n\ntail\n file\n\n\noutput the last 10 lines of \nfile",
            "title": "Cheat Sheet"
        },
        {
            "location": "/python/python/",
            "text": "Python (IPython)\n\n\nPython is a dynamic and high-level language that is easy to learn and fun to use. \n\n\nThe classic Hello World program is as simple as:\n\n\nprint(\"Hello World!!\")\n\n\nPython itself is an \ninterpreter\n, it translates Python \nsource code\n into\ninstructions that the computer can understand. It's a dynamic language (duck\ntyping), i.e you don't need a type to invoke an existing method on an object.\n\n\n\"If it looks like a duck, swims like a duck, and quacks like a duck, then it\nprobably is a duck.\"\n\n\nIf you come from different languages you probably code by doing scripts, with\nPython we can do the same but we have other tools that allows us to work\ninteractively.\n\n\nLet's work with one of these tools (IPython).\n\n\nType in a terminal :\n\n\nipython\n\n\nPython is awesome!\n\n\nimport this \nimport antigravity\n\n\n\n\nVariable assignment, types and  duck typing\n\n\nIn Python is perfectly legal to do:\n\n\nIn [1]: x = 3\n\n\n\n\nTry:\n\n\nIn [2]: x\nOut[2]: 3\n\n\n\n\nIn [3]: print(x)\n3\n\n\n\n\nIn [4]: type(x)\nOut[4]: int\n\n\n\n\nIn [5]: float(x)\nOut[5]: 3.0\n\n\n\n\nIn [6]: complex(x)\nOut[6]: (3+0j)\n\n\n\n\nIn [7]: z = complex(x)\nIn [8]: z = z + 1j\nIn [9]: print(z)\n(3+1j)\n\n\n\n\nRegular arithmetic symbols are preserved except for the \"power\" operator.\n\n\nFor example, raising \nx\n to the power of 2 would be:\n\n\nx**2\n\n\nWhat about strings?\n\n\nIn [9]: y = \"hello\"\n\nIn [10]: y\nOut[10]: 'hello'\n\nIn [11]: print(y)\nhello\n\nIn [12]: type(y)\nOut[12]: str\n\nIn [13]: s = \"world\"\n\n\n\n\nWhat if we try to \"sum\" strings? \n\n\nIn [14]: my_string = y+s\n\nIn [15]: print(my_string)\nhelloworld\n\n\n\n\nLet's add a space in between:\n\n\nIn [16]: my_string = y +' '+ s\n\nIn [17]: print(my_string)\nhello world\n\n\n\n\nWe can access the different elements of a string and slices of it, for example:\n\n\nIn [18]: my_string[0]\nOut[18]: 'h'\n\nIn [19]: my_string[8]\nOut[19]: 'r'\n\nIn [20]: my_string[-1]\nOut[20]: 'd'\n\nIn [21]: my_string[2:5]\nOut[21]: 'llo'\n\nIn [22]: my_string[2:]\nOut[22]: 'llo world'\n\nIn [23]: my_string[1:-1]\nOut[23]: 'ello worl'\n\n\n\n\n Note that the start index is inclusive and the end one is exclusive!!\n\n\nSlices also allow us to pick specific elements from specific slices, for example:\n\n\nIn [24]: my_string[::2]\nOut[24]: 'hlowrd'\n\nIn [25]: my_string[1:-1:2]\nOut[25]: 'el ol'\n\nIn [26]: my_string[2::3]\nOut[26]: 'l r'\n\nIn [27]: my_string[::-1]\nOut[27]: 'dlrow olleh'\n\n\n\n\n\nStrings have different methods that we can apply to them, for example:\n\n\nMake all uppercase:\n\n\nIn [28]: my_string.upper()\nOut[28]: 'HELLO WORLD'\n\n\n\n\nFind where a character is or starts:\n\n\nIn [29]: my_string.find('hello')\nOut[29]: 0\n\nIn [30]: my_string.find('l')\nOut[30]: 2\n\n\n\n\nIf you want to know all of the available methods for a certain object, there is\na simple command that will give you that information:\n\n\ndir(my_string)\n\n\nSpecial variables\n\n\nPython has special variables that are built into the language: namely \nTrue\n,\n\nFalse\n, \nNone\n and \nNotImplemented\n.\n\n\nBoolean variables\n\n\nTrue\n and \nFalse\n.\n\n\nIn general, if the value is zero or empty, then it's converted to \nFalse\n.\nOtherwise, it'll be converted to \nTrue\n.\n\n\nIn [31]: bool(0)\nOut[31]: False\n\nIn [32]: bool(\"Do we need oxygen?\")\nOut[32]: True\n\n\n\n\nIt also applies to logic statements, for example:\n\n\nIn [33]: x = 3\nIn [34]: y = 5\nIn [35]: z = x > y\n\nIn [36]: z\nOut[36]: False\n\nIn [37]: type(z)\nOut[37]: bool\n\n\n\n\n\nNone is not Zero\n\n\nIt is used to indicate that no value was given or that the behavior was\nundefined. This is different than zero, an empty string, or some other nil\nvalue.\n\n\nNotImplemented is not None\n\n\nNotImplemented\n is used to indicate that a behavior is not defined or that the\naction we are trying to execute is impossible. For example, \nNotImplemented\n is\nused under the covers when you try to divide a string by a float. We will end up\nwith a \nTypeError\n. (Try it on the shell)\n\n\nImportant notes\n:\n\n\n\n\n\n\nVariables names can be upper- or lower-case letters, and we can put digits\n  (0-9) and underscores. However, they can not start with a digit.\n\n\n\n\n\n\nThere are reserved words you can't use and you can find them in\n  this \nlink\n\n\n\n\n\n\nVariables are mutable.\n\n\n\n\n\n\nStandard data types: Numerical, String, \nList\n, Tuple, Dictionaries. (list\n  are the default in Python)\n\n\n\n\n\n\nNumerical types: \nint\n, \nlong\n (long integers), \nfloat\n and \ncomplex\n.\n\n\n\n\n\n\nData structures\n\n\nLists, dictionaries and tuples are kinds of \nCollections\n\n\n\n\nA \ncollection\n allows us to put many values in a single \"variable\".\n\n\nThey are convenient because we can carry many values around in one\n  convenient package.\n\n\nSimple variables are not collections.\n\n\n\n\nLists\n\n\n\n\nThe beginning and end of a list is denoted by square brackets \n[]\n and its\n  elements are separated by commas.\n\n\nA list element can be any Python object, even another list.\n\n\nList are MUTABLE, strings are NOT MUTABLE. (Note: there is a method called\n  replace for strings but it creates a copy)\n\n\n\n\nIn [38]: my_list = [2, 50, 4, 61]\nIn [39]: print(my_list)\n[2, 50, 4, 61]\n\nIn [40]: my_list[1] = 1\n\nIn [41]: print(my_list)\n[2, 1, 4, 61]                 #Now we have a 1 instead of a 50 in the second place.\n\n\n\n\n\n\n\n\nlen()\n function gives us the number of elements in the list.\n\n\n\n\n\n\nWe can \nconcatenate\n lists by using the operator \n+\n: \n\n\n\n\n\n\nIn [42]: lst1 = [0,1,2,3]\n\nIn [43]: lst2 = ['a','b','c']\n\nIn [44]: conc = lst1 + lst2\n\nIn [45]: print(conc)\n[0, 1, 2, 3, 'a', 'b', 'c']\n\n\n\n\n\n\n\nThere are different methods to apply to a list (check them by typing\n  \ndir(lst1)\n), an example is \nappend()\n.\n\n\n\n\nIn [46]: lst1.append(4)\n\nIn [47]: print(lst1)\n[0, 1, 2, 3, 4]\n\nIn [48]: lst1.append('five')\n\nIn [49]: lst1\nOut[49]: [0, 1, 2, 3, 4, 'five']\n\nIn [50]: lst1.append([6, 7, 8])\n\nIn [51]: lst1\nOut[51]: [0, 1, 2, 3, 4, 'five', [6, 7, 8]]\n\n\n\n\n\n\nSimilar to strings we can access elements of a list by doing \nlist[i]\n and\n  also we can do slices.\n\n\n\n\nIn [52]: lst1[-1]\nOut[52]: [6, 7, 8]\n\nIn [53]: lst1[5:]\nOut[53]: ['five', [6, 7, 8]]\n\n\n\n\n\n\nWe can check if something is or not in a list.\n\n\n\n\nIn [54]: my_numbers = [3, 17, 27, 19]\n\nIn [55]: 17 in my_numbers\nOut[55]: True\n\nIn [56]: 2 in my_numbers\nOut[56]: False\n\nIn [57]: 13 not in my_numbers\nOut[57]: True\n\n\n\n\nNote: This operators does NOT modify the list.\n\n\n\n\nA \nlist\n is an ordered sequence. We can change the order by sorting the list. \n\n\n\n\nIn [58]: names = ['Naty', 'Gil', 'Lorena', 'Chris']\n\nIn [59]: names.sort()\n\nIn [60]: print(names)\n['Chris', 'Gil', 'Lorena', 'Naty']\n\n\n\n\n\n\nStrings\n and \nlists\n. The \nsplit()\n function.\n\n\n\n\nIn [61]: string = 'Just three words'\n\nIn [62]: str_list = string.split()\n\nIn [63]: print(str_list)\n['Just', 'three', 'words']\n\n\n\n\nPython takes care of long spaces, for example:\n\n\nIn [64]: line = 'A lot of       space'\n\nIn [65]: stuff = line.split()\n\nIn [66]: print(stuff)\n['A', 'lot', 'of', 'space']\n\n\n\n\nIf we specify the delimiter (it can be a space \n' '\n, a \n;\n, a \n:\n, whatever you want).\n\n\nIn [67]: line = 'A lot of       space'\n\nIn [68]: stuff = line.split(' ')\n\nIn [69]: print(stuff)\n['A', 'lot', 'of', '', '', '', '', '', '', 'space']\n\n\n\n\n;\n and many other characters are not delimiters by default:\n\n\nIn [70]: s = 'a;b;c'\n\nIn [71]: thing  = s.split()\n\nIn [72]: print(thing)\na;b;c\n\nIn [73]: thing = s.split(';')\n\nIn [74]: print(thing)\n['a', 'b', 'c']\n\n\n\n\nDictionaries\n\n\n\n\nDictionaries are also a type of collections and they are MUTABLE too. \n\n\n\n\nDifference between lists and dictionaries:\n\n\n\n\nList: a linear collection of values that stay in order.\n\n\nDictionary: A \"bag\" of values, each with it's own label (key).\n\n\nDictionaries don't maintain order, we index the the things we put in the\n  dictionary with a \"look up\" tag. For example:\n\n\n\n\nIn [75]: bag = {}\n\nIn [76]: bag['money'] = 12\n\nIn [77]: bag['candy'] = 5 \n\nIn [78]: bag['tissues'] = 7 \n\nIn [79]: print(bag)\n{'tissues': 7, 'money': 12, 'candy': 5}\n\nIn [80]: print(bag['money'])\n12\n\nIn [81]: bag['money'] += 2\n\nIn [82]: print(bag)\n{'tissues': 7, 'money': 14, 'candy': 5}\n\n\n\n\n\n\n\nWe can use the operators \nin\n and \nnot in\n to check if a key is or not in a\n  dictionary.\n\n\n\n\nIn [83]: 'book' in bag\nOut[83]: False\n\nIn [84]: 'candy' in bag\nOut[84]: True\n\nIn [85]: 'cigarette' not in bag\nOut[85]: True\n\n\n\n\n\n\nRetrieving keys and values.\n\n\n\n\nIn [86]: ages = {'John': 30, 'Maria': 28, 'Lucas': 23}\n\nIn [87]: print(ages.keys())\ndict_keys(['Maria', 'Lucas', 'John'])\n\nIn [88]: print(ages.values())\ndict_values([28, 23, 30])\n\n\n\n\nNote\n: If we don't change the dictionary in between these operations, the keys\nand values are displayed in order.\n\n\n\n\nWe can get a list with pairs (key, value) by doing:\n\n\n\n\nIn [89]: print(ages.items())\ndict_items([('Maria', 28), ('Lucas', 23), ('John', 30)])\n\n\n\n\n\n\nDictionaries are good for counting how often we \"see\" something. \n\n\n\n\nThere are another two built in data structures: tuples and sets. You can read\nabout them in the\nPython\n\ndocumentation\n.\n\n\nA quick insight into them:\n\n\n\n\nTuples\n are like lists but they are IMMUTABLE, which saves time in\n  accessing memory. (Syntax \n(element1, element2, ...)\n)\n\n\nSets\n are collections that have no order, no duplicate elements and their\n  elements are \nhashable\n\n  which saves time when accessing memory. We can do operations like union,\n  intersection, difference, and symmetric difference. (Syntax \n{element1,\n  element2, ...}\n and an empty set is created by doing \nset()\n)\n\n\n\n\n Example: look for an element in a list and in a set (%timeit)\n\n\nIn [90]: rg = range(50000)\n\nIn [91]: lst = list(rg)\n\nIn [92]: %time (40035 in lst)\nCPU times: user 4 ms, sys: 0 ns, total: 4 ms\nWall time: 1.91 ms\nOut[92]: True\n\nIn [93]: st = set(rg)\n\nIn [94]: %time (40035 in st)\nCPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 14.3 \u00b5s\nOut[94]: True\n\nIn [95]: %time (51000 in lst)\nCPU times: user 4 ms, sys: 0 ns, total: 4 ms\nWall time: 2.36 ms\nOut[95]: False\n\nIn [96]: %time (51000 in st)\nCPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 12.9 \u00b5s\nOut[96]: False\n\n\n\n\nControl Flow\n\n\nConditionals\n\n\nThe syntax is \"If \nx\n true, then do something; otherwise, do something else.\"\n\n\nIf\n statement on its own:\n\n\nIn [97]: a = 8\n\nIn [98]: b = 3 \n\nIn [99]: if a > b :\n    ...:     print('a is bigger than b')\n    ...:\na is bigger than b\n\n\n\n\nIf-else\n statement: \n\n\nIn [100]: x = float(input('Insert your number: '))\nInsert your number: 711457\n\nIn [101]: if x % 17 == 0:\n     ...:     print('Your number is a multiple of 17.')\n     ...: else :\n     ...:     print('Your number is not a multiple of 17.')\n\nYour number is not a multiple of 17.\n\n\n\n\nIf-elif-else\n statement:\n\n\nIn [102]: a = 3\n\nIn [103]: b = 5\n\nIn [104]: if a > b:\n     ...:     print('a is bigger than b.')\n     ...: elif a < b:\n     ...:     print('a is smaller than b.')\n     ...: else:\n     ...:     print('a is equal to b.')\n     ...:\na is smaller than b.\n\n\n\n\nNote:\n We can have as many \nelif\n as we want.\n\n\nLoops\n\n\nLoops are useful for executing the same block multiple times. In Python we have\nmultiple looping formats: \nwhile\n loops, \nfor\n loops, and comprehensions.\n\n\nWhile loops\n\n\nThey are related to an \nif\n statement, they will compute \nwhile\n a condition is true.\n\n\nIn [105]: sec = 5\nIn [106]: while 0 < sec :\n     ...:     print('You have {} seconds'.format(sec))\n     ...:     sec -= 1\n     ...: print('Boom!!!')\n    ...:\nYou have 5 seconds\nYou have 4 seconds\nYou have 3 seconds\nYou have 2 seconds\nYou have 1 seconds\nBoom!!!\n\n\n\n\nNote:\n You have to be careful to not generate an infinite loop. \n\n\nFor loops\n\nIt is usually more useful to iterate over a certain group of things or an \"iterable\".\n\n\nIn [107]: for i in range(5,0,-1):\n    ...:     print('You have {} seconds'.format(i))\n    ...: print('Boom!!!')\n    ...: \nYou have 5 seconds\nYou have 4 seconds\nYou have 3 seconds\nYou have 2 seconds\nYou have 1 seconds\nBoom!!!\n\n\n\n\nAnother example:\n\n\nIn [108]: name = input('Insert your name: ')\nInsert your name: Naty\n\nIn [109]: name\nOut[109]: 'Naty'\n\nIn [110]: count = 1\n\nIn [111]: for letter in name:\n    ...:     print('The {} letter is {}'.format(count,letter))\n    ...:     count +=1\n    ...:\nThe 1 letter is N\nThe 2 letter is a\nThe 3 letter is t\nThe 4 letter is y\n\n\n\n\nWe can loop over dictionaries too:\n\n\nIn [112]: d = {\"name\":\"Lionel\", \"last_name\": \"Messi\", \"birthday\": [6, 24, 1987]}\n\nIn [113]: for item in d.items():\n     ...:     print(item)\n     ...:\n('birthday', [6, 24, 1987])\n('name', 'Lionel')\n('last_name', 'Messi')\n\nIn [114]: for key, value in d.items():\n     ...:     print(key, value)\n     ...:\nbirthday [6, 24, 1987]\nname Lionel\nlast_name Messi\n\n\n\n\nComprehensions\n\n\nfor\n and \nwhile\n loops are really useful but they take at least \"two lines\",\nand if you need to save the result of each loop in the iteration in a list, set,\netc. This takes at least \"three lines\". Thankfully, Python is so great that in\nsome cases we can reduce these cases to just ONE line! Let's see this with an\nexample:\n\n\nNormal loop:\n\n\nIn [114]: fruits = ['apple', 'orange', 'grape', 'banana', 'pineapple', 'strawberry', 'watermelon']\n\nIn [115]: upper_fruits = []\n\nIn [116]: for fruit in fruits:\n    ...:     upper_fruits.append(fruit.upper())\n\nIn [117]: upper_fruits\nOut[117]: ['APPLE', 'ORANGE', 'GRAPE', 'BANANA', 'PINEAPPLE', 'STRAWBERRY', 'WATERMELON']\n\n\n\n\nPythonic way:\n\n\nIn [118]: upper_pythonic = [fruit.upper() for fruit in fruits]\n\nIn [119]: upper_pythonic\nOut[119]: ['APPLE', 'ORANGE', 'GRAPE', 'BANANA', 'PINEAPPLE', 'STRAWBERRY', 'WATERMELON']\n\n\n\n\nWe can add a filter, for example:\n\n\nIn [120]: entries = [2, 11, 49, 3, 57, 33, 9]\n\nIn [121]: entries\nOut[121]: [2, 11, 49, 3, 57, 33, 9]\n\nIn [122]: some = [x**2 for x in entries if x%3==0]\n\nIn [123]: some\nOut[123]: [9, 3249, 1089, 81]\n\nIn [124]: orig = [int(x**(1/2)) for x in some]\n\nIn [125]: orig\nOut[125]: [3, 57, 33, 9]\n\n\n\n\nIPython Magics\n\n\nSometimes when we are working on IPython we would like to select certain lines\nand save them into a Python script. We can do that!\n\n\nWe can check the history of what we've done in the current session of IPython by\ntyping:\n\n\n%hist -n\n\n\nIf we want to select certain lines and put them into a script, the command\n\n%edit\n allow us to do this. Let's suppose we want lines 1, 3 and from 5-10; \nthen we should do:\n\n\n%edit 1 3 5-10\n\n\nand this will open the default editor with the lines we want. \n\n\nOnce we have that script saved if we want to run it we can use the command\n\n%run\n:\n\n\n%run my_script.py\n\n\nExercises\n\n\nSlicing\n \n\n\n1- Write code using \nfind()\n and string slicing to extract the number at the end\nof the line below. Convert the extracted value to a floating point number and\nprint it out. (Content taken from online course \"Python Data Structures\" by Dr.\nCharles Severance)\n\n\ntext = \"X-DSPAM-Confidence:    0.8475\"\n\n\n\n\nSolution\n\n\nSlicing and for loop\n \n\n\n2- Write a code that grabs, from each word, all the letters except 'ball' at the\nend of each one. Save the output in a list and print it out.\n\n\nThe original list is:\n\n\nsports = ['Football', 'Volleyball', 'Basketball', 'Baseball', 'Handball', 'Softball']\n\n\n\n\nAnd your output should look like:\n\n\n['Foot', 'Volley', 'Basket', 'Base', 'Hand', 'Soft']\n\n\n\n\nSolution\n\n\nDictionaries for counting\n\n\n3- Write a code that counts the frequency of each word that we have in the\nphrase provided.\n\n\nTips: split the text and put the words in a list, use a dictionary to count the\nrepetitions.\n\n\n(The idea of the following two exercises was inspired by content from the online\ncourse \"Python Data Structures\" by Dr. Charles Severance)\n\n\nhappy = \"I felt happy because I saw the others were happy and because I knew I should feel happy but I was not really happy\"\n\n\n\n\nYour output should look like:\n\n\n{'happy': 4, 'really': 1, 'and': 1, 'feel': 1, 'others': 1, 'felt': 1, 'not': 1, 'the': 1, 'should': 1, 'knew': 1, 'was': 1, 'saw': 1, 'I': 5, 'but': 1, 'were': 1, 'because': 2}\n\n\n\n\nRemember that dictionaries don't preserve order, therefore the items in your\noutput might be in a different order. However, you can still check if your\noutput is equal to the one we provide by doing a logical comparison using the\n\n==\n operation, which should return \nTrue\n if you get it right.\n\n\nOptional\n: Rewrite exercise 3 using the method \nget()\n. \n\n\nSolutions",
            "title": "Python"
        },
        {
            "location": "/python/python/#python-ipython",
            "text": "Python is a dynamic and high-level language that is easy to learn and fun to use.   The classic Hello World program is as simple as:  print(\"Hello World!!\")  Python itself is an  interpreter , it translates Python  source code  into\ninstructions that the computer can understand. It's a dynamic language (duck\ntyping), i.e you don't need a type to invoke an existing method on an object.  \"If it looks like a duck, swims like a duck, and quacks like a duck, then it\nprobably is a duck.\"  If you come from different languages you probably code by doing scripts, with\nPython we can do the same but we have other tools that allows us to work\ninteractively.  Let's work with one of these tools (IPython).  Type in a terminal :  ipython",
            "title": "Python (IPython)"
        },
        {
            "location": "/python/python/#python-is-awesome",
            "text": "import this \nimport antigravity",
            "title": "Python is awesome!"
        },
        {
            "location": "/python/python/#variable-assignment-types-and-duck-typing",
            "text": "In Python is perfectly legal to do:  In [1]: x = 3  Try:  In [2]: x\nOut[2]: 3  In [3]: print(x)\n3  In [4]: type(x)\nOut[4]: int  In [5]: float(x)\nOut[5]: 3.0  In [6]: complex(x)\nOut[6]: (3+0j)  In [7]: z = complex(x)\nIn [8]: z = z + 1j\nIn [9]: print(z)\n(3+1j)  Regular arithmetic symbols are preserved except for the \"power\" operator.  For example, raising  x  to the power of 2 would be:  x**2",
            "title": "Variable assignment, types and  duck typing"
        },
        {
            "location": "/python/python/#what-about-strings",
            "text": "In [9]: y = \"hello\"\n\nIn [10]: y\nOut[10]: 'hello'\n\nIn [11]: print(y)\nhello\n\nIn [12]: type(y)\nOut[12]: str\n\nIn [13]: s = \"world\"  What if we try to \"sum\" strings?   In [14]: my_string = y+s\n\nIn [15]: print(my_string)\nhelloworld  Let's add a space in between:  In [16]: my_string = y +' '+ s\n\nIn [17]: print(my_string)\nhello world  We can access the different elements of a string and slices of it, for example:  In [18]: my_string[0]\nOut[18]: 'h'\n\nIn [19]: my_string[8]\nOut[19]: 'r'\n\nIn [20]: my_string[-1]\nOut[20]: 'd'\n\nIn [21]: my_string[2:5]\nOut[21]: 'llo'\n\nIn [22]: my_string[2:]\nOut[22]: 'llo world'\n\nIn [23]: my_string[1:-1]\nOut[23]: 'ello worl'   Note that the start index is inclusive and the end one is exclusive!!  Slices also allow us to pick specific elements from specific slices, for example:  In [24]: my_string[::2]\nOut[24]: 'hlowrd'\n\nIn [25]: my_string[1:-1:2]\nOut[25]: 'el ol'\n\nIn [26]: my_string[2::3]\nOut[26]: 'l r'\n\nIn [27]: my_string[::-1]\nOut[27]: 'dlrow olleh'  Strings have different methods that we can apply to them, for example:  Make all uppercase:  In [28]: my_string.upper()\nOut[28]: 'HELLO WORLD'  Find where a character is or starts:  In [29]: my_string.find('hello')\nOut[29]: 0\n\nIn [30]: my_string.find('l')\nOut[30]: 2  If you want to know all of the available methods for a certain object, there is\na simple command that will give you that information:  dir(my_string)",
            "title": "What about strings?"
        },
        {
            "location": "/python/python/#special-variables",
            "text": "Python has special variables that are built into the language: namely  True , False ,  None  and  NotImplemented .  Boolean variables  True  and  False .  In general, if the value is zero or empty, then it's converted to  False .\nOtherwise, it'll be converted to  True .  In [31]: bool(0)\nOut[31]: False\n\nIn [32]: bool(\"Do we need oxygen?\")\nOut[32]: True  It also applies to logic statements, for example:  In [33]: x = 3\nIn [34]: y = 5\nIn [35]: z = x > y\n\nIn [36]: z\nOut[36]: False\n\nIn [37]: type(z)\nOut[37]: bool  None is not Zero  It is used to indicate that no value was given or that the behavior was\nundefined. This is different than zero, an empty string, or some other nil\nvalue.  NotImplemented is not None  NotImplemented  is used to indicate that a behavior is not defined or that the\naction we are trying to execute is impossible. For example,  NotImplemented  is\nused under the covers when you try to divide a string by a float. We will end up\nwith a  TypeError . (Try it on the shell)  Important notes :    Variables names can be upper- or lower-case letters, and we can put digits\n  (0-9) and underscores. However, they can not start with a digit.    There are reserved words you can't use and you can find them in\n  this  link    Variables are mutable.    Standard data types: Numerical, String,  List , Tuple, Dictionaries. (list\n  are the default in Python)    Numerical types:  int ,  long  (long integers),  float  and  complex .",
            "title": "Special variables"
        },
        {
            "location": "/python/python/#data-structures",
            "text": "Lists, dictionaries and tuples are kinds of  Collections   A  collection  allows us to put many values in a single \"variable\".  They are convenient because we can carry many values around in one\n  convenient package.  Simple variables are not collections.",
            "title": "Data structures"
        },
        {
            "location": "/python/python/#lists",
            "text": "The beginning and end of a list is denoted by square brackets  []  and its\n  elements are separated by commas.  A list element can be any Python object, even another list.  List are MUTABLE, strings are NOT MUTABLE. (Note: there is a method called\n  replace for strings but it creates a copy)   In [38]: my_list = [2, 50, 4, 61]\nIn [39]: print(my_list)\n[2, 50, 4, 61]\n\nIn [40]: my_list[1] = 1\n\nIn [41]: print(my_list)\n[2, 1, 4, 61]                 #Now we have a 1 instead of a 50 in the second place.    len()  function gives us the number of elements in the list.    We can  concatenate  lists by using the operator  + :     In [42]: lst1 = [0,1,2,3]\n\nIn [43]: lst2 = ['a','b','c']\n\nIn [44]: conc = lst1 + lst2\n\nIn [45]: print(conc)\n[0, 1, 2, 3, 'a', 'b', 'c']   There are different methods to apply to a list (check them by typing\n   dir(lst1) ), an example is  append() .   In [46]: lst1.append(4)\n\nIn [47]: print(lst1)\n[0, 1, 2, 3, 4]\n\nIn [48]: lst1.append('five')\n\nIn [49]: lst1\nOut[49]: [0, 1, 2, 3, 4, 'five']\n\nIn [50]: lst1.append([6, 7, 8])\n\nIn [51]: lst1\nOut[51]: [0, 1, 2, 3, 4, 'five', [6, 7, 8]]   Similar to strings we can access elements of a list by doing  list[i]  and\n  also we can do slices.   In [52]: lst1[-1]\nOut[52]: [6, 7, 8]\n\nIn [53]: lst1[5:]\nOut[53]: ['five', [6, 7, 8]]   We can check if something is or not in a list.   In [54]: my_numbers = [3, 17, 27, 19]\n\nIn [55]: 17 in my_numbers\nOut[55]: True\n\nIn [56]: 2 in my_numbers\nOut[56]: False\n\nIn [57]: 13 not in my_numbers\nOut[57]: True  Note: This operators does NOT modify the list.   A  list  is an ordered sequence. We can change the order by sorting the list.    In [58]: names = ['Naty', 'Gil', 'Lorena', 'Chris']\n\nIn [59]: names.sort()\n\nIn [60]: print(names)\n['Chris', 'Gil', 'Lorena', 'Naty']   Strings  and  lists . The  split()  function.   In [61]: string = 'Just three words'\n\nIn [62]: str_list = string.split()\n\nIn [63]: print(str_list)\n['Just', 'three', 'words']  Python takes care of long spaces, for example:  In [64]: line = 'A lot of       space'\n\nIn [65]: stuff = line.split()\n\nIn [66]: print(stuff)\n['A', 'lot', 'of', 'space']  If we specify the delimiter (it can be a space  ' ' , a  ; , a  : , whatever you want).  In [67]: line = 'A lot of       space'\n\nIn [68]: stuff = line.split(' ')\n\nIn [69]: print(stuff)\n['A', 'lot', 'of', '', '', '', '', '', '', 'space']  ;  and many other characters are not delimiters by default:  In [70]: s = 'a;b;c'\n\nIn [71]: thing  = s.split()\n\nIn [72]: print(thing)\na;b;c\n\nIn [73]: thing = s.split(';')\n\nIn [74]: print(thing)\n['a', 'b', 'c']",
            "title": "Lists"
        },
        {
            "location": "/python/python/#dictionaries",
            "text": "Dictionaries are also a type of collections and they are MUTABLE too.    Difference between lists and dictionaries:   List: a linear collection of values that stay in order.  Dictionary: A \"bag\" of values, each with it's own label (key).  Dictionaries don't maintain order, we index the the things we put in the\n  dictionary with a \"look up\" tag. For example:   In [75]: bag = {}\n\nIn [76]: bag['money'] = 12\n\nIn [77]: bag['candy'] = 5 \n\nIn [78]: bag['tissues'] = 7 \n\nIn [79]: print(bag)\n{'tissues': 7, 'money': 12, 'candy': 5}\n\nIn [80]: print(bag['money'])\n12\n\nIn [81]: bag['money'] += 2\n\nIn [82]: print(bag)\n{'tissues': 7, 'money': 14, 'candy': 5}   We can use the operators  in  and  not in  to check if a key is or not in a\n  dictionary.   In [83]: 'book' in bag\nOut[83]: False\n\nIn [84]: 'candy' in bag\nOut[84]: True\n\nIn [85]: 'cigarette' not in bag\nOut[85]: True   Retrieving keys and values.   In [86]: ages = {'John': 30, 'Maria': 28, 'Lucas': 23}\n\nIn [87]: print(ages.keys())\ndict_keys(['Maria', 'Lucas', 'John'])\n\nIn [88]: print(ages.values())\ndict_values([28, 23, 30])  Note : If we don't change the dictionary in between these operations, the keys\nand values are displayed in order.   We can get a list with pairs (key, value) by doing:   In [89]: print(ages.items())\ndict_items([('Maria', 28), ('Lucas', 23), ('John', 30)])   Dictionaries are good for counting how often we \"see\" something.    There are another two built in data structures: tuples and sets. You can read\nabout them in the\nPython documentation .  A quick insight into them:   Tuples  are like lists but they are IMMUTABLE, which saves time in\n  accessing memory. (Syntax  (element1, element2, ...) )  Sets  are collections that have no order, no duplicate elements and their\n  elements are  hashable \n  which saves time when accessing memory. We can do operations like union,\n  intersection, difference, and symmetric difference. (Syntax  {element1,\n  element2, ...}  and an empty set is created by doing  set() )    Example: look for an element in a list and in a set (%timeit)  In [90]: rg = range(50000)\n\nIn [91]: lst = list(rg)\n\nIn [92]: %time (40035 in lst)\nCPU times: user 4 ms, sys: 0 ns, total: 4 ms\nWall time: 1.91 ms\nOut[92]: True\n\nIn [93]: st = set(rg)\n\nIn [94]: %time (40035 in st)\nCPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 14.3 \u00b5s\nOut[94]: True\n\nIn [95]: %time (51000 in lst)\nCPU times: user 4 ms, sys: 0 ns, total: 4 ms\nWall time: 2.36 ms\nOut[95]: False\n\nIn [96]: %time (51000 in st)\nCPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 12.9 \u00b5s\nOut[96]: False",
            "title": "Dictionaries"
        },
        {
            "location": "/python/python/#control-flow",
            "text": "",
            "title": "Control Flow"
        },
        {
            "location": "/python/python/#conditionals",
            "text": "The syntax is \"If  x  true, then do something; otherwise, do something else.\"  If  statement on its own:  In [97]: a = 8\n\nIn [98]: b = 3 \n\nIn [99]: if a > b :\n    ...:     print('a is bigger than b')\n    ...:\na is bigger than b  If-else  statement:   In [100]: x = float(input('Insert your number: '))\nInsert your number: 711457\n\nIn [101]: if x % 17 == 0:\n     ...:     print('Your number is a multiple of 17.')\n     ...: else :\n     ...:     print('Your number is not a multiple of 17.')\n\nYour number is not a multiple of 17.  If-elif-else  statement:  In [102]: a = 3\n\nIn [103]: b = 5\n\nIn [104]: if a > b:\n     ...:     print('a is bigger than b.')\n     ...: elif a < b:\n     ...:     print('a is smaller than b.')\n     ...: else:\n     ...:     print('a is equal to b.')\n     ...:\na is smaller than b.  Note:  We can have as many  elif  as we want.",
            "title": "Conditionals"
        },
        {
            "location": "/python/python/#loops",
            "text": "Loops are useful for executing the same block multiple times. In Python we have\nmultiple looping formats:  while  loops,  for  loops, and comprehensions.  While loops  They are related to an  if  statement, they will compute  while  a condition is true.  In [105]: sec = 5\nIn [106]: while 0 < sec :\n     ...:     print('You have {} seconds'.format(sec))\n     ...:     sec -= 1\n     ...: print('Boom!!!')\n    ...:\nYou have 5 seconds\nYou have 4 seconds\nYou have 3 seconds\nYou have 2 seconds\nYou have 1 seconds\nBoom!!!  Note:  You have to be careful to not generate an infinite loop.   For loops \nIt is usually more useful to iterate over a certain group of things or an \"iterable\".  In [107]: for i in range(5,0,-1):\n    ...:     print('You have {} seconds'.format(i))\n    ...: print('Boom!!!')\n    ...: \nYou have 5 seconds\nYou have 4 seconds\nYou have 3 seconds\nYou have 2 seconds\nYou have 1 seconds\nBoom!!!  Another example:  In [108]: name = input('Insert your name: ')\nInsert your name: Naty\n\nIn [109]: name\nOut[109]: 'Naty'\n\nIn [110]: count = 1\n\nIn [111]: for letter in name:\n    ...:     print('The {} letter is {}'.format(count,letter))\n    ...:     count +=1\n    ...:\nThe 1 letter is N\nThe 2 letter is a\nThe 3 letter is t\nThe 4 letter is y  We can loop over dictionaries too:  In [112]: d = {\"name\":\"Lionel\", \"last_name\": \"Messi\", \"birthday\": [6, 24, 1987]}\n\nIn [113]: for item in d.items():\n     ...:     print(item)\n     ...:\n('birthday', [6, 24, 1987])\n('name', 'Lionel')\n('last_name', 'Messi')\n\nIn [114]: for key, value in d.items():\n     ...:     print(key, value)\n     ...:\nbirthday [6, 24, 1987]\nname Lionel\nlast_name Messi  Comprehensions  for  and  while  loops are really useful but they take at least \"two lines\",\nand if you need to save the result of each loop in the iteration in a list, set,\netc. This takes at least \"three lines\". Thankfully, Python is so great that in\nsome cases we can reduce these cases to just ONE line! Let's see this with an\nexample:  Normal loop:  In [114]: fruits = ['apple', 'orange', 'grape', 'banana', 'pineapple', 'strawberry', 'watermelon']\n\nIn [115]: upper_fruits = []\n\nIn [116]: for fruit in fruits:\n    ...:     upper_fruits.append(fruit.upper())\n\nIn [117]: upper_fruits\nOut[117]: ['APPLE', 'ORANGE', 'GRAPE', 'BANANA', 'PINEAPPLE', 'STRAWBERRY', 'WATERMELON']  Pythonic way:  In [118]: upper_pythonic = [fruit.upper() for fruit in fruits]\n\nIn [119]: upper_pythonic\nOut[119]: ['APPLE', 'ORANGE', 'GRAPE', 'BANANA', 'PINEAPPLE', 'STRAWBERRY', 'WATERMELON']  We can add a filter, for example:  In [120]: entries = [2, 11, 49, 3, 57, 33, 9]\n\nIn [121]: entries\nOut[121]: [2, 11, 49, 3, 57, 33, 9]\n\nIn [122]: some = [x**2 for x in entries if x%3==0]\n\nIn [123]: some\nOut[123]: [9, 3249, 1089, 81]\n\nIn [124]: orig = [int(x**(1/2)) for x in some]\n\nIn [125]: orig\nOut[125]: [3, 57, 33, 9]",
            "title": "Loops"
        },
        {
            "location": "/python/python/#ipython-magics",
            "text": "Sometimes when we are working on IPython we would like to select certain lines\nand save them into a Python script. We can do that!  We can check the history of what we've done in the current session of IPython by\ntyping:  %hist -n  If we want to select certain lines and put them into a script, the command %edit  allow us to do this. Let's suppose we want lines 1, 3 and from 5-10; \nthen we should do:  %edit 1 3 5-10  and this will open the default editor with the lines we want.   Once we have that script saved if we want to run it we can use the command %run :  %run my_script.py",
            "title": "IPython Magics"
        },
        {
            "location": "/python/python/#exercises",
            "text": "Slicing    1- Write code using  find()  and string slicing to extract the number at the end\nof the line below. Convert the extracted value to a floating point number and\nprint it out. (Content taken from online course \"Python Data Structures\" by Dr.\nCharles Severance)  text = \"X-DSPAM-Confidence:    0.8475\"  Solution  Slicing and for loop    2- Write a code that grabs, from each word, all the letters except 'ball' at the\nend of each one. Save the output in a list and print it out.  The original list is:  sports = ['Football', 'Volleyball', 'Basketball', 'Baseball', 'Handball', 'Softball']  And your output should look like:  ['Foot', 'Volley', 'Basket', 'Base', 'Hand', 'Soft']  Solution  Dictionaries for counting  3- Write a code that counts the frequency of each word that we have in the\nphrase provided.  Tips: split the text and put the words in a list, use a dictionary to count the\nrepetitions.  (The idea of the following two exercises was inspired by content from the online\ncourse \"Python Data Structures\" by Dr. Charles Severance)  happy = \"I felt happy because I saw the others were happy and because I knew I should feel happy but I was not really happy\"  Your output should look like:  {'happy': 4, 'really': 1, 'and': 1, 'feel': 1, 'others': 1, 'felt': 1, 'not': 1, 'the': 1, 'should': 1, 'knew': 1, 'was': 1, 'saw': 1, 'I': 5, 'but': 1, 'were': 1, 'because': 2}  Remember that dictionaries don't preserve order, therefore the items in your\noutput might be in a different order. However, you can still check if your\noutput is equal to the one we provide by doing a logical comparison using the ==  operation, which should return  True  if you get it right.  Optional : Rewrite exercise 3 using the method  get() .   Solutions",
            "title": "Exercises"
        },
        {
            "location": "/python/solutions/python.slicing.soln/",
            "text": "Slicing\n \n\n\n1- Write code using find() and string slicing to extract the number at the end of the line below. Convert the extracted value to\na floating point number and print it out. (Content took from online course \"Python Data Structures\" by Dr. Charles Severance)\n\n\ntext = \"X-DSPAM-Confidence:    0.8475\"\n\n\n\n\nSolution:\n\n\nIn [126]: text = \"X-DSPAM-Confidence:    0.8475\"\n\nIn [127]: pos0 = text.find(':')  #They can also look for '0' but this is more problem dependent.\nIn [128]: str_num = text[pos0+1:]\n\nIn [129]: num_float = float(str_num)\n\nIn [130]: print(num_float)\n0.8475",
            "title": "_slicing_solution"
        },
        {
            "location": "/python/solutions/python.dict.count.soln/",
            "text": "Dictionaries for counting\n\n\n3- Write a code that counts the frequency of each word that we have in the\nphrase provided.\n\n\nTips: split the text and put the words in a list, use a dictionary to count the\nrepetitions.\n\n\n(The idea of the following two exercises was inspired by content from the online\ncourse \"Python Data Structures\" by Dr. Charles Severance)\n\n\nhappy = \"I felt happy because I saw the others were happy and because I knew I should feel happy but I was not really happy\"\n\n\n\n\nYour output should look like:\n\n\n{'happy': 4, 'really': 1, 'and': 1, 'feel': 1, 'others': 1, 'felt': 1, 'not': 1, 'the': 1, 'should': 1, 'knew': 1, 'was': 1, 'saw': 1, 'I': 5, 'but': 1, 'were': 1, 'because': 2}\n\n\n\n\nRemember that dictionaries don't preserve order, therefore the items in your\noutput might be in a different order. However, you can still check if your\noutput is equal to the one we provide by doing a logical comparison using the\n\n==\n operation, which should return \nTrue\n if you get it right.\n\n\nOptional\n: Rewrite exercise 3 using the method \nget()\n. \n\n\nSolution\n\n\nIn [135]: happy = \"I felt happy because I saw the others were happy and because I knew I should feel happy\n    ...:  but I was not really happy\"\n\nIn [136]: words = happy.split()\n\nIn [137]: counts = {}\n\nIn [138]: for word in words:\n     ...:     if word not in counts:\n     ...:         counts[word] = 1\n     ...:     else:\n     ...:         counts[word] += 1\n     ...: \n\nIn [139]: print(counts)\n{'happy': 4, 'really': 1, 'and': 1, 'feel': 1, 'others': 1, 'felt': 1, 'not': 1, 'the': 1, 'should': 1, 'knew': 1, 'was': 1, 'saw': 1, 'I': 5, 'but': 1, 'were': 1, 'because': 2}\n\n\n\n\n\nSolution using \nget()\n\n\nIn [140]: happy = \"I felt happy because I saw the others were happy and because I knew I should feel happy\n    ...:  but I was not really happy\"\n\nIn [141]: words = happy.split()\n\nIn [142]: counts = {}\nIn [143]: for word in words:\n     ...:     counts[word] = counts.get(word, 0) +1 \n     ...:\n\nIn [144]: print(counts)\n{'happy': 4, 'really': 1, 'and': 1, 'feel': 1, 'others': 1, 'felt': 1, 'not': 1, 'the': 1, 'should': 1, 'knew': 1, 'was': 1, 'saw': 1, 'I': 5, 'but': 1, 'were': 1, 'because': 2}",
            "title": "_dict_solution"
        },
        {
            "location": "/python/solutions/python.ball.soln/",
            "text": "Slicing and for loop\n \n\n\n2- Write a code that grabs, from each word, all the letters except 'ball' at the\nend of each one. Save the output in a list and print it out.\n\n\nThe original list is:\n\n\nsports = ['Football', 'Volleyball', 'Basketball', 'Baseball', 'Handball', 'Softball']\n\n\n\n\nAnd your output should look like:\n\n\n['Foot', 'Volley', 'Basket', 'Base', 'Hand', 'Soft']\n\n\n\n\nSolution\n:\n\n\nIn [131]: sports = ['Football', 'Volleyball', 'Basketball', 'Baseball', 'Handball', 'Softball']\n\nIn [132]: random = []\n\nIn [133]: for sport in sports:\n     ...:     word = sport[:-4]\n     ...:     random.append(word)\n     ...:\n\nIn [134]: print(random)\n['Foot', 'Volley', 'Basket', 'Base', 'Hand', 'Soft']",
            "title": "_ball_solution"
        },
        {
            "location": "/openlicenses/openlicensing/",
            "text": "Open Licensing\n\n\nWhat is meant by \"open\"?\n\n\nIn an \nOctober 2015 interview\n, Prof. Barba addresses why she advocates so strongly for open-source technology in research and education. \nShe explains: \n\n\n\"Free and open-source software (FOSS) is a human invention of tremendous impact. It poses an alternative to intellectual-property instruments that are limiting and want to control how a creative work is used. Open-source licenses allow people to coordinate their work freely, within the confines of copyright law, while making access and wide distribution a priority. I\u2019ve always thought that this is fundamentally aligned with the method of science, where we value academic freedom and wide dissemination of scientific findings.\"\n\n\nThere are two ideas here. One is that open-source licenses give us a framework that promotes collaboration. \nCertainly, access and wide dissemination are a priority. But the power of open-source software comes not just from being able to \nread\n the source code, but from being able to \ncontribute\n to and \nbuild\n from it. \nFor this power to be realized, it's not sufficient to make the source public to read. \nWe must attach a \nlicense\n that allows others to modify and distribute the code. \n\n\nLet\u2019s be clear about this: \n\u201cOpen data and content can be freely used, modified, and shared by anyone for any purpose.\u201d \n\u2014See \nThe Open Definition\n.\n1\n\n\nOpen source and reproducibility\n\n\nThe second idea in Barba's quote above is that open source is aligned with the method of science. \nShe continues: \n\n\"It\u2019s a long tradition in science that publication of scientific findings should be accompanied by detailed description of materials and methods, such that another scientist may be able to replicate the findings. [\u2026] I frankly cannot see how one can publish a scientific finding that relies on software, without the source code of the software that generated the findings also being public. The code is part of the method and findings cannot be replicated without it.\"\n \n\n\nStanford professor David Donoho and co-workers appear to be the first ones to publicly state that reproducibility depends on open code and data.\n2\n \nThey define reproducible computational research as that \n\"in which all details of computations\u2014code and data\u2014are made conveniently available to others.\"\n \nThey took inspiration from geophysics professor Jon Claerbout, who said that in computational science \n\"the actual scholarship is the complete software development environment and the complete set of instructions which generated the figures.\"\n \n\n\nGuide to licenses\n\n\nEveryone developing software in an academic setting should have working knowledge of software licenses. \nWe recommend reading: \"A Quick Guide to Software Licensing for the Scientist-Programmer,\" by Morin et al.\n3\n\n\nThe first thing to understand is that simply making the source code public does not make your project open source. \nSoftware is a creative work, and \ncopyright is automatically attached\n to it. \nWithout a license, your software is in a legal limbo where readers don't know how they can use it, if at all: \na well-informed reader will opt for \nnot using\n your software, as the only safe behavior. \nThe license is a contract between the authors of software and the users. \nIt gives software authors the power to share with users, and to collaborate with other developers.\n\n\n\n\nAlways add a license to software you plan to make public.\n\n\n\n\nFree and open-source software (FOSS)\n\n\nFree and open-source software is under a license that grants the users \nfreedom\n: to access, use or modify the software for their purposes. \nThe most important distinction between the various FOSS licenses is whether they are \npermissive\n versus \ncopyleft\n. \nThese terms are often confused. \n\n\nA permissive license gives more freedoms: the only restriction of use could be that the original authors receive credit in any distribution of the software or any derivative works. \nEven commercial uses, or incorporating the software into other proprietary (closed) works, is allowed. \nAcademic software benefit most from permissive licensing. \nIn fact, permissive licenses originated at accademic institutions, including the Berkeley Software Distribution or BSD License, the MIT License and the Apache License.\n\n\nA copyleft license restricts the use of the software by requiring that any derivative works be \nalso\n under the license of the original. \nAnother word for this model is \"share-alike.\" \nMany developers want to ensure open access to their work and all derivatives for all posterity. \nThis may be considered virtuous in some circles, but we should recognize that it is achieved by placing \nrestrictions\n on the use of software. \nThe typical copyleft license is GPL.\n\n\nLicense compatibility\n\n\nMorin et al.\n3\n give an excellent overview of the important concept of license \ncompatibility\n. \nCompatible licenses allow source code from different works to be combined to make new software. \nNot all licenses are compatible! \nBecause incompatible terms can arise from subtle wording, the \nOpen Source Initiative\n (OSI) strongly recommends using an existing OSI-approved license, instead of attempting to craft a custom license. \nNote that comptibility is directional: it behaves differently whether a piece of code is built into or from another. (See the illustration below from Morin et al.)\nStaff at university or laboratory technology offices may be ignorant of these issues, making it even more important for researchers to be well informed.\n\n\n\n\nCredit:\n Figure 2 of Morin et al. (2012). Illustrates compatibility of licenses: permissive licenses (BSD, MIT) are forward-compatible with any other license, whereas copyleft licenses are only forward-compatible with themselves. \n\n\n\n\nDirectional compatibility of licenses is the reason why you should always be aware of the licensing terms of any code that you are reading and hoping to build upon. \n\n\n\n\nChoosing a license for your software\n\n\nFor academic and research software, you are likely to want a simple license that is most permissive. \nFor example, you can use the \nMIT License\n. \nMost of the research code in our group has been released under MIT. \n\n\nIf you use software by another researcher that was released under MIT, your obligation is to preserve the copyright notice in future distributions of the software, ensuring that rightful credit is given to the original authors (attribution). \n\n\nIllustrating how subtle language variations matter, we recently decided to change our pick to the \nBSD 3-clause License\n. \nIt is very similar to the MIT License, but there is a small difference in the wording about attribution (which we learned about from a \ncomment\n by A. Scopatz on a GitHub discussion thread).\n\n\nThe MIT License says:\n\n\n\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. \n\n\n\n\nWhile BSD 3-clause says:\n\n\n\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n\n\n\nAs Scopatz noted, this language implies that a user could copy \nsome\n of an MIT-licensed code (as long as the portion is deemed \"not substantial\") without attribution. \nIn academia, we always prefer full attribution of any portions of copied works, and BSD 3-clause is more precise in this. \n\n\nThe website \nchoosealicense.com\n offers a handy starting point for your choice of licenses. \nIt includes a detailed \nAppendix\n with a table of FOSS licenses and their features. \n\n\nBonus advice\n\n\n\n\nWrite into your grant proposals that your research software will be released under an OSI-approved license. \nIf you're lucky to have your grant funded, the condition of open-source code release becomes part of the contract with the university. \nThis can save you some grief when trying to explain to staff at the tech office why your software needs to be open source. \nThey often just don't understand the field, and want to default to a proprietary license, imagining some commercial value may exist in your research code.    \n\n\n\n\nReferences\n\n\n1\n The Open Definition, \nhttp://opendefinition.org\n\n\n2\n Donoho et al. (2009), Reproducible Research in Computational Harmonic Analysis, \nComp. Sci. Eng.\n 11(1):8\u201318.\n\n\n3\n Morin, A., J. Urban and P. Sliz (2012) A Quick Guide to Software Licensing for the Scientist-Programmer, \nPLoS Comput. Biol.\n 8(7): e1002598 \ndoi:10.1371/journal.pcbi.1002598",
            "title": "Open Licensing"
        },
        {
            "location": "/openlicenses/openlicensing/#open-licensing",
            "text": "",
            "title": "Open Licensing"
        },
        {
            "location": "/openlicenses/openlicensing/#what-is-meant-by-open",
            "text": "In an  October 2015 interview , Prof. Barba addresses why she advocates so strongly for open-source technology in research and education. \nShe explains:   \"Free and open-source software (FOSS) is a human invention of tremendous impact. It poses an alternative to intellectual-property instruments that are limiting and want to control how a creative work is used. Open-source licenses allow people to coordinate their work freely, within the confines of copyright law, while making access and wide distribution a priority. I\u2019ve always thought that this is fundamentally aligned with the method of science, where we value academic freedom and wide dissemination of scientific findings.\"  There are two ideas here. One is that open-source licenses give us a framework that promotes collaboration. \nCertainly, access and wide dissemination are a priority. But the power of open-source software comes not just from being able to  read  the source code, but from being able to  contribute  to and  build  from it. \nFor this power to be realized, it's not sufficient to make the source public to read. \nWe must attach a  license  that allows others to modify and distribute the code.   Let\u2019s be clear about this: \n\u201cOpen data and content can be freely used, modified, and shared by anyone for any purpose.\u201d \n\u2014See  The Open Definition . 1",
            "title": "What is meant by \"open\"?"
        },
        {
            "location": "/openlicenses/openlicensing/#open-source-and-reproducibility",
            "text": "The second idea in Barba's quote above is that open source is aligned with the method of science. \nShe continues:  \"It\u2019s a long tradition in science that publication of scientific findings should be accompanied by detailed description of materials and methods, such that another scientist may be able to replicate the findings. [\u2026] I frankly cannot see how one can publish a scientific finding that relies on software, without the source code of the software that generated the findings also being public. The code is part of the method and findings cannot be replicated without it.\"    Stanford professor David Donoho and co-workers appear to be the first ones to publicly state that reproducibility depends on open code and data. 2  \nThey define reproducible computational research as that  \"in which all details of computations\u2014code and data\u2014are made conveniently available to others.\"  \nThey took inspiration from geophysics professor Jon Claerbout, who said that in computational science  \"the actual scholarship is the complete software development environment and the complete set of instructions which generated the figures.\"",
            "title": "Open source and reproducibility"
        },
        {
            "location": "/openlicenses/openlicensing/#guide-to-licenses",
            "text": "Everyone developing software in an academic setting should have working knowledge of software licenses. \nWe recommend reading: \"A Quick Guide to Software Licensing for the Scientist-Programmer,\" by Morin et al. 3  The first thing to understand is that simply making the source code public does not make your project open source. \nSoftware is a creative work, and  copyright is automatically attached  to it. \nWithout a license, your software is in a legal limbo where readers don't know how they can use it, if at all: \na well-informed reader will opt for  not using  your software, as the only safe behavior. \nThe license is a contract between the authors of software and the users. \nIt gives software authors the power to share with users, and to collaborate with other developers.   Always add a license to software you plan to make public.",
            "title": "Guide to licenses"
        },
        {
            "location": "/openlicenses/openlicensing/#free-and-open-source-software-foss",
            "text": "Free and open-source software is under a license that grants the users  freedom : to access, use or modify the software for their purposes. \nThe most important distinction between the various FOSS licenses is whether they are  permissive  versus  copyleft . \nThese terms are often confused.   A permissive license gives more freedoms: the only restriction of use could be that the original authors receive credit in any distribution of the software or any derivative works. \nEven commercial uses, or incorporating the software into other proprietary (closed) works, is allowed. \nAcademic software benefit most from permissive licensing. \nIn fact, permissive licenses originated at accademic institutions, including the Berkeley Software Distribution or BSD License, the MIT License and the Apache License.  A copyleft license restricts the use of the software by requiring that any derivative works be  also  under the license of the original. \nAnother word for this model is \"share-alike.\" \nMany developers want to ensure open access to their work and all derivatives for all posterity. \nThis may be considered virtuous in some circles, but we should recognize that it is achieved by placing  restrictions  on the use of software. \nThe typical copyleft license is GPL.",
            "title": "Free and open-source software (FOSS)"
        },
        {
            "location": "/openlicenses/openlicensing/#license-compatibility",
            "text": "Morin et al. 3  give an excellent overview of the important concept of license  compatibility . \nCompatible licenses allow source code from different works to be combined to make new software. \nNot all licenses are compatible! \nBecause incompatible terms can arise from subtle wording, the  Open Source Initiative  (OSI) strongly recommends using an existing OSI-approved license, instead of attempting to craft a custom license. \nNote that comptibility is directional: it behaves differently whether a piece of code is built into or from another. (See the illustration below from Morin et al.)\nStaff at university or laboratory technology offices may be ignorant of these issues, making it even more important for researchers to be well informed.   Credit:  Figure 2 of Morin et al. (2012). Illustrates compatibility of licenses: permissive licenses (BSD, MIT) are forward-compatible with any other license, whereas copyleft licenses are only forward-compatible with themselves.    Directional compatibility of licenses is the reason why you should always be aware of the licensing terms of any code that you are reading and hoping to build upon.",
            "title": "License compatibility"
        },
        {
            "location": "/openlicenses/openlicensing/#choosing-a-license-for-your-software",
            "text": "For academic and research software, you are likely to want a simple license that is most permissive. \nFor example, you can use the  MIT License . \nMost of the research code in our group has been released under MIT.   If you use software by another researcher that was released under MIT, your obligation is to preserve the copyright notice in future distributions of the software, ensuring that rightful credit is given to the original authors (attribution).   Illustrating how subtle language variations matter, we recently decided to change our pick to the  BSD 3-clause License . \nIt is very similar to the MIT License, but there is a small difference in the wording about attribution (which we learned about from a  comment  by A. Scopatz on a GitHub discussion thread).  The MIT License says:   The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.    While BSD 3-clause says:   Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.   As Scopatz noted, this language implies that a user could copy  some  of an MIT-licensed code (as long as the portion is deemed \"not substantial\") without attribution. \nIn academia, we always prefer full attribution of any portions of copied works, and BSD 3-clause is more precise in this.   The website  choosealicense.com  offers a handy starting point for your choice of licenses. \nIt includes a detailed  Appendix  with a table of FOSS licenses and their features.",
            "title": "Choosing a license for your software"
        },
        {
            "location": "/openlicenses/openlicensing/#bonus-advice",
            "text": "Write into your grant proposals that your research software will be released under an OSI-approved license. \nIf you're lucky to have your grant funded, the condition of open-source code release becomes part of the contract with the university. \nThis can save you some grief when trying to explain to staff at the tech office why your software needs to be open source. \nThey often just don't understand the field, and want to default to a proprietary license, imagining some commercial value may exist in your research code.",
            "title": "Bonus advice"
        },
        {
            "location": "/openlicenses/openlicensing/#references",
            "text": "1  The Open Definition,  http://opendefinition.org  2  Donoho et al. (2009), Reproducible Research in Computational Harmonic Analysis,  Comp. Sci. Eng.  11(1):8\u201318.  3  Morin, A., J. Urban and P. Sliz (2012) A Quick Guide to Software Licensing for the Scientist-Programmer,  PLoS Comput. Biol.  8(7): e1002598  doi:10.1371/journal.pcbi.1002598",
            "title": "References"
        },
        {
            "location": "/git/git/",
            "text": "Intro to \ngit\n\n\nWhy version control?\n\n\n\n\nInitial configuration\n\n\nUser settings\n\n\nThe first time we use \ngit\n on a new computer we need to configure a few details.\nWe want \ngit\n to know who we are and how to reach us (we'll see why later!).\n\n\nWe're also going to specify a text editor to use with \ngit\n and we want git\noutput to be colorized.\n\n\n$ git config --global user.name \"Gil Forsyth\"\n$ git config --global user.email \"gilforsyth@gmail.com\"\n$ git config --global color.ui \"auto\"\n$ git config --global core.editor \"nano -w\"\n\n\n\n\nCreate\n\n\nLet's create a directory for our work and then move into that directory\n\n\n$ mkdir wordcount\n\n\n\n\n$ cd wordcount/\n\n\n\n\nNow we can use \nwget\n to download a Python script into the \nwordcount\n folder.\n\n\n$ wget https://raw.githubusercontent.com/barbagroup/essential_skills_RRC/master/resources/word_count.py\n\n\n\n\n$ ls\n\n\n\n\nword_count.py\n\n\n\n\nNow we tell \ngit\n to make \nwordcount\n a repository--a place where \ngit\n can\nstore versions of our files:\n\n\n$ git init\n\n\n\n\nInitialized empty Git repository in /home/gil/wordcount/.git/\n\n\n\n\nIf we use \nls\n to check the directory's contents, it appears that nothing has\nchanged:\n\n\n$ ls\n\n\n\n\nword_count.py\n\n\n\n\nBut if we add the \n-a\n flag to show everything, we can see that \ngit\n has\ncreated a hidden directory called \n.git\n\n\n$ ls -a\n\n\n\n\n.  ..  .git  word_count.py\n\n\n\n\nThis folder contains the entire history of the repository. This means that you\ncan move the repository around on your computer simply by moving the folder. It\nalso means that if you delete the \n.git\n folder, your history is gone.\n\n\ngit status\n\n\nThis is the most used command in \ngit\n.  Let's try it out!\n\n\n$ git status\n\n\n\n\nOn branch master\n\nInitial commit\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n\n    word_count.py\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n\n\n\n\nWhat is \ngit\n telling us? Quite a bit! We're \nOn branch master\n. We'll ignore\nthat for now and come back to it later on.\n\n\nWe are on the \nInitial commit\n. What's a commit? A commit is a granular change\nmade to a file (or set of files) that is logged in the history of the\nrepository.\n\n\nWhat about that last line? We can use \ngit add\n to track. Let's try that.\n\n\ngit add\n\n\nWe have a file in the new repo and we want to start tracking any changes\nmade to that file. \ngit\n ignores files until you tell it to look after them. To\nbegin tracking, we have to \nadd\n the file to the repository:\n\n\n$ git add word_count.py\n\n\n\n\nDid anything happen? Let's check! What command should we use?\n\n\n$ git status\n\n\n\n\nOn branch master\n\nInitial commit\n\nChanges to be committed:\n  (use \"git rm --cached <file>...\" to unstage)\n\n    new file:   word_count.py\n\n\n\n\n\nAgain, let's review the information that \ngit status\n gives us:\n\n\n\n\nWe are still \nOn branch master\n (and still ignoring this)\n\n\nIt is still the \nInitial commit\n (which makes sense, we haven't made any\n  commits yet...)\n\n\nThere are \nChanges to be committed\n\n\n\n\nNote that we haven't actually made a commit yet. We haven't finalized the\nsnapshot of the repo. Right now, we have a file called \nword_count.py\n located in\nwhat is called the \"Staging Area\".\n\n\nThe \"Staging Area\" is where we stage changes. It's a place to gather changes\nbefore committing those changes to the permanent history of the repository.\n\n\nWe'll talk more about the staging area later, but for now, let's finalize the\naddition of our new file by creating our first commit!\n\n\ngit commit\n\n\nIt's time!  Let's commit the changes to the repo history.\n\n\n$ git commit\n\n\n\n\nThis command will open up your text editor (\nnano\n) with the following text.\n\n\n\n# Please enter the commit message for your changes. Lines starting\n# with '#' will be ignored, and an empty message aborts the commit.\n# On branch master\n#\n# Initial commit\n#\n# Changes to be committed:\n#       new file:   word_count.py\n#\n\n\n\n\nAgain, \ngit\n has a bunch of helpful information. We can enter a commit message\non the first line and then save and quit.\n\n\n[master (root-commit) 47f748f] Add initial version of word count script\n 1 file changed, 9 insertions(+)\n create mode 100644 word_count.py\n\n\n\n\nGreat! We have created a snapshot of our file in the repo history. Now, even if\nwe make changes, we'll be able to roll them back if we don't like them.\n\n\nYou might be wondering what is a \"good commit\" message. We encourage you to go\nto this \nlink\n and read about why good\ncommit messages matter and how to write them. \n\n\nDid we miss anything? Check \ngit status\n to find out the state of the repository\nnow.\n\n\n$ git status \n\n\n\n\nOn branch master nothing to commit, working tree clean \n\n\n\n\nImprove the script\n\n\nHow does the script work right now? Let's run it and find out.\n\n\n$ python word_count.py \n\n\n\n\n{'not': 1, 'but': 1, 'because': 2, 'the': 1, 'was': 1, 'others': 1, 'happy': 4, \n'I': 5, 'really': 1, 'knew': 1, 'feel': 1, 'and': 1, 'felt': 1, 'should': 1, \n'saw': 1, 'were': 1}\n\n\n\n\nOk. This would be more useful if the user could decide what to input, don't you\nthink? Edit \nword_count.py\n and make it accept user input instead of a hardcoded\nsentence.\n\n\n$ nano word_count.py \n\n\n\n\nWith those changes saved, let's first see if the script works as expected!\n\n\n$ python word_count.py \nI can't quite tell if this is working.  Is it working?\n\n\n\n\n{'tell': 1, 'is': 1, 'working.': 1, 'if': 1, 'working?': 1, \"can't\": 1,\n 'quite': 1, 'Is': 1, 'it': 1, 'I': 1, 'this': 1}\n\n\n\n\nIt is working! Time to check in with \ngit\n.\n\n\n$ git status\n\n\n\n\nOn branch master\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git checkout -- <file>...\" to discard changes in working directory)\n\n    modified:   word_count.py\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\n\n\n\nOk, this is different than before. The first time we added a file, it was \"new\"\nand \ngit\n told us that. Now we have modified an \nexisting\n file and \ngit\n is\ntelling us it detected changes made to that file.\n\n\nNow we know what changes were made since we just made them, but what if we want to check?\n\n\ngit diff\n\n\ngit diff\n examines the \ndifference\n between the current state of a file and the\nlast committed version of the file (by default).\n\n\n$ git diff word_count.py\n\n\n\n\ndiff --git a/word_count.py b/word_count.py\nindex 3326ac7..1ac2be0 100644\n--- a/word_count.py\n+++ b/word_count.py\n@@ -1,4 +1,4 @@\n-happy = \"I felt happy because I saw the others were happy and because I knew I should feel happy but I was not really happy\"\n+happy = input()\n\n words = happy.split()\n\n\n\n\nHandy! \ngit\n shows which line(s) was changed and colors the replacement text\ngreen and the old text red. This looks ready to go. Time to stage the changes.\n\n\n$ git add word_count.py\n\n\n\n\nAnd a quick status check...\n\n\n$ git status\n\n\n\n\nOn branch master\nChanges to be committed:\n  (use \"git reset HEAD <file>...\" to unstage)\n\n    modified:   word_count.py\n\n\n\n\n\nLooks good, let's commit.\n\n\n$ git commit\n\n\n\n\n[master 97fba8d] allow user input of statement to word count\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\n\n\n\nAnd now the status should be clean.\n\n\n$ git status\n\n\n\n\nOn branch master\nnothing to commit, working tree clean\n\n\n\n\nWhat does \nword_count.py\n look like now?\n\n\ncat\n it and find out!\n\n\n$ cat word_count.py \n\n\n\n\nhappy = input()\n\nwords = happy.split()\n\ncounts = {}\nfor word in words:\n    counts[word] = counts.get(word, 0) + 1\n\nprint(counts)\n\n\n\n\nThat's the state of the file on the hard drive right now. The most recent change\nwe've made is what we see.\n\n\ncommit\n helpers\n\n\nThe blank \ninput\n line might be confusing. Let's add some prompt text to help the user understand what's happening:\n\n\n$ nano word_count.py \n\n\n\n\nOk. Changes made, let's test it out.\n\n\n$ python word_count.py\nEnter a statement to word count: This is a much better user experience\n\n\n\n\n{'This': 1, 'a': 1, 'much': 1, 'is': 1, 'experience': 1, 'user': 1, 'better': 1}\n\n\n\n\nCheck \nstatus\n\n\n$ git status\n\n\n\n\nOn branch master\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git checkout -- <file>...\" to discard changes in working directory)\n\n    modified:   word_count.py\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\n\n\n\nCheck the \ndiff\n\n\n$ git diff\n\n\n\n\ndiff --git a/word_count.py b/word_count.py\nindex 1ac2be0..2689774 100644\n--- a/word_count.py\n+++ b/word_count.py\n@@ -1,4 +1,4 @@\n-happy = input()\n+happy = input(\"Enter a statement to word count: \")\n\n words = happy.split()\n\n\n\n\nIf everything looks ok, then stage the file.\n\n\n$ git add word_count.py\n\n\n\n\nOne helpful shortcut that \ngit\n offers is the \n-m\n flag, which allows you to\nwrite your commit message right on the command line. This is great when you are\nmaking small, relatively simple changes.\n\n\n$ git commit -m \"add helper text to input function\"\n\n\n\n\n[master 09633c8] add helper text to input function\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\n\n\n\nSee? No text editor opened, but the commit has been made.\n\n\ngit log\n\n\nNow that we have a few \"snapshots\" of \nword_count.py\n we can take a look at its\nhistory. To do that, we use the \ngit log\n command.\n\n\n$ git log\n\n\n\n\ncommit 09633c88bb3f8b40d1c988b1df9004245320462a\nAuthor: Gil Forsyth <gilforsyth@gmail.com>\nDate:   Tue Dec 13 11:01:11 2016 -0500\n\n    add helper text to input function\n\ncommit 97fba8ddd7685e650813675f5b024267af0b94e7\nAuthor: Gil Forsyth <gilforsyth@gmail.com>\nDate:   Tue Dec 13 10:58:59 2016 -0500\n\n    allow user input of statement to word count\n\ncommit 47f748fea14e14ed84452a802dc14a5ea0829949\nAuthor: Gil Forsyth <gilforsyth@gmail.com>\nDate:   Tue Dec 13 10:53:23 2016 -0500\n\n    Add initial version of word count script\n\n\n\n\nThis is the full history of this repository. There are three commits. You can\nsee the author of each commit, a contact email, the date and time of the change\nand the commit message. This is some nice granular information!\n\n\nEach commit also has a long alphanumeric string for a header line. \nThis is the \nhash\n of that commit. It is a unique identifier for that particular commit. What can we use the hashes for? Time travel! (sort of...)\n\n\ndiff\n across time\n\n\nRemember that \ngit diff\n, by default, shows you the changes made to a specified\nfile since the most recent commit. You can also ask it to show you changes\nbetween two points in time by specifying the commit hashes to compare.\n\n\nNote\n: You don't need to type out the \nentire\n hash. Just the first 6 characters should do the trick. \n\n\n$ git diff 47f748 09633c word_count.py\n\n\n\n\ndiff --git a/word_count.py b/word_count.py\nindex 3326ac7..2689774 100644\n--- a/word_count.py\n+++ b/word_count.py\n@@ -1,4 +1,4 @@\n-happy = \"I felt happy because I saw the others were happy and because I knew I should feel happy but I was not really happy\"\n+happy = input(\"Enter a statement to word count: \")\n\n words = happy.split()\n\n\n\n\nThat diff is a comparison between the original commit and the most recent commit. \n\n\ncheckout\n an older version of a file\n\n\ndiff\n lets us compare the changes made in the repository history, but sometimes\nwe want to restore a previous version of a file entirely. Maybe we introduced a\nmistake somewhere or just like the old way better. People change their minds.\n\n\nTo \ncheckout\n a previous version of a file, we need the commit hashes. We can pass the \n--oneline\n flag to \ngit log\n for a more compact version:\n\n\n$ git log --oneline\n\n\n\n\n09633c8 add helper text to input function\n97fba8d allow user input of statement to word count\n47f748f Add initial version of word count script\n\n\n\n\nAs an example, we will restore \nword_count.py\n to its original state, before we\nmade any edits. To do that, we \ncheckout\n to the commit hash of the first commit\nand specify the file \nword_count.py\n\n\n$ git checkout 47f748 word_count.py \n\n\n\n\nDid it work? Use \ncat\n to find out:\n\n\n$ cat word_count.py \n\n\n\n\nhappy = \"I felt happy because I saw the others were happy and because I knew I should feel happy but I was not really happy\"\n\nwords = happy.split()\n\ncounts = {}\nfor word in words:\n    counts[word] = counts.get(word, 0) + 1\n\nprint(counts)\n\n\n\n\nNow the file, as it exists in the folder \nwordcount\n, is restored to its\noriginal state. Let's check on the \nstatus\n.\n\n\n$ git status\n\n\n\n\nOn branch master\nChanges to be committed:\n  (use \"git reset HEAD <file>...\" to unstage)\n\n    modified:   word_count.py\n\n\n\n\nThis is a little different than what happened before. \ngit\n isn't saying that\nchanges have been made, it's saying that changes are \nstaged\n. We're just one\n\ngit commit\n away from completely restoring \nword_count.py\n. \n\n\nWhen you restore a file in \ngit\n, rather than eliminating the commits that were\nmade, \ngit\n creates a \nnew\n commit that changes everything back. This way, if\nyou happen to change your mind \nagain\n, you haven't lost anything. Pretty cool,\nyeah?\n\n\nIn any case, we don't really want to restore the old version permanently, the new version is much more versatile. Instead of committing the changes, we can put things back the way they were a moment ago, again using \ncheckout\n.\n\n\nConsult \ngit log\n again and this time, use \ncheckout\n to go to the most recent\ncommit.\n\n\n$ git log --oneline\n\n\n\n\n09633c8 add helper text to input function\n97fba8d allow user input of statement to word count\n47f748f Add initial version of word count script\n\n\n\n\n$ git checkout 09633c8 word_count.py \n\n\n\n\n$ git status\n\n\n\n\nOn branch master\nnothing to commit, working tree clean\n\n\n\n\nAnd we're back to the future!",
            "title": "git"
        },
        {
            "location": "/git/git/#intro-to-git",
            "text": "",
            "title": "Intro to git"
        },
        {
            "location": "/git/git/#why-version-control",
            "text": "",
            "title": "Why version control?"
        },
        {
            "location": "/git/git/#initial-configuration",
            "text": "",
            "title": "Initial configuration"
        },
        {
            "location": "/git/git/#user-settings",
            "text": "The first time we use  git  on a new computer we need to configure a few details.\nWe want  git  to know who we are and how to reach us (we'll see why later!).  We're also going to specify a text editor to use with  git  and we want git\noutput to be colorized.  $ git config --global user.name \"Gil Forsyth\"\n$ git config --global user.email \"gilforsyth@gmail.com\"\n$ git config --global color.ui \"auto\"\n$ git config --global core.editor \"nano -w\"",
            "title": "User settings"
        },
        {
            "location": "/git/git/#create",
            "text": "Let's create a directory for our work and then move into that directory  $ mkdir wordcount  $ cd wordcount/  Now we can use  wget  to download a Python script into the  wordcount  folder.  $ wget https://raw.githubusercontent.com/barbagroup/essential_skills_RRC/master/resources/word_count.py  $ ls  word_count.py  Now we tell  git  to make  wordcount  a repository--a place where  git  can\nstore versions of our files:  $ git init  Initialized empty Git repository in /home/gil/wordcount/.git/  If we use  ls  to check the directory's contents, it appears that nothing has\nchanged:  $ ls  word_count.py  But if we add the  -a  flag to show everything, we can see that  git  has\ncreated a hidden directory called  .git  $ ls -a  .  ..  .git  word_count.py  This folder contains the entire history of the repository. This means that you\ncan move the repository around on your computer simply by moving the folder. It\nalso means that if you delete the  .git  folder, your history is gone.",
            "title": "Create"
        },
        {
            "location": "/git/git/#git-status",
            "text": "This is the most used command in  git .  Let's try it out!  $ git status  On branch master\n\nInitial commit\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n\n    word_count.py\n\nnothing added to commit but untracked files present (use \"git add\" to track)  What is  git  telling us? Quite a bit! We're  On branch master . We'll ignore\nthat for now and come back to it later on.  We are on the  Initial commit . What's a commit? A commit is a granular change\nmade to a file (or set of files) that is logged in the history of the\nrepository.  What about that last line? We can use  git add  to track. Let's try that.",
            "title": "git status"
        },
        {
            "location": "/git/git/#git-add",
            "text": "We have a file in the new repo and we want to start tracking any changes\nmade to that file.  git  ignores files until you tell it to look after them. To\nbegin tracking, we have to  add  the file to the repository:  $ git add word_count.py  Did anything happen? Let's check! What command should we use?  $ git status  On branch master\n\nInitial commit\n\nChanges to be committed:\n  (use \"git rm --cached <file>...\" to unstage)\n\n    new file:   word_count.py  Again, let's review the information that  git status  gives us:   We are still  On branch master  (and still ignoring this)  It is still the  Initial commit  (which makes sense, we haven't made any\n  commits yet...)  There are  Changes to be committed   Note that we haven't actually made a commit yet. We haven't finalized the\nsnapshot of the repo. Right now, we have a file called  word_count.py  located in\nwhat is called the \"Staging Area\".  The \"Staging Area\" is where we stage changes. It's a place to gather changes\nbefore committing those changes to the permanent history of the repository.  We'll talk more about the staging area later, but for now, let's finalize the\naddition of our new file by creating our first commit!",
            "title": "git add"
        },
        {
            "location": "/git/git/#git-commit",
            "text": "It's time!  Let's commit the changes to the repo history.  $ git commit  This command will open up your text editor ( nano ) with the following text.  \n# Please enter the commit message for your changes. Lines starting\n# with '#' will be ignored, and an empty message aborts the commit.\n# On branch master\n#\n# Initial commit\n#\n# Changes to be committed:\n#       new file:   word_count.py\n#  Again,  git  has a bunch of helpful information. We can enter a commit message\non the first line and then save and quit.  [master (root-commit) 47f748f] Add initial version of word count script\n 1 file changed, 9 insertions(+)\n create mode 100644 word_count.py  Great! We have created a snapshot of our file in the repo history. Now, even if\nwe make changes, we'll be able to roll them back if we don't like them.  You might be wondering what is a \"good commit\" message. We encourage you to go\nto this  link  and read about why good\ncommit messages matter and how to write them.   Did we miss anything? Check  git status  to find out the state of the repository\nnow.  $ git status   On branch master nothing to commit, working tree clean",
            "title": "git commit"
        },
        {
            "location": "/git/git/#improve-the-script",
            "text": "How does the script work right now? Let's run it and find out.  $ python word_count.py   {'not': 1, 'but': 1, 'because': 2, 'the': 1, 'was': 1, 'others': 1, 'happy': 4, \n'I': 5, 'really': 1, 'knew': 1, 'feel': 1, 'and': 1, 'felt': 1, 'should': 1, \n'saw': 1, 'were': 1}  Ok. This would be more useful if the user could decide what to input, don't you\nthink? Edit  word_count.py  and make it accept user input instead of a hardcoded\nsentence.  $ nano word_count.py   With those changes saved, let's first see if the script works as expected!  $ python word_count.py \nI can't quite tell if this is working.  Is it working?  {'tell': 1, 'is': 1, 'working.': 1, 'if': 1, 'working?': 1, \"can't\": 1,\n 'quite': 1, 'Is': 1, 'it': 1, 'I': 1, 'this': 1}  It is working! Time to check in with  git .  $ git status  On branch master\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git checkout -- <file>...\" to discard changes in working directory)\n\n    modified:   word_count.py\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")  Ok, this is different than before. The first time we added a file, it was \"new\"\nand  git  told us that. Now we have modified an  existing  file and  git  is\ntelling us it detected changes made to that file.  Now we know what changes were made since we just made them, but what if we want to check?",
            "title": "Improve the script"
        },
        {
            "location": "/git/git/#git-diff",
            "text": "git diff  examines the  difference  between the current state of a file and the\nlast committed version of the file (by default).  $ git diff word_count.py  diff --git a/word_count.py b/word_count.py\nindex 3326ac7..1ac2be0 100644\n--- a/word_count.py\n+++ b/word_count.py\n@@ -1,4 +1,4 @@\n-happy = \"I felt happy because I saw the others were happy and because I knew I should feel happy but I was not really happy\"\n+happy = input()\n\n words = happy.split()  Handy!  git  shows which line(s) was changed and colors the replacement text\ngreen and the old text red. This looks ready to go. Time to stage the changes.  $ git add word_count.py  And a quick status check...  $ git status  On branch master\nChanges to be committed:\n  (use \"git reset HEAD <file>...\" to unstage)\n\n    modified:   word_count.py  Looks good, let's commit.  $ git commit  [master 97fba8d] allow user input of statement to word count\n 1 file changed, 1 insertion(+), 1 deletion(-)  And now the status should be clean.  $ git status  On branch master\nnothing to commit, working tree clean",
            "title": "git diff"
        },
        {
            "location": "/git/git/#what-does-word_countpy-look-like-now",
            "text": "cat  it and find out!  $ cat word_count.py   happy = input()\n\nwords = happy.split()\n\ncounts = {}\nfor word in words:\n    counts[word] = counts.get(word, 0) + 1\n\nprint(counts)  That's the state of the file on the hard drive right now. The most recent change\nwe've made is what we see.",
            "title": "What does word_count.py look like now?"
        },
        {
            "location": "/git/git/#commit-helpers",
            "text": "The blank  input  line might be confusing. Let's add some prompt text to help the user understand what's happening:  $ nano word_count.py   Ok. Changes made, let's test it out.  $ python word_count.py\nEnter a statement to word count: This is a much better user experience  {'This': 1, 'a': 1, 'much': 1, 'is': 1, 'experience': 1, 'user': 1, 'better': 1}  Check  status  $ git status  On branch master\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git checkout -- <file>...\" to discard changes in working directory)\n\n    modified:   word_count.py\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")  Check the  diff  $ git diff  diff --git a/word_count.py b/word_count.py\nindex 1ac2be0..2689774 100644\n--- a/word_count.py\n+++ b/word_count.py\n@@ -1,4 +1,4 @@\n-happy = input()\n+happy = input(\"Enter a statement to word count: \")\n\n words = happy.split()  If everything looks ok, then stage the file.  $ git add word_count.py  One helpful shortcut that  git  offers is the  -m  flag, which allows you to\nwrite your commit message right on the command line. This is great when you are\nmaking small, relatively simple changes.  $ git commit -m \"add helper text to input function\"  [master 09633c8] add helper text to input function\n 1 file changed, 1 insertion(+), 1 deletion(-)  See? No text editor opened, but the commit has been made.",
            "title": "commit helpers"
        },
        {
            "location": "/git/git/#git-log",
            "text": "Now that we have a few \"snapshots\" of  word_count.py  we can take a look at its\nhistory. To do that, we use the  git log  command.  $ git log  commit 09633c88bb3f8b40d1c988b1df9004245320462a\nAuthor: Gil Forsyth <gilforsyth@gmail.com>\nDate:   Tue Dec 13 11:01:11 2016 -0500\n\n    add helper text to input function\n\ncommit 97fba8ddd7685e650813675f5b024267af0b94e7\nAuthor: Gil Forsyth <gilforsyth@gmail.com>\nDate:   Tue Dec 13 10:58:59 2016 -0500\n\n    allow user input of statement to word count\n\ncommit 47f748fea14e14ed84452a802dc14a5ea0829949\nAuthor: Gil Forsyth <gilforsyth@gmail.com>\nDate:   Tue Dec 13 10:53:23 2016 -0500\n\n    Add initial version of word count script  This is the full history of this repository. There are three commits. You can\nsee the author of each commit, a contact email, the date and time of the change\nand the commit message. This is some nice granular information!  Each commit also has a long alphanumeric string for a header line. \nThis is the  hash  of that commit. It is a unique identifier for that particular commit. What can we use the hashes for? Time travel! (sort of...)",
            "title": "git log"
        },
        {
            "location": "/git/git/#diff-across-time",
            "text": "Remember that  git diff , by default, shows you the changes made to a specified\nfile since the most recent commit. You can also ask it to show you changes\nbetween two points in time by specifying the commit hashes to compare.  Note : You don't need to type out the  entire  hash. Just the first 6 characters should do the trick.   $ git diff 47f748 09633c word_count.py  diff --git a/word_count.py b/word_count.py\nindex 3326ac7..2689774 100644\n--- a/word_count.py\n+++ b/word_count.py\n@@ -1,4 +1,4 @@\n-happy = \"I felt happy because I saw the others were happy and because I knew I should feel happy but I was not really happy\"\n+happy = input(\"Enter a statement to word count: \")\n\n words = happy.split()  That diff is a comparison between the original commit and the most recent commit.",
            "title": "diff across time"
        },
        {
            "location": "/git/git/#checkout-an-older-version-of-a-file",
            "text": "diff  lets us compare the changes made in the repository history, but sometimes\nwe want to restore a previous version of a file entirely. Maybe we introduced a\nmistake somewhere or just like the old way better. People change their minds.  To  checkout  a previous version of a file, we need the commit hashes. We can pass the  --oneline  flag to  git log  for a more compact version:  $ git log --oneline  09633c8 add helper text to input function\n97fba8d allow user input of statement to word count\n47f748f Add initial version of word count script  As an example, we will restore  word_count.py  to its original state, before we\nmade any edits. To do that, we  checkout  to the commit hash of the first commit\nand specify the file  word_count.py  $ git checkout 47f748 word_count.py   Did it work? Use  cat  to find out:  $ cat word_count.py   happy = \"I felt happy because I saw the others were happy and because I knew I should feel happy but I was not really happy\"\n\nwords = happy.split()\n\ncounts = {}\nfor word in words:\n    counts[word] = counts.get(word, 0) + 1\n\nprint(counts)  Now the file, as it exists in the folder  wordcount , is restored to its\noriginal state. Let's check on the  status .  $ git status  On branch master\nChanges to be committed:\n  (use \"git reset HEAD <file>...\" to unstage)\n\n    modified:   word_count.py  This is a little different than what happened before.  git  isn't saying that\nchanges have been made, it's saying that changes are  staged . We're just one git commit  away from completely restoring  word_count.py .   When you restore a file in  git , rather than eliminating the commits that were\nmade,  git  creates a  new  commit that changes everything back. This way, if\nyou happen to change your mind  again , you haven't lost anything. Pretty cool,\nyeah?  In any case, we don't really want to restore the old version permanently, the new version is much more versatile. Instead of committing the changes, we can put things back the way they were a moment ago, again using  checkout .  Consult  git log  again and this time, use  checkout  to go to the most recent\ncommit.  $ git log --oneline  09633c8 add helper text to input function\n97fba8d allow user input of statement to word count\n47f748f Add initial version of word count script  $ git checkout 09633c8 word_count.py   $ git status  On branch master\nnothing to commit, working tree clean  And we're back to the future!",
            "title": "checkout an older version of a file"
        },
        {
            "location": "/git/github1/",
            "text": "Git Remotes\n\n\nOne of the features inherent to modern version control systems is their ability\nto store your work elsewhere. Backups are important, for one thing. Even more\nimportantly, the ability to synchronize changes against a remote server allows\ncollaboration, which is what \ngit\n is all about.\n\n\nCreate github repo\n\n\nFirst, navigate to \ngithub.com\n and log in if you haven't\nalready. (Or make an account if you need one)\n\n\n\n\n\n\n\n\nAdd remote to local git repository\n\n\nCopy and paste the \nfirst\n line from the GitHub snippet into your terminal. \n\n\nWe are adding a \nremote\n, that by convention is called \norigin\n and it points at\n\nhttps://github.com/<username>/wordcount.git\n\n\nVerify that you have successfully added the remote with\n\n\ngit remote -v\n\n\n\n\norigin  https://github.com/gforsyth/wordcount.git (fetch)\norigin  https://github.com/gforsyth/wordcount.git (push)\n\n\n\n\ngit push\n\n\nNow the \"distributed\" part of \"distributed version control system\" will start to\nmake a whole lot more sense. \n\n\nRight now, we have a git repository in a folder \nwordcount\n and it has 3+\ncommits that we have made to a file called \nword_count.py\n.\n\n\nNow we want to transfer that history to our remote host (GitHub). To send\nupdates to a remote, we \npush\n.\n\n\ngit push -u origin master\n\n\n\n\nYou will be prompted for your username and password, then you should see\nsomething like:\n\n\nCounting objects: 9, done.\nDelta compression using up to 12 threads.\nCompressing objects: 100% (6/6), done.\nWriting objects: 100% (9/9), 1006 bytes | 0 bytes/s, done.\nTotal 9 (delta 1), reused 0 (delta 0)\nremote: Resolving deltas: 100% (1/1), done.\nBranch master set up to track remote branch master from origin.\nTo github.com:gforsyth/wordcount.git\n * [new branch]      master -> maste\n\n\n\n\nAgain, that's a bunch of information.  What did we do?\n\n\nWe told \ngit\n to \npush\n from \nmaster\n on the local machine, to a branch called\n\nmaster\n on the remote (which is called \norigin\n).\n\n\nThe \n-u\n flag is something we do the \nfirst\n time we push a \nnew\n branch to a\nrepository, don't worry about it right now, it will reappear later.\n\n\nNow look at the message from \ngit\n: It did a bunch of stuff that seems to\ninvolve compression -- that's good, because it means it's probably saving us\nbandwith and time. \n\n\nWe also have \"branch master set up to track remote branch master from origin\".\nWhat does this mean? It means we set the branch called \nmaster\n on the remote to\nbe the \ndefault\n choice for when we \npush\n! \n\n\nThat means that next time we make a change and want to update the remote, we can\njust do \ngit push\n. Cool!\n\n\nWhat happened on GitHub?\n\n\nLet's look!\n\n\n\n\nYou should see something like the image above.  Cool, huh?\n\n\nYou should see the \nword_count.py\n file -- try clicking it.\n\n\nAnd there's your code! Now hit 'Back' in the browser. You can explore more\nlater.\n\n\nNow click where it says \"Commits\"\n\n\nYou should see something like the following:\n\n\n\n\nThere are the commits you made! In fact, the complete history of your repository\nwas transferred to GitHub when you did a \npush\n! \n\n\nEdit your script on GitHub\n\n\nGitHub has a handy little text editor built right in that lets you make changes\nto files on the web. Let's try it!\n\n\nGo back to the main repo page again and again click on \nword_count.py\n.\n\n\nNow, click on the pencil icon in the top-right:\n\n\n\n\nNow let's edit the file. Add another \nprint\n statement to the script, just above\nthe last line, so that the script now reads:\n\n\nhappy = input(\"Enter a statement to word count: \")\n\nwords = happy.split()\n\ncounts = {}\nfor word in words:\n    counts[word] = counts.get(word, 0) + 1\n\nprint(\"The word frequency of your statement is: \")\nprint(counts)\n\n\n\n\nNow how do we save this? We have to commit it! We can do this in GitHub's\ninterface. Scroll down and enter a commit message in the box with the grey text\n\"Update word_count.py\" \n\n\nThen click \"Commit changes\"\n\n\ngit pull\n\n\nWe made a change to the script on GitHub! If you go back to the main repo page,\nyou'll see that the commit count and incremented by one. \n\n\nNow let's check in on the local copy of the repo.\n\n\nDo you remember how to view the commits we made on the command line?\n\n\n$ git log --oneline\n\n\n\n\n09633c8 add helper text to input function\n97fba8d allow user input of statement to word count\n47f748f Add initial version of word count script\n\n\n\n\nHmmm... only 3 commits? \n\n\nThe local copy of the repo and the remote are asynchronous. They don't \"sync\"\nthe way Google Drive or Dropbox do -- you have to tell them to update. This\nmight seem like an annoying extra step, but it's necessary. As we will see\nlater, many different users can edit the same repository and if everyone's\nchanges are constantly syncing in real time it would be a disaster.\n\n\nTo update the local copy of the repo with the changes we made on GitHub, we use\n\ngit pull\n. Makes sense, right? To move changes from the local repo \nto\n the\nremote, we \npush\n. To bring them to the local \nfrom\n the remote, we \npull\n. \n\n\n$ git pull\n\n\n\n\ngit pull\nremote: Counting objects: 3, done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (3/3), done.\nFrom github.com:gforsyth/wordcount\n   09633c8..2f53c34  master     -> origin/master\nUpdating 09633c8..2f53c34\nFast-forward\n word_count.py | 1 +\n 1 file changed, 1 insertion(+)\n\n\n\n\nIt worked! Or at least something happened. What does \ngit log\n say now?\n\n\n$ git log --oneline\n\n\n\n\n2f53c34 Add user-friendly print statement\n09633c8 add helper text to input function\n97fba8d allow user input of statement to word count\n47f748f Add initial version of word count script\n\n\n\n\nThere's the new commit! That should mean that \nword_count.py\n is updated! How\ncan we quickly check on \nword_count.py\n? \n\n\n$ cat word_count.py\n\n\n\n\nhappy = input(\"Enter a statement to word count: \")\n\nwords = happy.split()\n\ncounts = {}\nfor word in words:\n    counts[word] = counts.get(word, 0) + 1\n\nprint(\"The word frequency of your statement is: \")\nprint(counts)",
            "title": "push and pull"
        },
        {
            "location": "/git/github1/#git-remotes",
            "text": "One of the features inherent to modern version control systems is their ability\nto store your work elsewhere. Backups are important, for one thing. Even more\nimportantly, the ability to synchronize changes against a remote server allows\ncollaboration, which is what  git  is all about.",
            "title": "Git Remotes"
        },
        {
            "location": "/git/github1/#create-github-repo",
            "text": "First, navigate to  github.com  and log in if you haven't\nalready. (Or make an account if you need one)",
            "title": "Create github repo"
        },
        {
            "location": "/git/github1/#add-remote-to-local-git-repository",
            "text": "Copy and paste the  first  line from the GitHub snippet into your terminal.   We are adding a  remote , that by convention is called  origin  and it points at https://github.com/<username>/wordcount.git  Verify that you have successfully added the remote with  git remote -v  origin  https://github.com/gforsyth/wordcount.git (fetch)\norigin  https://github.com/gforsyth/wordcount.git (push)",
            "title": "Add remote to local git repository"
        },
        {
            "location": "/git/github1/#git-push",
            "text": "Now the \"distributed\" part of \"distributed version control system\" will start to\nmake a whole lot more sense.   Right now, we have a git repository in a folder  wordcount  and it has 3+\ncommits that we have made to a file called  word_count.py .  Now we want to transfer that history to our remote host (GitHub). To send\nupdates to a remote, we  push .  git push -u origin master  You will be prompted for your username and password, then you should see\nsomething like:  Counting objects: 9, done.\nDelta compression using up to 12 threads.\nCompressing objects: 100% (6/6), done.\nWriting objects: 100% (9/9), 1006 bytes | 0 bytes/s, done.\nTotal 9 (delta 1), reused 0 (delta 0)\nremote: Resolving deltas: 100% (1/1), done.\nBranch master set up to track remote branch master from origin.\nTo github.com:gforsyth/wordcount.git\n * [new branch]      master -> maste  Again, that's a bunch of information.  What did we do?  We told  git  to  push  from  master  on the local machine, to a branch called master  on the remote (which is called  origin ).  The  -u  flag is something we do the  first  time we push a  new  branch to a\nrepository, don't worry about it right now, it will reappear later.  Now look at the message from  git : It did a bunch of stuff that seems to\ninvolve compression -- that's good, because it means it's probably saving us\nbandwith and time.   We also have \"branch master set up to track remote branch master from origin\".\nWhat does this mean? It means we set the branch called  master  on the remote to\nbe the  default  choice for when we  push !   That means that next time we make a change and want to update the remote, we can\njust do  git push . Cool!",
            "title": "git push"
        },
        {
            "location": "/git/github1/#what-happened-on-github",
            "text": "Let's look!   You should see something like the image above.  Cool, huh?  You should see the  word_count.py  file -- try clicking it.  And there's your code! Now hit 'Back' in the browser. You can explore more\nlater.  Now click where it says \"Commits\"  You should see something like the following:   There are the commits you made! In fact, the complete history of your repository\nwas transferred to GitHub when you did a  push !",
            "title": "What happened on GitHub?"
        },
        {
            "location": "/git/github1/#edit-your-script-on-github",
            "text": "GitHub has a handy little text editor built right in that lets you make changes\nto files on the web. Let's try it!  Go back to the main repo page again and again click on  word_count.py .  Now, click on the pencil icon in the top-right:   Now let's edit the file. Add another  print  statement to the script, just above\nthe last line, so that the script now reads:  happy = input(\"Enter a statement to word count: \")\n\nwords = happy.split()\n\ncounts = {}\nfor word in words:\n    counts[word] = counts.get(word, 0) + 1\n\nprint(\"The word frequency of your statement is: \")\nprint(counts)  Now how do we save this? We have to commit it! We can do this in GitHub's\ninterface. Scroll down and enter a commit message in the box with the grey text\n\"Update word_count.py\"   Then click \"Commit changes\"",
            "title": "Edit your script on GitHub"
        },
        {
            "location": "/git/github1/#git-pull",
            "text": "We made a change to the script on GitHub! If you go back to the main repo page,\nyou'll see that the commit count and incremented by one.   Now let's check in on the local copy of the repo.  Do you remember how to view the commits we made on the command line?  $ git log --oneline  09633c8 add helper text to input function\n97fba8d allow user input of statement to word count\n47f748f Add initial version of word count script  Hmmm... only 3 commits?   The local copy of the repo and the remote are asynchronous. They don't \"sync\"\nthe way Google Drive or Dropbox do -- you have to tell them to update. This\nmight seem like an annoying extra step, but it's necessary. As we will see\nlater, many different users can edit the same repository and if everyone's\nchanges are constantly syncing in real time it would be a disaster.  To update the local copy of the repo with the changes we made on GitHub, we use git pull . Makes sense, right? To move changes from the local repo  to  the\nremote, we  push . To bring them to the local  from  the remote, we  pull .   $ git pull  git pull\nremote: Counting objects: 3, done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (3/3), done.\nFrom github.com:gforsyth/wordcount\n   09633c8..2f53c34  master     -> origin/master\nUpdating 09633c8..2f53c34\nFast-forward\n word_count.py | 1 +\n 1 file changed, 1 insertion(+)  It worked! Or at least something happened. What does  git log  say now?  $ git log --oneline  2f53c34 Add user-friendly print statement\n09633c8 add helper text to input function\n97fba8d allow user input of statement to word count\n47f748f Add initial version of word count script  There's the new commit! That should mean that  word_count.py  is updated! How\ncan we quickly check on  word_count.py ?   $ cat word_count.py  happy = input(\"Enter a statement to word count: \")\n\nwords = happy.split()\n\ncounts = {}\nfor word in words:\n    counts[word] = counts.get(word, 0) + 1\n\nprint(\"The word frequency of your statement is: \")\nprint(counts)",
            "title": "git pull"
        },
        {
            "location": "/git/merge/",
            "text": "Merging\n\n\nWe can make a commit in the local repo and push it to GitHub. We can also make a\ncommit on GitHub and pull it to the local repo. But what happens if we make a\ncommit in both places?\n\n\nThis happens all the time when multiple people are working on the same project.\nLet's investigate how \ngit\n handles these situations.\n\n\nAdd a README to your GitHub repo\n\n\nA README is a plain text file in the root of a repository that usually contains\n\n\n\n\na brief description of a project\n\n\ninstallation instructions\n\n\ninfo on how to collaborate\n\n\n\n\nor whatever else you want. \n\n\nOpen your GitHub copy of the \nwordcount\n repo and then click the \"Add README\"\nbutton\n\n\n\n\nDon't worry about adding any additional text beyond what is automatically added\nfor you. And the default GitHub commit message for this change is reasonable, so\nyou can just click \"Commit\"\n\n\n\n\nYou should see something like the above, where the GitHub copy of the repo now \nhas two files: \nword_count.py\n and \nREADME.md\n\n\nMake a change to the local repo\n\n\nWithout\n performing a \ngit pull\n now edit \nword_count.py\n locally to read:\n\n\nhappy = input(\"Enter a statement to word count: \")\n\nwords = happy.split()  # split user input on spaces\n\ncounts = {}\nfor word in words:\n    counts[word] = counts.get(word, 0) + 1\n\nprint(\"The word frequency of your statement is: \")\nprint(counts)\n\n\n\n\nStage and commit that change (remember the commands?). If we check the log in\nthe local copy of the repo, we should have something like:\n\n\n$ git log --oneline\n671907a add comment for future self\nde8fbc3 Add user-friendly print statement\n09633c8 add helper text to input function\n97fba8d allow user input of statement to word count\n47f748f Add initial version of word count script\n\n\n\n\nBut on GitHub, where we created the README file, the list of commits looks like\n\n\n\n\nWhat can we do? \n\n\nNothing to do but try something out. How about a \npush\n?\n\n\n git push\nTo github.com:gforsyth/wordcount.git\n ! [rejected]        master -> master (fetch first)\nerror: failed to push some refs to 'git@github.com:gforsyth/wordcount.git'\nhint: Updates were rejected because the remote contains work that you do\nhint: not have locally. This is usually caused by another repository pushing\nhint: to the same ref. You may want to first integrate the remote changes\nhint: (e.g., 'git pull ...') before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.\n\n\n\n\nAh, that didn't work. But the error message tells us what the problem is. There\nis work on the remote that we don't have locally (the README file). It wants us\nto integrate those changes into the local copy before we can push. Let's try\nthat.\n\n\ngit pull\nremote: Counting objects: 3, done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (3/3), done.\nFrom github.com:gforsyth/wordcount\n   de8fbc3..3f62d8f  master     -> origin/master\nMerge made by the 'recursive' strategy.\n README.md | 1 +\n 1 file changed, 1 insertion(+)\n create mode 100644 README.md\n\n\n\n\nThat did something. \ngit\n tells us that there was a \"Merge made by the\n'recursive' strategy\". And it says that it created a file. Let's check.\n\n\n$ ls\nREADME.md  word_count.py\n\n\n\n\nThere's the \nREADME\n! Let's take a look at the \nlog\n.\n\n\ngit log --oneline\naa92e92 Merge branch 'master' of github.com:gforsyth/wordcount\n671907a add comment for future self\n3f62d8f Create README.md\nde8fbc3 Add user-friendly print statement\n09633c8 add helper text to input function\n97fba8d allow user input of statement to word count\n47f748f Add initial version of word count script\n\n\n\n\nInteresting. There's a \nCreate README.md\n commit that we made on GitHub. That\none is further down in the list because it's older. Then there's the commit we\nmade locally, \nadd comment for future self\n.\n\n\nBut what's that most recent commit? It's called a \"Merge commit\". We'll come\nback to that a little bit later, but the quick explanation is that it's a commit\nthat helps join the \nslightly\n divergent histories of the GitHub repo and the\nlocal repo. \n\n\nOne last thing before we move on, now that we have \npull\ned and \nmerge\nd these \nchanges, we should \npush\n those changes back up to GitHub. \n\n\npush\n often!",
            "title": "merging"
        },
        {
            "location": "/git/merge/#merging",
            "text": "We can make a commit in the local repo and push it to GitHub. We can also make a\ncommit on GitHub and pull it to the local repo. But what happens if we make a\ncommit in both places?  This happens all the time when multiple people are working on the same project.\nLet's investigate how  git  handles these situations.",
            "title": "Merging"
        },
        {
            "location": "/git/merge/#add-a-readme-to-your-github-repo",
            "text": "A README is a plain text file in the root of a repository that usually contains   a brief description of a project  installation instructions  info on how to collaborate   or whatever else you want.   Open your GitHub copy of the  wordcount  repo and then click the \"Add README\"\nbutton   Don't worry about adding any additional text beyond what is automatically added\nfor you. And the default GitHub commit message for this change is reasonable, so\nyou can just click \"Commit\"   You should see something like the above, where the GitHub copy of the repo now \nhas two files:  word_count.py  and  README.md",
            "title": "Add a README to your GitHub repo"
        },
        {
            "location": "/git/merge/#make-a-change-to-the-local-repo",
            "text": "Without  performing a  git pull  now edit  word_count.py  locally to read:  happy = input(\"Enter a statement to word count: \")\n\nwords = happy.split()  # split user input on spaces\n\ncounts = {}\nfor word in words:\n    counts[word] = counts.get(word, 0) + 1\n\nprint(\"The word frequency of your statement is: \")\nprint(counts)  Stage and commit that change (remember the commands?). If we check the log in\nthe local copy of the repo, we should have something like:  $ git log --oneline\n671907a add comment for future self\nde8fbc3 Add user-friendly print statement\n09633c8 add helper text to input function\n97fba8d allow user input of statement to word count\n47f748f Add initial version of word count script  But on GitHub, where we created the README file, the list of commits looks like   What can we do?   Nothing to do but try something out. How about a  push ?   git push\nTo github.com:gforsyth/wordcount.git\n ! [rejected]        master -> master (fetch first)\nerror: failed to push some refs to 'git@github.com:gforsyth/wordcount.git'\nhint: Updates were rejected because the remote contains work that you do\nhint: not have locally. This is usually caused by another repository pushing\nhint: to the same ref. You may want to first integrate the remote changes\nhint: (e.g., 'git pull ...') before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.  Ah, that didn't work. But the error message tells us what the problem is. There\nis work on the remote that we don't have locally (the README file). It wants us\nto integrate those changes into the local copy before we can push. Let's try\nthat.  git pull\nremote: Counting objects: 3, done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (3/3), done.\nFrom github.com:gforsyth/wordcount\n   de8fbc3..3f62d8f  master     -> origin/master\nMerge made by the 'recursive' strategy.\n README.md | 1 +\n 1 file changed, 1 insertion(+)\n create mode 100644 README.md  That did something.  git  tells us that there was a \"Merge made by the\n'recursive' strategy\". And it says that it created a file. Let's check.  $ ls\nREADME.md  word_count.py  There's the  README ! Let's take a look at the  log .  git log --oneline\naa92e92 Merge branch 'master' of github.com:gforsyth/wordcount\n671907a add comment for future self\n3f62d8f Create README.md\nde8fbc3 Add user-friendly print statement\n09633c8 add helper text to input function\n97fba8d allow user input of statement to word count\n47f748f Add initial version of word count script  Interesting. There's a  Create README.md  commit that we made on GitHub. That\none is further down in the list because it's older. Then there's the commit we\nmade locally,  add comment for future self .  But what's that most recent commit? It's called a \"Merge commit\". We'll come\nback to that a little bit later, but the quick explanation is that it's a commit\nthat helps join the  slightly  divergent histories of the GitHub repo and the\nlocal repo.   One last thing before we move on, now that we have  pull ed and  merge d these \nchanges, we should  push  those changes back up to GitHub.   push  often!",
            "title": "Make a change to the local repo"
        },
        {
            "location": "/git/conflicts/",
            "text": "Conflicts\n\n\nThe last section went over one scenario where two different changesets are\nbrought together. \ngit\n used the 'recursive' strategy to smash the changes\ntogether and we got something called a 'merge commit' (again, more on this\nlater).\n\n\ngit\n was able to merge the two histories together because they were mostly\nindependent. One change was to the script \nword_count.py\n, the other was\ncreating a file called \nREADME.md\n. Not much overlap there. What about if there\nare changes made to the same file in two places?\n\n\nTime to find out!\n\n\nEdit on GitHub\n\n\nEdit the file \nword_count.py\n using GitHub's online editor and change the first \nline of the script to read: \n\n\nhappy = input(\"Enter a statement to check word frequency: \")\n\n\n\n\nCommit your changes on GitHub. \n\n\nEdit locally\n\n\nUsing \nnano\n, edit \nword_count.py\n and change the first line of the script to read:\n\n\nhappy = input(\"Enter a phrase to word count:\")\n\n\n\n\nCommit your changes to this file locally. \n\n\nWe now have changes made to the same file in two different places. What happens\nif we \npush\n?\n\n\n$ git push\n\n\n\n\nTo github.com:gforsyth/wordcount.git\n ! [rejected]        master -> master (fetch first)\nerror: failed to push some refs to 'git@github.com:gforsyth/wordcount.git'\nhint: Updates were rejected because the remote contains work that you do\nhint: not have locally. This is usually caused by another repository pushing\nhint: to the same ref. You may want to first integrate the remote changes\nhint: (e.g., 'git pull ...') before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details\n\n\n\n\nThe same thing as before -- if there are changes on a remote you have to \npull\n\nthem in before you can \npush\n.\n\n\n$ git pull\n\n\n\n\nremote: Counting objects: 3, done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (3/3), done.\nFrom github.com:gforsyth/wordcount\n   2f53c34..a717ad5  master     -> origin/master\nAuto-merging word_count.py\nCONFLICT (content): Merge conflict in word_count.py\nAutomatic merge failed; fix conflicts and then commit the result.\n\n\n\n\nAhh, that's different... \n\n\ngit\n is very clever, but if you make changes to the same line in two different\nplaces, how can \ngit\n know which change should take precedence? In this case, it\nneeds human input to decide how to proceed.\n\n\nWe're in uncharted waters, what should we do? \n\n\nWhen in doubt, \ngit status\n.\n\n\n$ git status\n\n\n\n\nOn branch master\nYour branch and 'origin/master' have diverged,\nand have 1 and 1 different commits each, respectively.\n  (use \"git pull\" to merge the remote branch into yours)\nYou have unmerged paths.\n  (fix conflicts and run \"git commit\")\n  (use \"git merge --abort\" to abort the merge)\n\nUnmerged paths:\n  (use \"git add <file>...\" to mark resolution)\n\n    both modified:   word_count.py\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\n\n\n\nstatus\n tells us that the two repositories have diverged. And we have to fix\nthe conflicts and then run \ngit commit\n.\n\n\nHow do we know what the conflicts are?\n\n\nResolving conflicts\n\n\ngit status\n told us that the file \nword_count.py\n is the trouble maker. Let's\nlook at it.\n\n\n$ cat word_count.py \n\n\n\n\n<<<<<<< HEAD\nhappy = input(\"Enter a phrase to word count: \")\n=======\nhappy = input(\"Enter a statement to check word frequency: \")\n>>>>>>> a717ad59e716184700199769a1f17b2f92874167\n\nwords = happy.split()\n\ncounts = {}\nfor word in words:\n    counts[word] = counts.get(word, 0) + 1\n\nprint(\"The word frequency of your statement is: \")\nprint(counts)\n\n\n\n\nBecause \ngit\n didn't know how to fix the conflict, it has highlighted the\nproblem area for us by surrounding it with chevrons. \n\n\nThe two versions of the same line are separated by the \n=\ns\n\n\nThere are also labels telling us where each version is from: \nHEAD\n refers to\nthe local repository, the commit hash is the commit we made on GitHub.\n\n\nTo resolve the conflict, open the file in \nnano\n and edit it to combine the two\nversions of the \ninput\n statement in whatever way you think is best. Make sure\nto remove all of the chevrons and the equal signs when you are done.\n\n\nWe ended up with this:\n\n\n$ cat word_count.py \nhappy = input(\"Enter a phrase to check word frequency: \")\n\nwords = happy.split()\n\ncounts = {}\nfor word in words:\n    counts[word] = counts.get(word, 0) + 1\n\nprint(\"The word frequency of your statement is: \")\nprint(counts)\n\n\n\n\nNow that we have manually resolved the conflict, what do we do? \n\ngit status\n told us, let's check again:\n\n\n$ git status\n\n\n\n\nOn branch master\nYour branch and 'origin/master' have diverged,\nand have 1 and 1 different commits each, respectively.\n  (use \"git pull\" to merge the remote branch into yours)\nYou have unmerged paths.\n  (fix conflicts and run \"git commit\")\n  (use \"git merge --abort\" to abort the merge)\n\nUnmerged paths:\n  (use \"git add <file>...\" to mark resolution)\n\n    both modified:   word_count.py\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\n\n\n\nRight, we need to stage the changes me made. \n\n\n$ git add word_count.py\n\n\n\n\nAnd now it's time to \ncommit\n!\n\n\n$ git commit\n\n\n\n\nMerge branch 'master' of github.com:gforsyth/wordcount\n\n# Conflicts:\n#   word_count.py\n#\n# It looks like you may be committing a merge.\n# If this is not correct, please remove the file\n#   .git/MERGE_HEAD\n# and try again.\n\n\n# Please enter the commit message for your changes. Lines starting\n# with '#' will be ignored, and an empty message aborts the commit.\n# On branch master\n# Your branch and 'origin/master' have diverged,\n# and have 1 and 1 different commits each, respectively.\n#   (use \"git pull\" to merge the remote branch into yours)\n#\n# All conflicts fixed but you are still merging.\n#\n# Changes to be committed:\n#   modified:   word_count.py\n#\n\n\n\n\nThat's different. When you \ngit commit\n after resolving a conflict, \ngit\n will\nautofill a message for you. You are welcome to use the automatic message, or\nreplace it with something else, it's up to you.\n\n\nSave and quit and we should see the following:\n\n\n[master d31e6b9] Merge branch 'master' of github.com:gforsyth/wordcount\n\n\n\n\nWe have resolved the conflict! That wasn't so bad, was it? Now that we fixed the\nconflict, we just have to push it back up to GitHub.\n\n\n$ git push\n\n\n\n\nCounting objects: 6, done.\nDelta compression using up to 12 threads.\nCompressing objects: 100% (4/4), done.\nWriting objects: 100% (6/6), 608 bytes | 0 bytes/s, done.\nTotal 6 (delta 2), reused 0 (delta 0)\nremote: Resolving deltas: 100% (2/2), completed with 2 local objects.\nTo github.com:gforsyth/wordcount.git\n   a717ad5..d31e6b9  master -> master",
            "title": "conflicts"
        },
        {
            "location": "/git/conflicts/#conflicts",
            "text": "The last section went over one scenario where two different changesets are\nbrought together.  git  used the 'recursive' strategy to smash the changes\ntogether and we got something called a 'merge commit' (again, more on this\nlater).  git  was able to merge the two histories together because they were mostly\nindependent. One change was to the script  word_count.py , the other was\ncreating a file called  README.md . Not much overlap there. What about if there\nare changes made to the same file in two places?  Time to find out!",
            "title": "Conflicts"
        },
        {
            "location": "/git/conflicts/#edit-on-github",
            "text": "Edit the file  word_count.py  using GitHub's online editor and change the first \nline of the script to read:   happy = input(\"Enter a statement to check word frequency: \")  Commit your changes on GitHub.",
            "title": "Edit on GitHub"
        },
        {
            "location": "/git/conflicts/#edit-locally",
            "text": "Using  nano , edit  word_count.py  and change the first line of the script to read:  happy = input(\"Enter a phrase to word count:\")  Commit your changes to this file locally.   We now have changes made to the same file in two different places. What happens\nif we  push ?  $ git push  To github.com:gforsyth/wordcount.git\n ! [rejected]        master -> master (fetch first)\nerror: failed to push some refs to 'git@github.com:gforsyth/wordcount.git'\nhint: Updates were rejected because the remote contains work that you do\nhint: not have locally. This is usually caused by another repository pushing\nhint: to the same ref. You may want to first integrate the remote changes\nhint: (e.g., 'git pull ...') before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details  The same thing as before -- if there are changes on a remote you have to  pull \nthem in before you can  push .  $ git pull  remote: Counting objects: 3, done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (3/3), done.\nFrom github.com:gforsyth/wordcount\n   2f53c34..a717ad5  master     -> origin/master\nAuto-merging word_count.py\nCONFLICT (content): Merge conflict in word_count.py\nAutomatic merge failed; fix conflicts and then commit the result.  Ahh, that's different...   git  is very clever, but if you make changes to the same line in two different\nplaces, how can  git  know which change should take precedence? In this case, it\nneeds human input to decide how to proceed.  We're in uncharted waters, what should we do?   When in doubt,  git status .  $ git status  On branch master\nYour branch and 'origin/master' have diverged,\nand have 1 and 1 different commits each, respectively.\n  (use \"git pull\" to merge the remote branch into yours)\nYou have unmerged paths.\n  (fix conflicts and run \"git commit\")\n  (use \"git merge --abort\" to abort the merge)\n\nUnmerged paths:\n  (use \"git add <file>...\" to mark resolution)\n\n    both modified:   word_count.py\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")  status  tells us that the two repositories have diverged. And we have to fix\nthe conflicts and then run  git commit .  How do we know what the conflicts are?",
            "title": "Edit locally"
        },
        {
            "location": "/git/conflicts/#resolving-conflicts",
            "text": "git status  told us that the file  word_count.py  is the trouble maker. Let's\nlook at it.  $ cat word_count.py   <<<<<<< HEAD\nhappy = input(\"Enter a phrase to word count: \")\n=======\nhappy = input(\"Enter a statement to check word frequency: \")\n>>>>>>> a717ad59e716184700199769a1f17b2f92874167\n\nwords = happy.split()\n\ncounts = {}\nfor word in words:\n    counts[word] = counts.get(word, 0) + 1\n\nprint(\"The word frequency of your statement is: \")\nprint(counts)  Because  git  didn't know how to fix the conflict, it has highlighted the\nproblem area for us by surrounding it with chevrons.   The two versions of the same line are separated by the  = s  There are also labels telling us where each version is from:  HEAD  refers to\nthe local repository, the commit hash is the commit we made on GitHub.  To resolve the conflict, open the file in  nano  and edit it to combine the two\nversions of the  input  statement in whatever way you think is best. Make sure\nto remove all of the chevrons and the equal signs when you are done.  We ended up with this:  $ cat word_count.py \nhappy = input(\"Enter a phrase to check word frequency: \")\n\nwords = happy.split()\n\ncounts = {}\nfor word in words:\n    counts[word] = counts.get(word, 0) + 1\n\nprint(\"The word frequency of your statement is: \")\nprint(counts)  Now that we have manually resolved the conflict, what do we do?  git status  told us, let's check again:  $ git status  On branch master\nYour branch and 'origin/master' have diverged,\nand have 1 and 1 different commits each, respectively.\n  (use \"git pull\" to merge the remote branch into yours)\nYou have unmerged paths.\n  (fix conflicts and run \"git commit\")\n  (use \"git merge --abort\" to abort the merge)\n\nUnmerged paths:\n  (use \"git add <file>...\" to mark resolution)\n\n    both modified:   word_count.py\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")  Right, we need to stage the changes me made.   $ git add word_count.py  And now it's time to  commit !  $ git commit  Merge branch 'master' of github.com:gforsyth/wordcount\n\n# Conflicts:\n#   word_count.py\n#\n# It looks like you may be committing a merge.\n# If this is not correct, please remove the file\n#   .git/MERGE_HEAD\n# and try again.\n\n\n# Please enter the commit message for your changes. Lines starting\n# with '#' will be ignored, and an empty message aborts the commit.\n# On branch master\n# Your branch and 'origin/master' have diverged,\n# and have 1 and 1 different commits each, respectively.\n#   (use \"git pull\" to merge the remote branch into yours)\n#\n# All conflicts fixed but you are still merging.\n#\n# Changes to be committed:\n#   modified:   word_count.py\n#  That's different. When you  git commit  after resolving a conflict,  git  will\nautofill a message for you. You are welcome to use the automatic message, or\nreplace it with something else, it's up to you.  Save and quit and we should see the following:  [master d31e6b9] Merge branch 'master' of github.com:gforsyth/wordcount  We have resolved the conflict! That wasn't so bad, was it? Now that we fixed the\nconflict, we just have to push it back up to GitHub.  $ git push  Counting objects: 6, done.\nDelta compression using up to 12 threads.\nCompressing objects: 100% (4/4), done.\nWriting objects: 100% (6/6), 608 bytes | 0 bytes/s, done.\nTotal 6 (delta 2), reused 0 (delta 0)\nremote: Resolving deltas: 100% (2/2), completed with 2 local objects.\nTo github.com:gforsyth/wordcount.git\n   a717ad5..d31e6b9  master -> master",
            "title": "Resolving conflicts"
        },
        {
            "location": "/jupyter/the_notebook/",
            "text": "The Notebook\n\n\nThe \nnotebook\n is a document that allows as to combine text elements (headings,\nparagraphs, hyperlinks), equations (LaTeX), figures and interactive code. The\nnotebook uses cells to divide text and code: text is formatted using markdown,\nand code is executed using the IPython kernel.\n\n\nMarkdown\n is a simple way to\nformat text; it has a plain text formatting syntax that can be converted easily\nto HTML. It is easy to learn, check out the syntax in the \"Daring Fireball\" \n(by John Gruber) \nwebpage\n.\n\n\nHow do we start the Jupyter notebook app?\n\n\nIf you follow the installation steps for this workshop listed in\nthe\n\nrequired software\n markdown,\nyou should have jupyter installed. If not, go to the link and follow the\ninstallation requirements.\n\n\nAll set? Let's open a terminal and type \njupyter notebook\n hit enter and tadah!!\nYour default browser will open with the jupyter app. It should look like this:\n\n\n\n\nTo start a new notebook click on the top right where it says \nNew\n and click \non \nPython 3\n. \n\n\n\n\nA new tab will appear in your browser and you will see:\n\n\n\n\nThe notebook opens by default with a single empty code cell. Try to write\nsome Python there and execute the cell by doing \n[shift] + [enter]\n.\n\n\nFor example:\n\n\n\n\n\n\n2 * 12\n \n\n\n\n\n\n\nprint('Hello world')\n\n\n\n\n\n\nx = 5\n # (notice there will be no output)\n\n\n\n\n\n\nprint(x * 'Hi there')\n\n\n\n\n\n\nWhat if we want to add a new cell?\n\n\nTo add a new cell (by default this will be a code cell) press the \n+\n button in\ntool bar. \n\n\nWhat if we want that cell to be a markdown cell?\n\n\nIn the tool bar you can see a dropdown menu that says \nCode\n, click there and\npick \nMarkdown\n. Now you can write some text using the markdown syntax and you\ncan write equations using \nmathjax\n. (mathjax is a web\ndisplay engine for mathematics that works in browsers. It supports most LaTeX\nsyntax.)\n\n\nFor example, type in a markdown cell:\n\n\n$\\frac{d}{dx}\\left( \\int_{0}^{x} f(u)\\,du\\right)=f(x)$\n\n\nor \n\n\n${\\frac {d}{dx}}\\arctan(\\sin({x}^{2}))=-2\\,{\\frac {\\cos({x}^{2})x}{-2+\n\\left (\\cos({x}^{2})\\right )^{2}}}$\n\n\nCool! ah? \n\n\nHow do we save our work?\n\n\n\n\n\n\nFirst, lets pick a name for our notebook. Click where it says Untitled (right\nnext to the Jupyter logo) and pick the name that you want.\n\n\n\n\n\n\nYou save your changes by either clicking in the floppy disk symbol in the tool\nbar or by doing  \n[Ctrl] + [s]\n. This will save in the directory you are located,\na \n.ipynb\n file with the name that you pick for your file. \n\n\n\n\n\n\nThe two different modes: Edit mode and Command mode [1]\n\n\nEdit mode:\n\n\n\n\n\n\nWe know we are in this mode when we see a green cell border and a prompt \nshowing in the editor area.\n\n\n\n\n\n\nWe enter in \nedit mode\n by pressing \nEnter\n or clicking on the cell.\n\n\n\n\n\n\nWhen we are in edit mode, we can type into the cell, like a normal text editor.\n\n\n\n\n\n\nCommand mode:\n\n\n\n\n\n\nWe know we are in this mode when we see a grey cell border with a left blue\nmargin.\n\n\n\n\n\n\nWe enter in \ncommand mode\n by pressing \nEsc\n or clicking outside the cell's\narea.\n\n\n\n\n\n\nIn \ncommand mode\n the certain keys are mapped to shortcuts that help with\n  common actions.\n\n\n\n\n\n\nYou can find a list of the shortcuts by selecting \nHelp->Keyboard Shortcuts\n\nfrom the notebook menu bar. Check them out and have fun!\n\n\nHow we shut down the Kernel?\n\n\nOnce you close your notebook, you will see in the main Jupyter page that your \nnotebook file has a green book symbol. You should click in the box at the left \nof that symbol, and then click where it says \nshutdown\n. Finally, go to the\nterminal that we use at the beginning to open the jupyter notebook and type\n\n[Ctrl] + [c]\n and you are all done!\n\n\nNbviewer\n\n\nNbviewer\n is a free webservice that allows you to share static html versions of hosted notebook files. If a notebook is publicly available, by giving its url to the Viewer, you should be able to view it [2]. You just need to host the notebook file (.ipynb extension) online and enter the public URL to the file on the nbviewer Go! box. The notebook will be rendered like a static webpage: visitors can read everything, but they cannot interact with the code. \n\n\nReferences:\n\n\n[1] \nNotebook Basics: Modal Editor\n\n\n[2] \nNbviewer",
            "title": "Navigating the notebook"
        },
        {
            "location": "/jupyter/the_notebook/#the-notebook",
            "text": "The  notebook  is a document that allows as to combine text elements (headings,\nparagraphs, hyperlinks), equations (LaTeX), figures and interactive code. The\nnotebook uses cells to divide text and code: text is formatted using markdown,\nand code is executed using the IPython kernel.  Markdown  is a simple way to\nformat text; it has a plain text formatting syntax that can be converted easily\nto HTML. It is easy to learn, check out the syntax in the \"Daring Fireball\" \n(by John Gruber)  webpage .",
            "title": "The Notebook"
        },
        {
            "location": "/jupyter/the_notebook/#how-do-we-start-the-jupyter-notebook-app",
            "text": "If you follow the installation steps for this workshop listed in\nthe required software  markdown,\nyou should have jupyter installed. If not, go to the link and follow the\ninstallation requirements.  All set? Let's open a terminal and type  jupyter notebook  hit enter and tadah!!\nYour default browser will open with the jupyter app. It should look like this:   To start a new notebook click on the top right where it says  New  and click \non  Python 3 .    A new tab will appear in your browser and you will see:   The notebook opens by default with a single empty code cell. Try to write\nsome Python there and execute the cell by doing  [shift] + [enter] .  For example:    2 * 12      print('Hello world')    x = 5  # (notice there will be no output)    print(x * 'Hi there')",
            "title": "How do we start the Jupyter notebook app?"
        },
        {
            "location": "/jupyter/the_notebook/#what-if-we-want-to-add-a-new-cell",
            "text": "To add a new cell (by default this will be a code cell) press the  +  button in\ntool bar.",
            "title": "What if we want to add a new cell?"
        },
        {
            "location": "/jupyter/the_notebook/#what-if-we-want-that-cell-to-be-a-markdown-cell",
            "text": "In the tool bar you can see a dropdown menu that says  Code , click there and\npick  Markdown . Now you can write some text using the markdown syntax and you\ncan write equations using  mathjax . (mathjax is a web\ndisplay engine for mathematics that works in browsers. It supports most LaTeX\nsyntax.)  For example, type in a markdown cell:  $\\frac{d}{dx}\\left( \\int_{0}^{x} f(u)\\,du\\right)=f(x)$  or   ${\\frac {d}{dx}}\\arctan(\\sin({x}^{2}))=-2\\,{\\frac {\\cos({x}^{2})x}{-2+\n\\left (\\cos({x}^{2})\\right )^{2}}}$  Cool! ah?",
            "title": "What if we want that cell to be a markdown cell?"
        },
        {
            "location": "/jupyter/the_notebook/#how-do-we-save-our-work",
            "text": "First, lets pick a name for our notebook. Click where it says Untitled (right\nnext to the Jupyter logo) and pick the name that you want.    You save your changes by either clicking in the floppy disk symbol in the tool\nbar or by doing   [Ctrl] + [s] . This will save in the directory you are located,\na  .ipynb  file with the name that you pick for your file.",
            "title": "How do we save our work?"
        },
        {
            "location": "/jupyter/the_notebook/#the-two-different-modes-edit-mode-and-command-mode-1",
            "text": "Edit mode:    We know we are in this mode when we see a green cell border and a prompt \nshowing in the editor area.    We enter in  edit mode  by pressing  Enter  or clicking on the cell.    When we are in edit mode, we can type into the cell, like a normal text editor.    Command mode:    We know we are in this mode when we see a grey cell border with a left blue\nmargin.    We enter in  command mode  by pressing  Esc  or clicking outside the cell's\narea.    In  command mode  the certain keys are mapped to shortcuts that help with\n  common actions.    You can find a list of the shortcuts by selecting  Help->Keyboard Shortcuts \nfrom the notebook menu bar. Check them out and have fun!",
            "title": "The two different modes: Edit mode and Command mode [1]"
        },
        {
            "location": "/jupyter/the_notebook/#how-we-shut-down-the-kernel",
            "text": "Once you close your notebook, you will see in the main Jupyter page that your \nnotebook file has a green book symbol. You should click in the box at the left \nof that symbol, and then click where it says  shutdown . Finally, go to the\nterminal that we use at the beginning to open the jupyter notebook and type [Ctrl] + [c]  and you are all done!",
            "title": "How we shut down the Kernel?"
        },
        {
            "location": "/jupyter/the_notebook/#nbviewer",
            "text": "Nbviewer  is a free webservice that allows you to share static html versions of hosted notebook files. If a notebook is publicly available, by giving its url to the Viewer, you should be able to view it [2]. You just need to host the notebook file (.ipynb extension) online and enter the public URL to the file on the nbviewer Go! box. The notebook will be rendered like a static webpage: visitors can read everything, but they cannot interact with the code.",
            "title": "Nbviewer"
        },
        {
            "location": "/jupyter/the_notebook/#references",
            "text": "[1]  Notebook Basics: Modal Editor  [2]  Nbviewer",
            "title": "References:"
        },
        {
            "location": "/jupyter/1/",
            "text": "Content under Creative Commons Attribution license CC-BY 4.0, code under BSD 3-Clause License \u00a9 2016 L.A. Barba, N.C. Clementi, G.F. Forsyth.  Based on \nJITcode-MechE\n, also under CC-BY and MIT licenses, 2014.\n\n\nJupyter Notebooks\n\n\nIntro to the python scientific stack\n\n\nWelcome to the Jupyter notebook, a place where you can combine markdown text, LaTeX equations, code and results in a single document. \n\n\nIn this notebook we will introduce useful python libraries that are important if you want to do scientific computing. We will learn:\n\n\n\n\n\n\nHow to import data, manipulate arrays and do operations with them using \nNumpy\n.\n\n\n\n\n\n\nHow to do some stats and data fitting using \nNumpy\n and \nScipy\n.\n\n\n\n\n\n\nHow to do nice plots using \nmatplotlib\n.\n\n\n\n\n\n\nContext \u2014 Earth temperature over time\n\n\nIs global temperature rising? How much? This is a question of burning importance in today's world!\n\n\nData about global temperatures are available from several sources: NASA, the National Climatic Data Center (NCDC) and the University of East Anglia in the UK. Check out the \nUniversity Corporation for Atmospheric Research\n (UCAR) for an in-depth discussion.\n\n\nThe \nNASA Goddard Space Flight Center\n is one of our sources of global climate data. They produced this video showing a color map of the changing global surface \ntemperature anomalies\n from 1880 to 2011.\n\n\nThe term \nglobal temperature anomaly\n means the difference in temperature with respect to a reference value or a long-term average. It is a very useful way of looking at the problem and in many ways better than absolute temperature. For example, a winter month may be colder than average in Washington DC, and also in Miami, but the absolute temperatures will be different in both places.\n\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo('lyb4gau3LyI')\n\n\n\n\n\n\n\nHow would we go about understanding the \ntrends\n from the data on global temperature?\n\n\nThe first step in analyzing unknown data is to generate some simple plots using \nmatplotlib\n. We are going to look at the temperature-anomaly history, contained in a file, and make our first plot to explore this data. \n\n\nWe are going to smooth the data and then we'll fit a line to it to find a trend, plotting along the way to see how it all looks.\n\n\nLet's get started!\n\n\nStep 1: Read a data file.\n\n\nWe took the data from the \nNOAA\n (National Oceanic and Atmospheric Administration) webpage. Feel free to play around with the webpage and analyze data on your own, but for now, let's make sure we're working with the same dataset.\n\n\nFirst we will download the data from the web by using some Python magic and invoke the command \nwget\n.  We will save the file in the folder \nresources\n under the name:\n\n\nland_global_temperature_anomaly-1880-2015.csv\n\n\nThis file contains the year and month on the first column and 12 monthly averages of land temperature anomaly listed sequentially on the second column, from 1880 to 2015. We will read the file, then make an initial plot to see what it looks like.\n\n\nIn the NOAA web page you can select the data that you want to download as  a \ncsv\n file and this will automatically open another tab with the data. We want to get that data so we will use \nwget\n but instead from a terminal we will invoke it from the notebook, yes! from the notebook.\n\n\nTo run shell commands in the notebook we just need to use an exclamation mark \n!\n before the command:\n\n\n!wget -O land_global_temperature_anomaly-1880-2015.csv  https://www.ncdc.noaa.gov/cag/time-series/global/globe/land/all/1/1880-2015.csv \n\n\n\n\n--2017-01-04 06:37:59--  https://www.ncdc.noaa.gov/cag/time-series/global/globe/land/all/1/1880-2015.csv\nResolving www.ncdc.noaa.gov (www.ncdc.noaa.gov)... 205.167.25.171, 205.167.25.172, 2610:20:8040:2::171, ...\nConnecting to www.ncdc.noaa.gov (www.ncdc.noaa.gov)|205.167.25.171|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/plain]\nSaving to: \u2018land_global_temperature_anomaly-1880-2015.csv\u2019\n\nland_global_tempera     [ <=>                ]  23.24K   132KB/s    in 0.2s\n\n2017-01-04 06:38:00 (132 KB/s) - \u2018land_global_temperature_anomaly-1880-2015.csv\u2019 saved [23796]\n\n\n\nWe got the data! Now we need to load it so we can work with it. The first thing to do is to load our favorite library: the \nNumPy\n library for array operations.\n\n\nimport numpy\n\n\n\n\nPretty easy, right? Now to load the file, we use a function from the NumPy library called \nloadtxt()\n. To tell Python where to look for this function, we precede the function name with the library name, and use a dot between the two names. You can read the documentation in this \nlink\n.\n\n\nloadtxt\n expects every row in the same format, so we need to check that that's true of this data file. \nWe could open it in Python, but we've already learned a useful command for peeking into files, right? \nLet's use \nhead\n to show the first ten lines of the \ncsv\n file, again using a \n!\n to denote a shell command.\n\n\n!head land_global_temperature_anomaly-1880-2015.csv\n\n\n\n\nGlobal Land Temperature Anomalies\nUnits: Degrees Celsius\nBase Period: 1901-2000\nYear,Value\n188001,-0.1538\n188002,-0.5771\n188003,-0.7250\n188004,-0.2990\n188005,-0.2098\n188006,-0.3002\n\n\n\nWe see that the first 4 lines of the file are metadata but those will mess up \nloadtxt\n. \nTo fix this, we will pass an argument to \nloadtxt\n to tell it to skip the first 4 rows.\n\n\nnumpy.loadtxt(fname='land_global_temperature_anomaly-1880-2015.csv',\n              delimiter=',', skiprows=4)\n\n\n\n\narray([[  1.88001000e+05,  -1.53800000e-01],\n       [  1.88002000e+05,  -5.77100000e-01],\n       [  1.88003000e+05,  -7.25000000e-01],\n       ..., \n       [  2.01510000e+05,   1.32120000e+00],\n       [  2.01511000e+05,   1.28280000e+00],\n       [  2.01512000e+05,   1.87220000e+00]])\n\n\n\nWe called the function with three parameters: the file name and path, the delimiter that separates each value on a line (a comma in this case), and \nskiprows=4\n which tells Numpy to omit the first 4 rows. \nNote that the first two parameters are strings (made up of characters) and we put them in quotes.\n\n\nAs the output of the function, we get an array. Because it's rather big, Python shows only a few rows and columns of the array. \n\n\nSo far, so good. Now, what if we want to manipulate this data? Or plot it? We need to refer to it with a name. We've only just read the file, but we did not assign the array any name! Let's try again.\n\n\nT_land = numpy.loadtxt(fname='land_global_temperature_anomaly-1880-2015.csv', \n                       delimiter=',', skiprows=4)\n\n\n\n\nThat's interesting. Now, we don't see any output from the function call. Why? It's simply that the output was stored into the variable \nT_land\n, so to see it, we can do:\n\n\nprint(T_land)\n\n\n\n\n[[  1.88001000e+05  -1.53800000e-01]\n [  1.88002000e+05  -5.77100000e-01]\n [  1.88003000e+05  -7.25000000e-01]\n ..., \n [  2.01510000e+05   1.32120000e+00]\n [  2.01511000e+05   1.28280000e+00]\n [  2.01512000e+05   1.87220000e+00]]\n\n\n\nAh, there it is! Let's find out how big the array is. For that, we use a cool NumPy function called \nshape()\n:\n\n\nnumpy.shape(T_land)\n\n\n\n\n(1632, 2)\n\n\n\nAgain, we've told Python where to find the function \nshape()\n by attaching it to the library name with a dot. However, NumPy arrays also happen to have a property shape that will return the same value, so we can get the same result another way:\n\n\nT_land.shape\n\n\n\n\n(1632, 2)\n\n\n\nIt's just shorter. The array \nT_shape\n holding our temperature-anomaly data has two columns and 1632 rows. Since we said we had monthly data, how many years is that?\n\n\n1632/12\n\n\n\n\n136.0\n\n\n\nThat's right: from 1880 through 2015.\n\n\nStep 2: Plot the data.\n\n\nWe will display the data in two ways: as a time series of the monthly temperature anomalies versus time, and as a histogram. To be fancy, we'll put both plots in one figure.\n\n\nLet's first load our plotting library, called \nmatplotlib\n. To get the plots inside the notebook (rather than as popups), we use a special \"magic\" command, \n%matplotlib inline\n:\n\n\nfrom matplotlib import pyplot\n%matplotlib inline\n\n\n\n\nWhat's this \nfrom\n business about?  \nmatplotlib\n is a pretty big (and awesome!) library.  All that we need is a subset of the library for creating 2D plots, so we ask for the \npyplot\n module of the \nmatplotlib\n library.  \n\n\nPlotting the time series of temperature is as easy as calling the function \nplot()\n from the module \npyplot\n.  \n\n\nBut remember the shape of \nT_land\n? It has two columns and the temperature-anomaly values are in the second column. We extract the values of the second column by specifying 1 as the second index (the first column has index 0) and using the colon notation \n:\n to mean \nall rows\n. Check it out: \n\n\npyplot.plot(T_land[:,1])\n\n\n\n\n[<matplotlib.lines.Line2D at 0x7fcfd42274a8>]\n\n\n\n\n\nYou can add a semicolon at the end of the plotting command to avoid that stuff that appeared on top of the figure, that \nOut[x]: [< ...>]\n ugliness. Try it.\n\n\nDo you see a trend in the data?\n\n\nThe plot above is certainly useful, but wouldn't it be nicer if we could look at the data relative to the year, instead of the location of the data in the array?\n\n\nThe plot function can take another input; let's get the year displayed as well.\n\n\npyplot.plot(T_land[:,0],T_land[:,1]);\n\n\n\n\n\n\nWhat happened? It does not look like the previous plot. \nIt looks a little sparse. \nDo you know why? \nNotice that in the \ncsv\n file the column for the years are integers, and even though we understand what they mean, is that what we want to plot? \nLet's take a closer look by just plotting the first two years. \n\n\npyplot.plot(T_land[0:25,0],T_land[0:25,1]);\n\n\n\n\n\n\nDo you notice what is going on? We are not using the right scale to plot our dates. However, \nPython\n is really cool and we can solve this in a nice and neat way. We will use \nnumpy.arange()\n and specify a data type for dates that you can explore in depth in this \nlink\n. \n\n\ndate = numpy.arange('1880', '2016', dtype=('datetime64[M]'))\n\n\n\n\nWe passed to \nnumpy.arange()\n three arguments, the beginning and the end of the array, and  \ndtype=datetime64[M]\n that indicates that we want to show every month in the range determined by the beginning and end of the array. Let's print some of the elements of the array \ndate\n: \n\n\nprint(date[0:15])\n\n\n\n\n['1880-01' '1880-02' '1880-03' '1880-04' '1880-05' '1880-06' '1880-07'\n '1880-08' '1880-09' '1880-10' '1880-11' '1880-12' '1881-01' '1881-02'\n '1881-03']\n\n\n\nWe create this array but we can not use it directly to plot, we need a dummy array the same length as our data array. Then we will use the \ndate\n array to plot the ticks on the \nx-axis\n.\n\n\nlen(T_land)\n\n\n\n\n1632\n\n\n\nThe \ndummy\n array should contain 1632 equally space elements but it doesn't matter when it starts and when it finishes.\nWhy? Because we aren't going to use that for display!\nFor simplity we will start at 1 and finish at 1632. \nWe will use \nnumpy.linspace()\n to create our array. The first argument indicates the start of the array, the second the end, and the third one indicates the number of elements we want our array to have.\n\n\ndummy = numpy.linspace(1, 1632, 1632)\n\n\n\n\nWe will use this array and the \ndate\n array to make the plot again but now in the \nx-axis\n we will plot the \ndate\n. Because plotting all the months will end up being non-readable, we will use \nslices\n to plot the \nx-ticks\n every ten years (120 months).    \n\n\npyplot.xticks(dummy[::12*10], date[::12*10] , rotation=75)\npyplot.plot(dummy,T_land[:,1]);\n\n\n\n\n\n\nNow we have a better plot but if I give you this plot without any information you would not be able to figure out what kind of data it is! We need labels on the axis, a title and why not a better color, font and size of the ticks. \n\nPublication quality\n plots should always be your standard for plotting. \nHow you present your data will allow others (and probably you in the future) to better understand your work. \n\n\nLet's make the font of a specific size and type. \nWe don't want to write this out every time we create a plot. \nInstead, the next few lines of code will apply for all the plots we create from now on.\n\n\nfrom matplotlib import rcParams\nrcParams['font.family'] = 'serif'\nrcParams['font.size'] = 16\n\n\n\n\nWe are going to plot the same plot as before but now we will add a few things to make it prettier and \npublication quality\n.\n\n\npyplot.figure(figsize=(10,5))\npyplot.xticks(dummy[::12*10], date[::12*10] , rotation=75)\npyplot.plot(dummy,T_land[:,1], color='#2929a3', ls='-', lw=1)\npyplot.title('Land global temperature anomalies. \\n')\npyplot.xlabel('Year-Month')\npyplot.ylabel('Land temperature anomaly [\u00b0C]')\npyplot.grid();\n\n\n\n\n\n\nBetter ah? Feel free to play around with the parameters and see how it changes. \n\n\nDo you see a trend in the data?\n\n\nThe temperature anomaly certainly seems to show an increasing trend. But we're not going to stop there, of course. It's not that easy to convince people that the planet is warming, as you know.\n\n\nPlotting a histogram is as easy as calling the function \nhist()\n. Why should it be any harder?\n\n\npyplot.figure(figsize=(10,5))\npyplot.hist(T_land[:,1])\npyplot.xlabel('Land temperature anomaly [\u00b0C]')\npyplot.ylabel('Frequency');\n\n\n\n\n\n\nYou can control several parameters of the \nhist()\n plot. Learn more by reading the manual page (yes, you have to read the manual sometimes!). The first option is the number of bins\u2014the default is 10\u2014but you can also change the appearance (color, transparency). Try some things out.\n\n\npyplot.figure(figsize=(10,5))\npyplot.hist(T_land[:,1], 25, normed=True, color='g', alpha=0.5)\npyplot.xlabel('Land temperature anomaly [\u00b0C]')\npyplot.ylabel('Normalized frequency');\n\n\n\n\n\n\nWhat does this plot tell you about the data? It's more interesting than just an increasing trend, that's for sure. You might want to look at more statistics now: mean, median, standard deviation ... NumPy makes that easy for you:\n\n\nmean_T = numpy.mean(T_land[:,1])\nmedian_T = numpy.median(T_land[:,1])\n\nprint('The mean value is {:.5} and the median {:.5}'.format(mean_T,\n                                                            median_T))\n\n\n\n\nThe mean value is 0.04031 and the median -0.0276\n\n\n\nvariance_T = numpy.var(T_land[:,1])\nsigma_T = numpy.sqrt(variance_T)\nprint('The variance is {:.5} and the standard deviation {:.5}'.format(variance_T,\n                                                                      sigma_T))\n\n\n\n\nThe variance is 0.28041 and the standard deviation 0.52954\n\n\n\nWith these data, we can plot the probability density function (pdf) that correspond to this data, compare with the histogram and see how far from a normal distribution we are.\n\n\nIn order to do that, we need to import the module \nstats\n from \nSciPy\n, another great python library that contains tools for scientific computing. \n\n\nfrom scipy import stats\n\n\n\n\nbins = numpy.linspace(min(T_land[:,1]), max(T_land[:,1]), 40)\n\npyplot.figure(figsize=(10,5))\npyplot.hist(T_land[:,1], bins, normed=True, facecolor='g', alpha=0.5)\npyplot.plot(bins, stats.norm.pdf(bins, mean_T, sigma_T),\n            color='#ff5733', ls='-', lw=2.5)\npyplot.xlim(min(T_land[:,1]), max(T_land[:,1]))\npyplot.xlabel('Land temperature anomaly [\u00b0C]')\npyplot.ylabel('Normalized frequency')\npyplot.grid();\n\n\n\n\n\n\nThis is fun. Finally, we'll put both plots on the same figure using the \nsubplot()\n function, which creates a grid of plots. The argument tells this function how many rows and columns of sub-plots we want, and where in the grid each plot will go.\n\n\npyplot.figure(figsize=(12,4))  \n\npyplot.subplot(121)   # creates a grid of 1 row, 2 columns and \n                      # selects the first plot\n\npyplot.xticks(dummy[::12*10], date[::12*10] , rotation=75)   \npyplot.plot(dummy,T_land[:,1], color='#2929a3', ls='-', lw=1) \npyplot.xlabel('Year-Month')\npyplot.ylabel('Land temperature anomaly [\u00b0C]')\npyplot.grid()\n\npyplot.subplot(122)            # prepares for the second plot\n\nbins = numpy.linspace(min(T_land[:,1]), max(T_land[:,1]), 40)\npyplot.hist(T_land[:,1], bins, normed=True, color='#2929a3',\n            alpha=0.4, histtype= 'stepfilled')\npyplot.plot(bins, stats.norm.pdf(bins, mean_T, sigma_T),\n            color='#ff5733', ls='-', lw=2.5)\npyplot.xlim(min(T_land[:,1]), max(T_land[:,1]))\npyplot.xlabel('Land temperature anomaly [\u00b0C]')\npyplot.ylabel('Normalized frequency')\npyplot.grid();\n\n\n\n\n\n\nDid you notice that the histogram looks a little different? You can tweak the type of histogram you want to plot by passing the \nhisttype\n argument to the \nhist()\n function. Curious about what other options do you have? Go back to the \ndocumentation\n and read about the other options that you have. \n\n\nStep 3: Smooth the data and do regression\n\n\nYou see a lot of fluctuations on the time series, so you might be asking yourself \"How can I smooth it out?\" No? Let's do it anyway.\n\n\nOne possible approach to smooth the data (there are others) is using a \nmoving average\n, also known as a sliding-window average. This is defined as:\n\n\n\n\n\\hat{x}_{i,n} = \\frac{1}{n} \\sum_{j=1}^{n} x_{i-j}\n\n\n\n\nThe only parameter to the moving average is the value \nn\n. As you can see, the moving average smooths the set of data points by creating a new data set consisting of local averages (of the \nn\n previous data points) at each point in the new set.\n\n\nA moving average is technically a \nconvolution\n, and luckily NumPy has a built-in function for that, \nconvolve()\n.\n\n\nWe need to pick \nN\n and create the \"window\" we want to use to smooth the data. but we might want to change \nN\n and try different values, so what if we create a simple function so then you can call this function with different parameters and see how the smoothed data varies. \n\n\ndef smooth_data(N, data):\n    \"\"\"\n    Returns smoothed data using a sliding_moving avarage.\n\n    Arguments:\n    ----------\n    N (int)       : amount of data values we want to average.\n    data (array)  : array of data we want to smooth.\n\n\n    Returns:\n    --------\n    smooth (array): array with smoothed data.\n    \"\"\"\n\n    window = numpy.ones(N)/N\n    smooth = numpy.convolve(data, window, 'same')\n\n    return smooth\n\n\n\n\nDid you notice the function \nones()\n? It creates an array filled with ... you guessed it: ones!\n\n\nOk! We \ndefine\n our function and now we want to call the it with the parameters we want. But before calling the function you might be wondering what is the text that is between triple quotes?  That is a \ndocstring\n and they explain what the function does. It's a good practice to always write the corresponding docstring, your future \"you\" and colleagues will thank you. \n\n\nNow we can read the docstring and see what our function does.  But let's suppose you are working on a big project and you see a function and you want to know what a function does; you can use the question mark \n?\n and you'll have this information. Let's try it:\n\n\n?smooth_data\n\n\n\n\nCool uh? Let's use this information and call our function with the land temperature data and \nN=12\n, we want each point of the smoothed data to be an average of 12 months. \n\n\nWe also want the return data to be saved in a variable that we can use to plot afterwards, then:\n\n\nsmooth = smooth_data(12, T_land[:,1])\n\n\n\n\npyplot.figure(figsize=(10, 5))\npyplot.xticks(dummy[::12*10], date[::12*10] , rotation=75)   \npyplot.plot(dummy, smooth, color='r', ls='-', lw=2)\n\npyplot.xlabel('Year-Month')\npyplot.ylabel('Land temperature anomaly [\u00b0C]')\npyplot.grid(); \n\n\n\n\n\n\nLooking at the plot, we can still see a trend, but the range of values is smaller. Let's plot the original time series together with the smoothed version:\n\n\npyplot.figure(figsize=(10, 5))\npyplot.xticks(dummy[::12*10], date[::12*10] , rotation=75)   \npyplot.plot(dummy, T_land[:,1], color='#2929a3', ls='-', lw=1, alpha=0.5) \npyplot.plot(dummy, smooth, color='r', ls='-', lw=2)\n\npyplot.xlabel('Year-Month')\npyplot.ylabel('Land temperature anomaly [\u00b0C]')\npyplot.grid(); \n\n\n\n\n\n\n\nThat is interesting! The smoothed data follows the trend nicely but has much less noise. Well, that is what filtering data is all about. \n\n\nLet's now fit a straight line through the temperature-anomaly data, to see the trends. We need to perform a least-squares linear regression to find the slope and intercept of a line \n\n\n\n\ny = mx+b\n\n\n\n\nthat fits our data. Thankfully, Python and NumPy are here to help with the \npolyfit()\n function and the \npoly1d()\n class. The \npolyfit()\n function takes three arguments: the two array variables \nx\n and \ny\n, and the order of the polynomial for the fit (in this case, 1 for linear regression). The \npoly1d()\n class takes an array with the polynomial's coefficient in decreasing power and it gives us back the polynomial itself ready to be used as a function. \n\n\nTo fit our data we will keep using our \ndummy\n variable in the abscissa axis. However, if we want to use our linear regression to have an estimate of the temperature anomaly value in a certain year, we need to know which index in the array is the one that correspond to the date we want to give as input. Once we know that index we can evaluate our \ndummy\n variable in that index and pass that value to the \nfitting-function\n. \n\n\nWe call \npolyfit()\n with our data and we save the coefficients of our linear polynomial into the \ntuple\n \n(m, b)\n. Then, we feed \npoly1d()\n with \n(m, b)\n to get our linear regression as a function.   \n\n\nm, b = numpy.polyfit(dummy, T_land[:,1], 1)\n\nf_linear = numpy.poly1d((m, b)) \n\n\n\n\nprint(f_linear)\n\n\n\n\n0.0008322 x - 0.6392\n\n\n\npyplot.figure(figsize=(10, 5))\n\npyplot.xticks(dummy[::12*10], date[::12*10] , rotation=75)   \npyplot.plot(dummy, T_land[:,1], color='#2929a3', ls='-', lw=1, alpha=0.5) \npyplot.plot(dummy, f_linear(dummy), 'k--', linewidth=2,\n            label='linear regression')\npyplot.xlabel('Year-Month')\npyplot.ylabel('Land temperature anomaly [\u00b0C]')\npyplot.legend(loc='best', fontsize=15)\npyplot.grid();\n\n\n\n\n\n\nIf we want to know the value of the land temperature anomaly that our linear regression gives for a certain year we need to know the index in the array that corresponds to the date we want. \nLet's create a function that does that for us, with the help of \nnumpy.where()\n this task should be easy.\n\n\nOur \ndate\n array has a specific data type, and we need to take this into account when we use the \nnumpy.where()\n function. Why don't we take a look at the array to refresh our minds.\n\n\n#Our date array\ndate\n\n\n\n\narray(['1880-01', '1880-02', '1880-03', ..., '2015-10', '2015-11',\n       '2015-12'], dtype='datetime64[M]')\n\n\n\n#A specific element of our array\ndate[23]\n\n\n\n\nnumpy.datetime64('1881-12')\n\n\n\nWith this information in mind we design our function \ndate_index()\n that takes as inputs the array that contains the dates and the desired date, and it'll return the index on the date's array that correspond to the date we want.\n\n\nWhat if we pass a date that is not in the array, or we pass it in the wrong format?\n\nTo prevent this we use \ntry and except\n to help us handle errors in an easy way. \n\n\ndef date_index(dates_array, date_desired):\n    \"\"\"\n    Looks for the index of a specific date in the dates_array.\n\n    Arguments:\n    ----------\n    dates_array  : array,  contains the dates (dtype='datetime64[M]')\n    date_desired : str, in the format YYYY-MM we want to know their \n                        index.\n\n    Returns:\n    --------\n    date_index   : array, index in the dates_array that correspond to the\n                   desired_date    \n    \"\"\"\n    try:    \n        date_index = numpy.where(dates_array==numpy.datetime64(date_desired))\n    except ValueError:\n        raise ValueError('Invalid date entered (bad format)')\n\n\n    if len(date_index[0]) > 0:\n        return date_index[0][0]\n    else:\n        raise ValueError(\"Date not found in date range\")\n\n\n\n\n\nIf we did things right, then '1880-01' should correspond to index 0, and '2015-12' to 1631.  \n\n\nidx_first = date_index(date, '1880-01')\nidx_last  = date_index(date, '2015-12')\nprint('The index for 1880-01 is: {} and for 2015-12: {}'.format(idx_first,\n                                                                idx_last))\n\n\n\n\nThe index for 1880-01 is: 0 and for 2015-12: 1631\n\n\n\nIf we force our function to fail, it should \nraise\n the errors we told it to raise. Let's try it!\n\n\n# Bad format date\ndate_index(date, '1890-13')\n\n\n\n\n---------------------------------------------------------------------------\n\nValueError                                Traceback (most recent call last)\n\n<ipython-input-39-74303b0d9f6e> in date_index(dates_array, date_desired)\n     16     try:\n---> 17         date_index = numpy.where(dates_array==numpy.datetime64(date_desired))\n     18     except ValueError:\n\n\nValueError: Month out of range in datetime string \"1890-13\"\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nValueError                                Traceback (most recent call last)\n\n<ipython-input-41-713570594e34> in <module>()\n      1 # Bad format date\n----> 2 date_index(date, '1890-13')\n\n\n<ipython-input-39-74303b0d9f6e> in date_index(dates_array, date_desired)\n     17         date_index = numpy.where(dates_array==numpy.datetime64(date_desired))\n     18     except ValueError:\n---> 19         raise ValueError('Invalid date entered (bad format)')\n     20 \n     21\n\n\nValueError: Invalid date entered (bad format)\n\n\n\n# Date out of range\ndate_index(date, '1879-01')\n\n\n\n\n---------------------------------------------------------------------------\n\nValueError                                Traceback (most recent call last)\n\n<ipython-input-42-d4cc294cbd7c> in <module>()\n      1 # Date out of range\n----> 2 date_index(date, '1879-01')\n\n\n<ipython-input-39-74303b0d9f6e> in date_index(dates_array, date_desired)\n     23         return date_index[0][0]\n     24     else:\n---> 25         raise ValueError(\"Date not found in date range\")\n     26\n\n\nValueError: Date not found in date range\n\n\n\nNow we have the beginning and end indices, let's use them to see what our linear regression gives us when we evaluate  specific dates. For example:\n\n\nyear_first = dummy[idx_first]\nyear_last = dummy[idx_last]\n\nreg_first = f_linear(year_first)\nreg_last = f_linear(year_last)\n\nprint('Temp anomaly estimation for 1880-01: {}'.format(reg_first))\nprint('Temp anomaly estimation for 2015-12: {}'.format(reg_last))\n\n\n\n\nTemp anomaly estimation for 1880-01: -0.6383595707557367\nTemp anomaly estimation for 2015-12: 0.7189794236969136\n\n\n\nLet's compare that to the actual dataset:\n\n\nprint('Temp anomaly data for 1880-01: {}'.format(T_land[idx_first,1]))\nprint('Temp anomaly data for 2015-12: {}'.format(T_land[idx_last,1]))\n\n\n\n\nTemp anomaly data for 1880-01: -0.1538\nTemp anomaly data for 2015-12: 1.8722\n\n\n\nIt seems like a linear regression might not be the best way of modeling this data, but what if we try doing the regression in \"chunks\"? \nIf we take a look at the plot, we see that around the 1970's the slope starts to be more pronounced. \nMaybe doing a linear regression in two different ranges is a good idea. \nWriting a function to do this is a smart decision so we can reuse it in the future if we want to do more than two regressions.\n\n\ndef regression_range(start_date, end_date, date_array, x, T):\n    \"\"\"\n    Perform a linear regression in the range of dates provided.\n\n    Arguments:\n    ----------\n    start_date: str 'YYYY-MM', starting date. \n    end_date: str 'YYYY-MM', ending date.\n    date_array: array, it contains the dates in the format 'YYYY-MM'.\n    x: dummy array we need in order to plot to plot\n    T: temperature anomalies data array.\n\n    Returns:\n    --------\n    x_range: array, range of the x array used to perform the regression.\n    T_range: array, range of the T array used to perform the regression.\n    f_range: function, linear regression performed on the range provided.\n    \"\"\"\n\n    idx_start = date_index(date_array, start_date)\n    idx_end = date_index(date_array, end_date)\n\n    x_range = x[idx_start: idx_end+1]\n    T_range = T[idx_start: idx_end+1]\n\n    m, b = numpy.polyfit(x_range, T_range, 1)\n\n    f_range = numpy.poly1d((m,b)) \n\n    return x_range, T_range, f_range\n\n\n\n\n\nNow we call the function in two different ranges, one from \n1880-01\n to \n1969-12\n and the other from \n1970-01\n to \n2015-12\n:\n\n\n# First range \nx1, T1, f1 = regression_range('1880-01', '1969-12', date,\n                              dummy, T_land[:,1])\n\n\n\n\n# Second range\nx2, T2, f2 = regression_range('1970-01', '2015-12', date,\n                              dummy, T_land[:,1])\n\n\n\n\npyplot.figure(figsize=(10, 5))\n\npyplot.xticks(dummy[::12*10], date[::12*10] , rotation=75)   \npyplot.plot(dummy, T_land[:,1], color='#2929a3', ls='-', lw=1,\n            alpha=0.4, label = 'raw data')\n\npyplot.plot(x1, f1(x1), 'g--', linewidth=2, label = 'reg 1880-1969')\npyplot.plot(x2, f1(x2), 'r--', linewidth=2)\npyplot.plot(x2, f2(x2), 'b--', linewidth=2, label = 'reg 1970-2015')\npyplot.xlabel('Year-Month')\npyplot.ylabel('Land temperature anomaly [\u00b0C]')\npyplot.legend(loc='best', ncol=1, fontsize=15)\npyplot.grid();\n\n\n\n\n\n\nLet's pick a value in the middle of the array and examine the difference between the real data and the prediction made by the linear regression. \n\n\nmid_idx1 = date_index(date, '1925-05')\nmid_idx2 = date_index(date, '1990-05')\nprint('The index for 1925-05 is: {} and for 1995-05: {}'.format(mid_idx1,\n                                                                mid_idx2))\n\n\n\n\nThe index for 1925-05 is: 544 and for 1995-05: 1324\n\n\n\nmid_year1 = dummy[mid_idx1]\nmid_year2 = dummy[mid_idx2]\n\nmid_reg1 = f1(mid_year1)\nmid_reg2 = f2(mid_year2)\n\nprint('Temp anomaly estimation for 1925-05: {}'.format(mid_reg1))\nprint('Temp anomaly estimation for 1990-05: {}'.format(mid_reg2))\n\n\n\n\nTemp anomaly estimation for 1925-05: -0.20146469691379698\nTemp anomaly estimation for 1990-05: 0.44674762414762137\n\n\n\nprint('Temp anomaly data for 1925-05: {}'.format(T_land[mid_idx1,1]))\nprint('Temp anomaly data for 1990-05: {}'.format(T_land[mid_idx2,1]))\n\n\n\n\nTemp anomaly data for 1925-05: -0.2339\nTemp anomaly data for 1990-05: 0.5212\n\n\n\nThat's certainly better than before. Is it good, though? That's hard to know. \nLater in the course, we'll take a more in-depth look at error analysis and try to answer that question in more detail.\n\n\nDig Deeper:\n\n\n\n\n\n\nYou can explore a different data set in the \nNOAA\n webpage and do a similar analysis. \n\n\n\n\n\n\nA different way of doing linear regressions is by using \nscipy.stats.linregress\n. Read the documentation and explore the differences with the \nnumpy.polyfit\n. You can also study  which one is faster by using the magic \n%timeit\n .\n\n\n\n\n\n\n#Ignore this cell, It simply loads a style for the notebook.\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    try:\n        styles = open(\"../styles/custom.css\", \"r\").read()\n        return HTML(styles)\n    except:\n        pass\ncss_styling()\n\n\n\n\n\n\n\n\n\n\n</p>\n<p>@font-face {\n    font-family: \"Computer Modern\";\n    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n}</p>\n<h1 id=\"notebook_panel-main-background\">notebook_panel { /<em> main background </em>/</h1>\n<pre><code>background: rgb(245,245,245);\n</code></pre>\n<p>}</p>\n<p>div.cell { /<em> set cell width </em>/\n    width: 750px;\n}</p>\n<p>div #notebook { /<em> centre the content </em>/\n    background: #fff; /<em> white background for content </em>/\n    width: 1000px;\n    margin: auto;\n    padding-left: 0em;\n}</p>\n<h1 id=\"notebook-li-more-space-between-bullet-points\">notebook li { /<em> More space between bullet points </em>/</h1>\n<p>margin-top:0.8em;\n}</p>\n<p>/<em> draw border around running cells </em>/\ndiv.cell.border-box-sizing.code_cell.running { \n    border: 1px solid #111;\n}</p>\n<p>/<em> Put a solid color box around each cell and its output, visually linking them</em>/\ndiv.cell.code_cell {\n    background-color: rgb(256,256,256); \n    border-radius: 0px; \n    padding: 0.5em;\n    margin-left:1em;\n    margin-top: 1em;\n}</p>\n<p>div.text_cell_render{\n    font-family: 'Alegreya Sans' sans-serif;\n    line-height: 140%;\n    font-size: 125%;\n    font-weight: 400;\n    width:600px;\n    margin-left:auto;\n    margin-right:auto;\n}</p>\n<p>/<em> Formatting for header cells </em>/\n.text_cell_render h1 {\n    font-family: 'Fenix', sans-serif;\n    font-style:regular;\n    font-weight: 200;  <br />\n    font-size: 40pt;\n    line-height: 100%;\n    color: #138d75;\n    margin-bottom: 0.5em;\n    margin-top: 0.5em;\n    display: block;\n} <br />\n.text_cell_render h2 {\n    font-family: 'Fenix', serif;\n    font-size: 20pt;\n    line-height: 100%;\n    margin-bottom: 0.1em;\n    color: #1f618d;\n    margin-top: 0.3em;\n    display: block;\n}   </p>\n<p>.text_cell_render h3 {\n    font-family: 'Fenix', serif;\n    margin-top:12px;\n    font-size: 16pt;\n    margin-bottom: 3px;\n    font-style: regular;\n}</p>\n<p>.text_cell_render h4 {    /<em>Use this for captions</em>/\n    font-family: 'Fenix', serif;\n    font-size: 12pt;\n    text-align: center;\n    margin-top: 0em;\n    margin-bottom: 2em;\n    font-style: regular;\n}</p>\n<p>.text_cell_render h5 {  /<em>Use this for small titles</em>/\n    font-family: 'Alegreya Sans', sans-serif;\n    font-weight: 300;\n    font-size: 16pt;\n    color: #CD2305;\n    font-style: italic;\n    margin-bottom: .5em;\n    margin-top: 0.5em;\n    display: block;\n}</p>\n<p>.text_cell_render h6 { /<em>use this for copyright note</em>/\n    font-family: 'Source Code Pro', sans-serif;\n    font-weight: 300;\n    font-size: 9pt;\n    line-height: 100%;\n    color: grey;\n    margin-bottom: 1px;\n    margin-top: 1px;\n}</p>\n<pre><code>.CodeMirror{\n        font-family: \"Source Code Pro\";\n        font-size: 90%;\n}\n</code></pre>\n<p>/<em>    .prompt{\n        display: None;\n    }</em>/</p>\n<pre><code>.warning{\n    color: rgb( 240, 20, 20 )\n    }\n</code></pre>\n<p>\n\n\n\n    MathJax.Hub.Config({\n                        TeX: {\n                           extensions: [\"AMSmath.js\"], \n                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n                           },\n                tex2jax: {\n                    inlineMath: [ ['<script type=\"math/tex\">','\n'], [\"\\(\",\"\\)\"] ],\n                    displayMath: [ ['\n','\n'], [\"\\[\",\"\\]\"] ]\n                },\n                displayAlign: 'center', // Change this to 'center' to center equations.\n                \"HTML-CSS\": {\n                    styles: {'.MathJax_Display': {\"margin\": 4}}\n                }\n        });",
            "title": "The Python Scientific Stack"
        },
        {
            "location": "/jupyter/1/#jupyter-notebooks",
            "text": "",
            "title": "Jupyter Notebooks"
        },
        {
            "location": "/jupyter/1/#intro-to-the-python-scientific-stack",
            "text": "Welcome to the Jupyter notebook, a place where you can combine markdown text, LaTeX equations, code and results in a single document.   In this notebook we will introduce useful python libraries that are important if you want to do scientific computing. We will learn:    How to import data, manipulate arrays and do operations with them using  Numpy .    How to do some stats and data fitting using  Numpy  and  Scipy .    How to do nice plots using  matplotlib .",
            "title": "Intro to the python scientific stack"
        },
        {
            "location": "/jupyter/1/#context-earth-temperature-over-time",
            "text": "Is global temperature rising? How much? This is a question of burning importance in today's world!  Data about global temperatures are available from several sources: NASA, the National Climatic Data Center (NCDC) and the University of East Anglia in the UK. Check out the  University Corporation for Atmospheric Research  (UCAR) for an in-depth discussion.  The  NASA Goddard Space Flight Center  is one of our sources of global climate data. They produced this video showing a color map of the changing global surface  temperature anomalies  from 1880 to 2011.  The term  global temperature anomaly  means the difference in temperature with respect to a reference value or a long-term average. It is a very useful way of looking at the problem and in many ways better than absolute temperature. For example, a winter month may be colder than average in Washington DC, and also in Miami, but the absolute temperatures will be different in both places.  from IPython.display import YouTubeVideo\nYouTubeVideo('lyb4gau3LyI')   How would we go about understanding the  trends  from the data on global temperature?  The first step in analyzing unknown data is to generate some simple plots using  matplotlib . We are going to look at the temperature-anomaly history, contained in a file, and make our first plot to explore this data.   We are going to smooth the data and then we'll fit a line to it to find a trend, plotting along the way to see how it all looks.  Let's get started!",
            "title": "Context \u2014 Earth temperature over time"
        },
        {
            "location": "/jupyter/1/#step-1-read-a-data-file",
            "text": "We took the data from the  NOAA  (National Oceanic and Atmospheric Administration) webpage. Feel free to play around with the webpage and analyze data on your own, but for now, let's make sure we're working with the same dataset.  First we will download the data from the web by using some Python magic and invoke the command  wget .  We will save the file in the folder  resources  under the name:  land_global_temperature_anomaly-1880-2015.csv  This file contains the year and month on the first column and 12 monthly averages of land temperature anomaly listed sequentially on the second column, from 1880 to 2015. We will read the file, then make an initial plot to see what it looks like.  In the NOAA web page you can select the data that you want to download as  a  csv  file and this will automatically open another tab with the data. We want to get that data so we will use  wget  but instead from a terminal we will invoke it from the notebook, yes! from the notebook.  To run shell commands in the notebook we just need to use an exclamation mark  !  before the command:  !wget -O land_global_temperature_anomaly-1880-2015.csv  https://www.ncdc.noaa.gov/cag/time-series/global/globe/land/all/1/1880-2015.csv   --2017-01-04 06:37:59--  https://www.ncdc.noaa.gov/cag/time-series/global/globe/land/all/1/1880-2015.csv\nResolving www.ncdc.noaa.gov (www.ncdc.noaa.gov)... 205.167.25.171, 205.167.25.172, 2610:20:8040:2::171, ...\nConnecting to www.ncdc.noaa.gov (www.ncdc.noaa.gov)|205.167.25.171|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/plain]\nSaving to: \u2018land_global_temperature_anomaly-1880-2015.csv\u2019\n\nland_global_tempera     [ <=>                ]  23.24K   132KB/s    in 0.2s\n\n2017-01-04 06:38:00 (132 KB/s) - \u2018land_global_temperature_anomaly-1880-2015.csv\u2019 saved [23796]  We got the data! Now we need to load it so we can work with it. The first thing to do is to load our favorite library: the  NumPy  library for array operations.  import numpy  Pretty easy, right? Now to load the file, we use a function from the NumPy library called  loadtxt() . To tell Python where to look for this function, we precede the function name with the library name, and use a dot between the two names. You can read the documentation in this  link .  loadtxt  expects every row in the same format, so we need to check that that's true of this data file. \nWe could open it in Python, but we've already learned a useful command for peeking into files, right? \nLet's use  head  to show the first ten lines of the  csv  file, again using a  !  to denote a shell command.  !head land_global_temperature_anomaly-1880-2015.csv  Global Land Temperature Anomalies\nUnits: Degrees Celsius\nBase Period: 1901-2000\nYear,Value\n188001,-0.1538\n188002,-0.5771\n188003,-0.7250\n188004,-0.2990\n188005,-0.2098\n188006,-0.3002  We see that the first 4 lines of the file are metadata but those will mess up  loadtxt . \nTo fix this, we will pass an argument to  loadtxt  to tell it to skip the first 4 rows.  numpy.loadtxt(fname='land_global_temperature_anomaly-1880-2015.csv',\n              delimiter=',', skiprows=4)  array([[  1.88001000e+05,  -1.53800000e-01],\n       [  1.88002000e+05,  -5.77100000e-01],\n       [  1.88003000e+05,  -7.25000000e-01],\n       ..., \n       [  2.01510000e+05,   1.32120000e+00],\n       [  2.01511000e+05,   1.28280000e+00],\n       [  2.01512000e+05,   1.87220000e+00]])  We called the function with three parameters: the file name and path, the delimiter that separates each value on a line (a comma in this case), and  skiprows=4  which tells Numpy to omit the first 4 rows. \nNote that the first two parameters are strings (made up of characters) and we put them in quotes.  As the output of the function, we get an array. Because it's rather big, Python shows only a few rows and columns of the array.   So far, so good. Now, what if we want to manipulate this data? Or plot it? We need to refer to it with a name. We've only just read the file, but we did not assign the array any name! Let's try again.  T_land = numpy.loadtxt(fname='land_global_temperature_anomaly-1880-2015.csv', \n                       delimiter=',', skiprows=4)  That's interesting. Now, we don't see any output from the function call. Why? It's simply that the output was stored into the variable  T_land , so to see it, we can do:  print(T_land)  [[  1.88001000e+05  -1.53800000e-01]\n [  1.88002000e+05  -5.77100000e-01]\n [  1.88003000e+05  -7.25000000e-01]\n ..., \n [  2.01510000e+05   1.32120000e+00]\n [  2.01511000e+05   1.28280000e+00]\n [  2.01512000e+05   1.87220000e+00]]  Ah, there it is! Let's find out how big the array is. For that, we use a cool NumPy function called  shape() :  numpy.shape(T_land)  (1632, 2)  Again, we've told Python where to find the function  shape()  by attaching it to the library name with a dot. However, NumPy arrays also happen to have a property shape that will return the same value, so we can get the same result another way:  T_land.shape  (1632, 2)  It's just shorter. The array  T_shape  holding our temperature-anomaly data has two columns and 1632 rows. Since we said we had monthly data, how many years is that?  1632/12  136.0  That's right: from 1880 through 2015.",
            "title": "Step 1: Read a data file."
        },
        {
            "location": "/jupyter/1/#step-2-plot-the-data",
            "text": "We will display the data in two ways: as a time series of the monthly temperature anomalies versus time, and as a histogram. To be fancy, we'll put both plots in one figure.  Let's first load our plotting library, called  matplotlib . To get the plots inside the notebook (rather than as popups), we use a special \"magic\" command,  %matplotlib inline :  from matplotlib import pyplot\n%matplotlib inline  What's this  from  business about?   matplotlib  is a pretty big (and awesome!) library.  All that we need is a subset of the library for creating 2D plots, so we ask for the  pyplot  module of the  matplotlib  library.    Plotting the time series of temperature is as easy as calling the function  plot()  from the module  pyplot .    But remember the shape of  T_land ? It has two columns and the temperature-anomaly values are in the second column. We extract the values of the second column by specifying 1 as the second index (the first column has index 0) and using the colon notation  :  to mean  all rows . Check it out:   pyplot.plot(T_land[:,1])  [<matplotlib.lines.Line2D at 0x7fcfd42274a8>]   You can add a semicolon at the end of the plotting command to avoid that stuff that appeared on top of the figure, that  Out[x]: [< ...>]  ugliness. Try it.  Do you see a trend in the data?  The plot above is certainly useful, but wouldn't it be nicer if we could look at the data relative to the year, instead of the location of the data in the array?  The plot function can take another input; let's get the year displayed as well.  pyplot.plot(T_land[:,0],T_land[:,1]);   What happened? It does not look like the previous plot. \nIt looks a little sparse. \nDo you know why? \nNotice that in the  csv  file the column for the years are integers, and even though we understand what they mean, is that what we want to plot? \nLet's take a closer look by just plotting the first two years.   pyplot.plot(T_land[0:25,0],T_land[0:25,1]);   Do you notice what is going on? We are not using the right scale to plot our dates. However,  Python  is really cool and we can solve this in a nice and neat way. We will use  numpy.arange()  and specify a data type for dates that you can explore in depth in this  link .   date = numpy.arange('1880', '2016', dtype=('datetime64[M]'))  We passed to  numpy.arange()  three arguments, the beginning and the end of the array, and   dtype=datetime64[M]  that indicates that we want to show every month in the range determined by the beginning and end of the array. Let's print some of the elements of the array  date :   print(date[0:15])  ['1880-01' '1880-02' '1880-03' '1880-04' '1880-05' '1880-06' '1880-07'\n '1880-08' '1880-09' '1880-10' '1880-11' '1880-12' '1881-01' '1881-02'\n '1881-03']  We create this array but we can not use it directly to plot, we need a dummy array the same length as our data array. Then we will use the  date  array to plot the ticks on the  x-axis .  len(T_land)  1632  The  dummy  array should contain 1632 equally space elements but it doesn't matter when it starts and when it finishes.\nWhy? Because we aren't going to use that for display!\nFor simplity we will start at 1 and finish at 1632. \nWe will use  numpy.linspace()  to create our array. The first argument indicates the start of the array, the second the end, and the third one indicates the number of elements we want our array to have.  dummy = numpy.linspace(1, 1632, 1632)  We will use this array and the  date  array to make the plot again but now in the  x-axis  we will plot the  date . Because plotting all the months will end up being non-readable, we will use  slices  to plot the  x-ticks  every ten years (120 months).      pyplot.xticks(dummy[::12*10], date[::12*10] , rotation=75)\npyplot.plot(dummy,T_land[:,1]);   Now we have a better plot but if I give you this plot without any information you would not be able to figure out what kind of data it is! We need labels on the axis, a title and why not a better color, font and size of the ticks.  Publication quality  plots should always be your standard for plotting. \nHow you present your data will allow others (and probably you in the future) to better understand your work.   Let's make the font of a specific size and type. \nWe don't want to write this out every time we create a plot. \nInstead, the next few lines of code will apply for all the plots we create from now on.  from matplotlib import rcParams\nrcParams['font.family'] = 'serif'\nrcParams['font.size'] = 16  We are going to plot the same plot as before but now we will add a few things to make it prettier and  publication quality .  pyplot.figure(figsize=(10,5))\npyplot.xticks(dummy[::12*10], date[::12*10] , rotation=75)\npyplot.plot(dummy,T_land[:,1], color='#2929a3', ls='-', lw=1)\npyplot.title('Land global temperature anomalies. \\n')\npyplot.xlabel('Year-Month')\npyplot.ylabel('Land temperature anomaly [\u00b0C]')\npyplot.grid();   Better ah? Feel free to play around with the parameters and see how it changes.   Do you see a trend in the data?  The temperature anomaly certainly seems to show an increasing trend. But we're not going to stop there, of course. It's not that easy to convince people that the planet is warming, as you know.  Plotting a histogram is as easy as calling the function  hist() . Why should it be any harder?  pyplot.figure(figsize=(10,5))\npyplot.hist(T_land[:,1])\npyplot.xlabel('Land temperature anomaly [\u00b0C]')\npyplot.ylabel('Frequency');   You can control several parameters of the  hist()  plot. Learn more by reading the manual page (yes, you have to read the manual sometimes!). The first option is the number of bins\u2014the default is 10\u2014but you can also change the appearance (color, transparency). Try some things out.  pyplot.figure(figsize=(10,5))\npyplot.hist(T_land[:,1], 25, normed=True, color='g', alpha=0.5)\npyplot.xlabel('Land temperature anomaly [\u00b0C]')\npyplot.ylabel('Normalized frequency');   What does this plot tell you about the data? It's more interesting than just an increasing trend, that's for sure. You might want to look at more statistics now: mean, median, standard deviation ... NumPy makes that easy for you:  mean_T = numpy.mean(T_land[:,1])\nmedian_T = numpy.median(T_land[:,1])\n\nprint('The mean value is {:.5} and the median {:.5}'.format(mean_T,\n                                                            median_T))  The mean value is 0.04031 and the median -0.0276  variance_T = numpy.var(T_land[:,1])\nsigma_T = numpy.sqrt(variance_T)\nprint('The variance is {:.5} and the standard deviation {:.5}'.format(variance_T,\n                                                                      sigma_T))  The variance is 0.28041 and the standard deviation 0.52954  With these data, we can plot the probability density function (pdf) that correspond to this data, compare with the histogram and see how far from a normal distribution we are.  In order to do that, we need to import the module  stats  from  SciPy , another great python library that contains tools for scientific computing.   from scipy import stats  bins = numpy.linspace(min(T_land[:,1]), max(T_land[:,1]), 40)\n\npyplot.figure(figsize=(10,5))\npyplot.hist(T_land[:,1], bins, normed=True, facecolor='g', alpha=0.5)\npyplot.plot(bins, stats.norm.pdf(bins, mean_T, sigma_T),\n            color='#ff5733', ls='-', lw=2.5)\npyplot.xlim(min(T_land[:,1]), max(T_land[:,1]))\npyplot.xlabel('Land temperature anomaly [\u00b0C]')\npyplot.ylabel('Normalized frequency')\npyplot.grid();   This is fun. Finally, we'll put both plots on the same figure using the  subplot()  function, which creates a grid of plots. The argument tells this function how many rows and columns of sub-plots we want, and where in the grid each plot will go.  pyplot.figure(figsize=(12,4))  \n\npyplot.subplot(121)   # creates a grid of 1 row, 2 columns and \n                      # selects the first plot\n\npyplot.xticks(dummy[::12*10], date[::12*10] , rotation=75)   \npyplot.plot(dummy,T_land[:,1], color='#2929a3', ls='-', lw=1) \npyplot.xlabel('Year-Month')\npyplot.ylabel('Land temperature anomaly [\u00b0C]')\npyplot.grid()\n\npyplot.subplot(122)            # prepares for the second plot\n\nbins = numpy.linspace(min(T_land[:,1]), max(T_land[:,1]), 40)\npyplot.hist(T_land[:,1], bins, normed=True, color='#2929a3',\n            alpha=0.4, histtype= 'stepfilled')\npyplot.plot(bins, stats.norm.pdf(bins, mean_T, sigma_T),\n            color='#ff5733', ls='-', lw=2.5)\npyplot.xlim(min(T_land[:,1]), max(T_land[:,1]))\npyplot.xlabel('Land temperature anomaly [\u00b0C]')\npyplot.ylabel('Normalized frequency')\npyplot.grid();   Did you notice that the histogram looks a little different? You can tweak the type of histogram you want to plot by passing the  histtype  argument to the  hist()  function. Curious about what other options do you have? Go back to the  documentation  and read about the other options that you have.",
            "title": "Step 2: Plot the data."
        },
        {
            "location": "/jupyter/1/#step-3-smooth-the-data-and-do-regression",
            "text": "You see a lot of fluctuations on the time series, so you might be asking yourself \"How can I smooth it out?\" No? Let's do it anyway.  One possible approach to smooth the data (there are others) is using a  moving average , also known as a sliding-window average. This is defined as:   \\hat{x}_{i,n} = \\frac{1}{n} \\sum_{j=1}^{n} x_{i-j}   The only parameter to the moving average is the value  n . As you can see, the moving average smooths the set of data points by creating a new data set consisting of local averages (of the  n  previous data points) at each point in the new set.  A moving average is technically a  convolution , and luckily NumPy has a built-in function for that,  convolve() .  We need to pick  N  and create the \"window\" we want to use to smooth the data. but we might want to change  N  and try different values, so what if we create a simple function so then you can call this function with different parameters and see how the smoothed data varies.   def smooth_data(N, data):\n    \"\"\"\n    Returns smoothed data using a sliding_moving avarage.\n\n    Arguments:\n    ----------\n    N (int)       : amount of data values we want to average.\n    data (array)  : array of data we want to smooth.\n\n\n    Returns:\n    --------\n    smooth (array): array with smoothed data.\n    \"\"\"\n\n    window = numpy.ones(N)/N\n    smooth = numpy.convolve(data, window, 'same')\n\n    return smooth  Did you notice the function  ones() ? It creates an array filled with ... you guessed it: ones!  Ok! We  define  our function and now we want to call the it with the parameters we want. But before calling the function you might be wondering what is the text that is between triple quotes?  That is a  docstring  and they explain what the function does. It's a good practice to always write the corresponding docstring, your future \"you\" and colleagues will thank you.   Now we can read the docstring and see what our function does.  But let's suppose you are working on a big project and you see a function and you want to know what a function does; you can use the question mark  ?  and you'll have this information. Let's try it:  ?smooth_data  Cool uh? Let's use this information and call our function with the land temperature data and  N=12 , we want each point of the smoothed data to be an average of 12 months.   We also want the return data to be saved in a variable that we can use to plot afterwards, then:  smooth = smooth_data(12, T_land[:,1])  pyplot.figure(figsize=(10, 5))\npyplot.xticks(dummy[::12*10], date[::12*10] , rotation=75)   \npyplot.plot(dummy, smooth, color='r', ls='-', lw=2)\n\npyplot.xlabel('Year-Month')\npyplot.ylabel('Land temperature anomaly [\u00b0C]')\npyplot.grid();    Looking at the plot, we can still see a trend, but the range of values is smaller. Let's plot the original time series together with the smoothed version:  pyplot.figure(figsize=(10, 5))\npyplot.xticks(dummy[::12*10], date[::12*10] , rotation=75)   \npyplot.plot(dummy, T_land[:,1], color='#2929a3', ls='-', lw=1, alpha=0.5) \npyplot.plot(dummy, smooth, color='r', ls='-', lw=2)\n\npyplot.xlabel('Year-Month')\npyplot.ylabel('Land temperature anomaly [\u00b0C]')\npyplot.grid();    That is interesting! The smoothed data follows the trend nicely but has much less noise. Well, that is what filtering data is all about.   Let's now fit a straight line through the temperature-anomaly data, to see the trends. We need to perform a least-squares linear regression to find the slope and intercept of a line    y = mx+b   that fits our data. Thankfully, Python and NumPy are here to help with the  polyfit()  function and the  poly1d()  class. The  polyfit()  function takes three arguments: the two array variables  x  and  y , and the order of the polynomial for the fit (in this case, 1 for linear regression). The  poly1d()  class takes an array with the polynomial's coefficient in decreasing power and it gives us back the polynomial itself ready to be used as a function.   To fit our data we will keep using our  dummy  variable in the abscissa axis. However, if we want to use our linear regression to have an estimate of the temperature anomaly value in a certain year, we need to know which index in the array is the one that correspond to the date we want to give as input. Once we know that index we can evaluate our  dummy  variable in that index and pass that value to the  fitting-function .   We call  polyfit()  with our data and we save the coefficients of our linear polynomial into the  tuple   (m, b) . Then, we feed  poly1d()  with  (m, b)  to get our linear regression as a function.     m, b = numpy.polyfit(dummy, T_land[:,1], 1)\n\nf_linear = numpy.poly1d((m, b))   print(f_linear)  0.0008322 x - 0.6392  pyplot.figure(figsize=(10, 5))\n\npyplot.xticks(dummy[::12*10], date[::12*10] , rotation=75)   \npyplot.plot(dummy, T_land[:,1], color='#2929a3', ls='-', lw=1, alpha=0.5) \npyplot.plot(dummy, f_linear(dummy), 'k--', linewidth=2,\n            label='linear regression')\npyplot.xlabel('Year-Month')\npyplot.ylabel('Land temperature anomaly [\u00b0C]')\npyplot.legend(loc='best', fontsize=15)\npyplot.grid();   If we want to know the value of the land temperature anomaly that our linear regression gives for a certain year we need to know the index in the array that corresponds to the date we want. \nLet's create a function that does that for us, with the help of  numpy.where()  this task should be easy.  Our  date  array has a specific data type, and we need to take this into account when we use the  numpy.where()  function. Why don't we take a look at the array to refresh our minds.  #Our date array\ndate  array(['1880-01', '1880-02', '1880-03', ..., '2015-10', '2015-11',\n       '2015-12'], dtype='datetime64[M]')  #A specific element of our array\ndate[23]  numpy.datetime64('1881-12')  With this information in mind we design our function  date_index()  that takes as inputs the array that contains the dates and the desired date, and it'll return the index on the date's array that correspond to the date we want.  What if we pass a date that is not in the array, or we pass it in the wrong format? \nTo prevent this we use  try and except  to help us handle errors in an easy way.   def date_index(dates_array, date_desired):\n    \"\"\"\n    Looks for the index of a specific date in the dates_array.\n\n    Arguments:\n    ----------\n    dates_array  : array,  contains the dates (dtype='datetime64[M]')\n    date_desired : str, in the format YYYY-MM we want to know their \n                        index.\n\n    Returns:\n    --------\n    date_index   : array, index in the dates_array that correspond to the\n                   desired_date    \n    \"\"\"\n    try:    \n        date_index = numpy.where(dates_array==numpy.datetime64(date_desired))\n    except ValueError:\n        raise ValueError('Invalid date entered (bad format)')\n\n\n    if len(date_index[0]) > 0:\n        return date_index[0][0]\n    else:\n        raise ValueError(\"Date not found in date range\")  If we did things right, then '1880-01' should correspond to index 0, and '2015-12' to 1631.    idx_first = date_index(date, '1880-01')\nidx_last  = date_index(date, '2015-12')\nprint('The index for 1880-01 is: {} and for 2015-12: {}'.format(idx_first,\n                                                                idx_last))  The index for 1880-01 is: 0 and for 2015-12: 1631  If we force our function to fail, it should  raise  the errors we told it to raise. Let's try it!  # Bad format date\ndate_index(date, '1890-13')  ---------------------------------------------------------------------------\n\nValueError                                Traceback (most recent call last)\n\n<ipython-input-39-74303b0d9f6e> in date_index(dates_array, date_desired)\n     16     try:\n---> 17         date_index = numpy.where(dates_array==numpy.datetime64(date_desired))\n     18     except ValueError:\n\n\nValueError: Month out of range in datetime string \"1890-13\"\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nValueError                                Traceback (most recent call last)\n\n<ipython-input-41-713570594e34> in <module>()\n      1 # Bad format date\n----> 2 date_index(date, '1890-13')\n\n\n<ipython-input-39-74303b0d9f6e> in date_index(dates_array, date_desired)\n     17         date_index = numpy.where(dates_array==numpy.datetime64(date_desired))\n     18     except ValueError:\n---> 19         raise ValueError('Invalid date entered (bad format)')\n     20 \n     21\n\n\nValueError: Invalid date entered (bad format)  # Date out of range\ndate_index(date, '1879-01')  ---------------------------------------------------------------------------\n\nValueError                                Traceback (most recent call last)\n\n<ipython-input-42-d4cc294cbd7c> in <module>()\n      1 # Date out of range\n----> 2 date_index(date, '1879-01')\n\n\n<ipython-input-39-74303b0d9f6e> in date_index(dates_array, date_desired)\n     23         return date_index[0][0]\n     24     else:\n---> 25         raise ValueError(\"Date not found in date range\")\n     26\n\n\nValueError: Date not found in date range  Now we have the beginning and end indices, let's use them to see what our linear regression gives us when we evaluate  specific dates. For example:  year_first = dummy[idx_first]\nyear_last = dummy[idx_last]\n\nreg_first = f_linear(year_first)\nreg_last = f_linear(year_last)\n\nprint('Temp anomaly estimation for 1880-01: {}'.format(reg_first))\nprint('Temp anomaly estimation for 2015-12: {}'.format(reg_last))  Temp anomaly estimation for 1880-01: -0.6383595707557367\nTemp anomaly estimation for 2015-12: 0.7189794236969136  Let's compare that to the actual dataset:  print('Temp anomaly data for 1880-01: {}'.format(T_land[idx_first,1]))\nprint('Temp anomaly data for 2015-12: {}'.format(T_land[idx_last,1]))  Temp anomaly data for 1880-01: -0.1538\nTemp anomaly data for 2015-12: 1.8722  It seems like a linear regression might not be the best way of modeling this data, but what if we try doing the regression in \"chunks\"? \nIf we take a look at the plot, we see that around the 1970's the slope starts to be more pronounced. \nMaybe doing a linear regression in two different ranges is a good idea. \nWriting a function to do this is a smart decision so we can reuse it in the future if we want to do more than two regressions.  def regression_range(start_date, end_date, date_array, x, T):\n    \"\"\"\n    Perform a linear regression in the range of dates provided.\n\n    Arguments:\n    ----------\n    start_date: str 'YYYY-MM', starting date. \n    end_date: str 'YYYY-MM', ending date.\n    date_array: array, it contains the dates in the format 'YYYY-MM'.\n    x: dummy array we need in order to plot to plot\n    T: temperature anomalies data array.\n\n    Returns:\n    --------\n    x_range: array, range of the x array used to perform the regression.\n    T_range: array, range of the T array used to perform the regression.\n    f_range: function, linear regression performed on the range provided.\n    \"\"\"\n\n    idx_start = date_index(date_array, start_date)\n    idx_end = date_index(date_array, end_date)\n\n    x_range = x[idx_start: idx_end+1]\n    T_range = T[idx_start: idx_end+1]\n\n    m, b = numpy.polyfit(x_range, T_range, 1)\n\n    f_range = numpy.poly1d((m,b)) \n\n    return x_range, T_range, f_range  Now we call the function in two different ranges, one from  1880-01  to  1969-12  and the other from  1970-01  to  2015-12 :  # First range \nx1, T1, f1 = regression_range('1880-01', '1969-12', date,\n                              dummy, T_land[:,1])  # Second range\nx2, T2, f2 = regression_range('1970-01', '2015-12', date,\n                              dummy, T_land[:,1])  pyplot.figure(figsize=(10, 5))\n\npyplot.xticks(dummy[::12*10], date[::12*10] , rotation=75)   \npyplot.plot(dummy, T_land[:,1], color='#2929a3', ls='-', lw=1,\n            alpha=0.4, label = 'raw data')\n\npyplot.plot(x1, f1(x1), 'g--', linewidth=2, label = 'reg 1880-1969')\npyplot.plot(x2, f1(x2), 'r--', linewidth=2)\npyplot.plot(x2, f2(x2), 'b--', linewidth=2, label = 'reg 1970-2015')\npyplot.xlabel('Year-Month')\npyplot.ylabel('Land temperature anomaly [\u00b0C]')\npyplot.legend(loc='best', ncol=1, fontsize=15)\npyplot.grid();   Let's pick a value in the middle of the array and examine the difference between the real data and the prediction made by the linear regression.   mid_idx1 = date_index(date, '1925-05')\nmid_idx2 = date_index(date, '1990-05')\nprint('The index for 1925-05 is: {} and for 1995-05: {}'.format(mid_idx1,\n                                                                mid_idx2))  The index for 1925-05 is: 544 and for 1995-05: 1324  mid_year1 = dummy[mid_idx1]\nmid_year2 = dummy[mid_idx2]\n\nmid_reg1 = f1(mid_year1)\nmid_reg2 = f2(mid_year2)\n\nprint('Temp anomaly estimation for 1925-05: {}'.format(mid_reg1))\nprint('Temp anomaly estimation for 1990-05: {}'.format(mid_reg2))  Temp anomaly estimation for 1925-05: -0.20146469691379698\nTemp anomaly estimation for 1990-05: 0.44674762414762137  print('Temp anomaly data for 1925-05: {}'.format(T_land[mid_idx1,1]))\nprint('Temp anomaly data for 1990-05: {}'.format(T_land[mid_idx2,1]))  Temp anomaly data for 1925-05: -0.2339\nTemp anomaly data for 1990-05: 0.5212  That's certainly better than before. Is it good, though? That's hard to know. \nLater in the course, we'll take a more in-depth look at error analysis and try to answer that question in more detail.",
            "title": "Step 3: Smooth the data and do regression"
        },
        {
            "location": "/jupyter/1/#dig-deeper",
            "text": "You can explore a different data set in the  NOAA  webpage and do a similar analysis.     A different way of doing linear regressions is by using  scipy.stats.linregress . Read the documentation and explore the differences with the  numpy.polyfit . You can also study  which one is faster by using the magic  %timeit  .    #Ignore this cell, It simply loads a style for the notebook.\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    try:\n        styles = open(\"../styles/custom.css\", \"r\").read()\n        return HTML(styles)\n    except:\n        pass\ncss_styling()     </p>\n<p>@font-face {\n    font-family: \"Computer Modern\";\n    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n}</p>\n<h1 id=\"notebook_panel-main-background\">notebook_panel { /<em> main background </em>/</h1>\n<pre><code>background: rgb(245,245,245);\n</code></pre>\n<p>}</p>\n<p>div.cell { /<em> set cell width </em>/\n    width: 750px;\n}</p>\n<p>div #notebook { /<em> centre the content </em>/\n    background: #fff; /<em> white background for content </em>/\n    width: 1000px;\n    margin: auto;\n    padding-left: 0em;\n}</p>\n<h1 id=\"notebook-li-more-space-between-bullet-points\">notebook li { /<em> More space between bullet points </em>/</h1>\n<p>margin-top:0.8em;\n}</p>\n<p>/<em> draw border around running cells </em>/\ndiv.cell.border-box-sizing.code_cell.running { \n    border: 1px solid #111;\n}</p>\n<p>/<em> Put a solid color box around each cell and its output, visually linking them</em>/\ndiv.cell.code_cell {\n    background-color: rgb(256,256,256); \n    border-radius: 0px; \n    padding: 0.5em;\n    margin-left:1em;\n    margin-top: 1em;\n}</p>\n<p>div.text_cell_render{\n    font-family: 'Alegreya Sans' sans-serif;\n    line-height: 140%;\n    font-size: 125%;\n    font-weight: 400;\n    width:600px;\n    margin-left:auto;\n    margin-right:auto;\n}</p>\n<p>/<em> Formatting for header cells </em>/\n.text_cell_render h1 {\n    font-family: 'Fenix', sans-serif;\n    font-style:regular;\n    font-weight: 200;  <br />\n    font-size: 40pt;\n    line-height: 100%;\n    color: #138d75;\n    margin-bottom: 0.5em;\n    margin-top: 0.5em;\n    display: block;\n} <br />\n.text_cell_render h2 {\n    font-family: 'Fenix', serif;\n    font-size: 20pt;\n    line-height: 100%;\n    margin-bottom: 0.1em;\n    color: #1f618d;\n    margin-top: 0.3em;\n    display: block;\n}   </p>\n<p>.text_cell_render h3 {\n    font-family: 'Fenix', serif;\n    margin-top:12px;\n    font-size: 16pt;\n    margin-bottom: 3px;\n    font-style: regular;\n}</p>\n<p>.text_cell_render h4 {    /<em>Use this for captions</em>/\n    font-family: 'Fenix', serif;\n    font-size: 12pt;\n    text-align: center;\n    margin-top: 0em;\n    margin-bottom: 2em;\n    font-style: regular;\n}</p>\n<p>.text_cell_render h5 {  /<em>Use this for small titles</em>/\n    font-family: 'Alegreya Sans', sans-serif;\n    font-weight: 300;\n    font-size: 16pt;\n    color: #CD2305;\n    font-style: italic;\n    margin-bottom: .5em;\n    margin-top: 0.5em;\n    display: block;\n}</p>\n<p>.text_cell_render h6 { /<em>use this for copyright note</em>/\n    font-family: 'Source Code Pro', sans-serif;\n    font-weight: 300;\n    font-size: 9pt;\n    line-height: 100%;\n    color: grey;\n    margin-bottom: 1px;\n    margin-top: 1px;\n}</p>\n<pre><code>.CodeMirror{\n        font-family: \"Source Code Pro\";\n        font-size: 90%;\n}\n</code></pre>\n<p>/<em>    .prompt{\n        display: None;\n    }</em>/</p>\n<pre><code>.warning{\n    color: rgb( 240, 20, 20 )\n    }\n</code></pre>\n<p>  \n    MathJax.Hub.Config({\n                        TeX: {\n                           extensions: [\"AMSmath.js\"], \n                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n                           },\n                tex2jax: {\n                    inlineMath: [ ['<script type=\"math/tex\">',' '], [\"\\(\",\"\\)\"] ],\n                    displayMath: [ [' ',' '], [\"\\[\",\"\\]\"] ]\n                },\n                displayAlign: 'center', // Change this to 'center' to center equations.\n                \"HTML-CSS\": {\n                    styles: {'.MathJax_Display': {\"margin\": 4}}\n                }\n        });",
            "title": "Dig Deeper:"
        },
        {
            "location": "/laplace/1/",
            "text": "Content under Creative Commons Attribution license CC-BY 4.0, code under BSD 3-Clause License \u00a9 2016 L.A. Barba, N.C. Clementi, G.F. Forsyth.  Based on https://github.com/numerical-mooc/numerical-mooc/blob/master/lessons/05_relax/05_01_2D.Laplace.Equation.ipynb also under CC-BY and MIT licenses, 2015.\n\n\nRelax and hold steady\n\n\nMany problems in physics have no time dependence, yet are rich with physical meaning: the gravitational field produced by a massive object, the electrostatic potential of a charge distribution, the displacement of a stretched membrane and the steady flow of fluid through a porous medium ... all these can be modeled by \nPoisson's equation\n:\n\n\n\n\n\\begin{equation}\n\\nabla^2 u = f\n\\end{equation}\n\n\n\n\nwhere the unknown \nu\n and the known \nf\n are functions of space, in a domain \n\\Omega\n. To find the solution, we require boundary conditions. These could be  Dirichlet boundary conditions, specifying the value of the solution on the boundary,\n\n\n\n\n\\begin{equation}\nu = b_1 \\text{ on } \\partial\\Omega,\n\\end{equation}\n\n\n\n\nor Neumann boundary conditions, specifying the normal derivative of the solution on the boundary,\n\n\n\n\n\\begin{equation}\n\\frac{\\partial u}{\\partial n} = b_2 \\text{ on } \\partial\\Omega.\n\\end{equation}\n\n\n\n\nA boundary-value problem consists of finding \nu\n, given the above information. Numerically, we can do this using \nrelaxation methods\n, which start with an initial guess for \nu\n and then iterate towards the solution. Let's find out how!\n\n\nLaplace's equation\n\n\nThe particular case of \nf=0\n (homogeneous case) results in Laplace's equation:\n\n\n\n\n\\begin{equation}\n\\nabla^2 u = 0\n\\end{equation}\n\n\n\n\nFor example, the equation for steady, two-dimensional heat conduction is:\n\n\n\n\n\\begin{equation}\n\\frac{\\partial ^2 T}{\\partial x^2} + \\frac{\\partial ^2 T}{\\partial y^2} = 0\n\\end{equation}\n\n\n\n\nwhere \nT\n is a temperature that has reached steady state. The Laplace equation models the equilibrium state of a system under the supplied boundary conditions.\n\n\nThe study of solutions to Laplace's equation is called \npotential theory\n, and the solutions themselves are often potential fields. Let's use \np\n from now on to represent our generic dependent variable, and write Laplace's equation again (in two dimensions):\n\n\n\n\n\\begin{equation}\n\\frac{\\partial ^2 p}{\\partial x^2} + \\frac{\\partial ^2 p}{\\partial y^2} = 0\n\\end{equation}\n\n\n\n\nLike in the diffusion equation, we discretize the second-order derivatives with \ncentral differences\n. If you need to refresh your mind, check out this \nlesson\n and try to discretize the equation by yourself. On a two-dimensional Cartesian grid, it gives:\n\n\n\n\n\\begin{equation}\n\\frac{p_{i+1, j}  - 2p_{i,j}  + p_{i-1,j} }{\\Delta x^2} + \\frac{p_{i,j+1}  - 2p_{i,j}  + p_{i, j-1} }{\\Delta y^2} = 0\n\\end{equation}\n\n\n\n\nWhen \n\\Delta x = \\Delta y\n, we end up with the following equation:\n\n\n\n\n\\begin{equation}\np_{i+1, j}   + p_{i-1,j} + p_{i,j+1}  + p_{i, j-1}- 4 p_{i,j} = 0\n\\end{equation}\n\n\n\n\nThis tells us that the Laplacian differential operator at grid point \n(i,j)\n can be evaluated discretely using the value of \np\n at that point (with a factor \n-4\n) and the four neighboring points to the left and right, above and below grid point \n(i,j)\n.\n\n\nThe stencil of the discrete Laplacian operator is shown in Figure 1. It is typically called the \nfive-point stencil\n, for obvious reasons.\n\n\n\n\nFigure 1: Laplace five-point stencil.\n\n\nThe discrete equation above is valid for every interior point in the domain. If we write the equations for \nall\n interior points, we have a linear system of algebraic equations. We \ncould\n solve the linear system directly (e.g., with Gaussian elimination), but we can be more clever than that!\n\n\nNotice that the coefficient matrix of such a linear system has mostly zeroes. For a uniform spatial grid, the matrix is \nblock diagonal\n: it has diagonal blocks that are tridiagonal with \n-4\n on the main diagonal and \n1\n on two off-center diagonals, and two more diagonals with \n1\n. All of the other elements are zero. Iterative methods are particularly suited for a system with this structure, and save us from storing all those zeroes.\n\n\nWe will start with an initial guess for the solution, \np_{i,j}^{0}\n, and use the discrete Laplacian to get an update, \np_{i,j}^{1}\n, then continue on computing \np_{i,j}^{k}\n until we're happy.  Note that \nk\n is \nnot\n a time index here, but an index corresponding to the number of iterations we perform in the \nrelaxation scheme\n.  \n\n\nAt each iteration, we compute updated values \np_{i,j}^{k+1}\n in a (hopefully) clever way so that they converge to a set of values satisfying Laplace's equation. The system will reach equilibrium only as the number of iterations tends to \n\\infty\n, but we can approximate the equilibrium state by iterating until the change between one iteration and the next is \nvery\n small.  \n\n\nThe most intuitive method of iterative solution is known as the \nJacobi method\n, in which the values at the grid points are replaced by the corresponding weighted averages:\n\n\n\n\n\\begin{equation}\np^{k+1}_{i,j} = \\frac{1}{4} \\left(p^{k}_{i,j-1} + p^k_{i,j+1} + p^{k}_{i-1,j} + p^k_{i+1,j} \\right)\n\\end{equation}\n\n\n\n\nThis method does indeed converge to the solution of Laplace's equation. Thank you Professor Jacobi!\n\n\nChallenge task\n\n\nGrab a piece of paper and write out the coefficient matrix for a discretization with 7 grid points in the \nx\n direction (5 interior points) and 5 points in the \ny\n direction (3 interior). The system should have 15 unknowns, and the coefficient matrix three diagonal blocks. Assume prescribed Dirichlet boundary conditions on all sides (not necessarily zero).\n\n\nBoundary conditions and relaxation\n\n\nSuppose we want to model steady-state heat transfer on (say) a computer chip with one side insulated (zero Neumann BC), two sides held at a fixed temperature (Dirichlet condition) and one side touching a component that has a sinusoidal distribution of temperature.\nWe would need to solve Laplace's equation with boundary conditions like\n\n\n\n\n\\begin{equation}\n  \\begin{gathered}\np=0 \\text{ at } x=0\\\\\n\\frac{\\partial p}{\\partial x} = 0 \\text{ at } x = L\\\\\np = 0 \\text{ at }y = 0 \\\\\np = \\sin \\left(  \\frac{\\frac{3}{2}\\pi x}{L} \\right) \\text{ at } y = H.\n  \\end{gathered}\n\\end{equation}\n\n\n\n\nWe'll take \nL=1\n and \nH=1\n for the sizes of the domain in the \nx\n and \ny\n directions.\n\n\nOne of the defining features of elliptic PDEs is that they are \"driven\" by the boundary conditions.  In the iterative solution of Laplace's equation, boundary conditions are set and \nthe solution relaxes\n from an initial guess to join the boundaries together smoothly, given those conditions.  Our initial guess will be \np=0\n everywhere. Now, let's relax!\n\n\nFirst, we import our usual smattering of libraries (plus a few new ones!)\n\n\nfrom matplotlib import pyplot\nimport numpy\n%matplotlib inline\nfrom matplotlib import rcParams\nrcParams['font.family'] = 'serif'\nrcParams['font.size'] = 16\n\n\n\n\nTo visualize 2D data, we can use \npyplot.imshow()\n, but a 3D plot can sometimes show a more intuitive view the solution. Or it's just prettier!\n\n\nBe sure to enjoy the many examples of 3D plots in the \nmplot3d\n section of the \nMatplotlib Gallery\n.  \n\n\nWe'll import the \nAxes3D\n library from Matplotlib and also grab the \ncm\n package, which provides different colormaps for visualizing plots.  \n\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\n\n\n\n\nLet's define a function for setting up our plotting environment, to avoid repeating this set-up over and over again. It will save us some typing.\n\n\ndef plot_3D(x, y, p):\n    '''Creates 3D plot with appropriate limits and viewing angle\n\n    Parameters:\n    ----------\n    x: array of float\n        nodal coordinates in x\n    y: array of float\n        nodal coordinates in y\n    p: 2D array of float\n        calculated potential field\n\n    '''\n    fig = pyplot.figure(figsize=(11,7), dpi=100)\n    ax = fig.gca(projection='3d')\n    X,Y = numpy.meshgrid(x,y)\n    surf = ax.plot_surface(X,Y,p[:], rstride=1, cstride=1, cmap=cm.viridis,\n            linewidth=0, antialiased=False)\n\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,1)\n    ax.set_xlabel('$x$')\n    ax.set_ylabel('$y$')\n    ax.set_zlabel('$z$')\n    ax.view_init(30,45)\n\n\n\n\n\nNote\n\n\nThis plotting function uses \nViridis\n, a new (and \nawesome\n) colormap available in Matplotlib versions 1.5 and greater.  If you see an error when you try to plot using \ncm.viridis\n, just update Matplotlib using \nconda\n or \npip\n.\n\n\nAnalytical solution\n\n\nThe Laplace equation with the boundary conditions listed above has an analytical solution, given by\n\n\n\n\n\\begin{equation}\np(x,y) = \\frac{\\sinh \\left( \\frac{\\frac{3}{2} \\pi y}{L}\\right)}{\\sinh \\left(  \\frac{\\frac{3}{2} \\pi H}{L}\\right)} \\sin \\left( \\frac{\\frac{3}{2} \\pi x}{L} \\right)\n\\end{equation}\n\n\n\n\nwhere \nL\n and \nH\n are the length of the domain in the \nx\n and \ny\n directions, respectively.\n\n\nWe will use \nnumpy.meshgrid\n to plot our 2D solutions. This is a function that takes two vectors (\nx\n and \ny\n, say) and returns two 2D arrays of \nx\n and \ny\n coordinates that we then use to create the contour plot. Always useful, \nlinspace\n creates 1-row arrays of equally spaced numbers: it helps for defining \nx\n and \ny\n axes in line plots, but now we want the analytical solution plotted for every point in our domain.  To do this, we'll use in the analytical solution the 2D arrays generated by \nnumpy.meshgrid\n.\n\n\ndef p_analytical(x, y):\n    X, Y = numpy.meshgrid(x,y)\n\n    p_an = numpy.sinh(1.5*numpy.pi*Y / x[-1]) /\\\n    (numpy.sinh(1.5*numpy.pi*y[-1]/x[-1]))*numpy.sin(1.5*numpy.pi*X/x[-1])\n\n    return p_an\n\n\n\n\nOk, let's try out the analytical solution and use it to test the \nplot_3D\n function we wrote above.  \n\n\nnx = 41\nny = 41\n\nx = numpy.linspace(0,1,nx)\ny = numpy.linspace(0,1,ny)\n\np_an = p_analytical(x,y)\n\n\n\n\nplot_3D(x,y,p_an)\n\n\n\n\n\n\nIt worked!  This is what the solution \nshould\n look like when we're 'done' relaxing. (And isn't viridis a cool colormap?) \n\n\nHow long do we iterate?\n\n\nWe noted above that there is no time dependence in the Laplace equation.  So it doesn't make a lot of sense to use a \nfor\n loop with \nnt\n iterations.\n\n\nInstead, we can use a \nwhile\n loop that continues to iteratively apply the relaxation scheme until the difference between two successive iterations is small enough.  \n\n\nBut how small is small enough?  That's a good question.  We'll try to work that out as we go along.  \n\n\nTo compare two successive potential fields, a good option is to use the \nL2 norm\n of the difference.  It's defined as\n\n\n\n\n\\begin{equation}\n|\\textbf{x}| = \\sqrt{\\sum_{i=0, j=0}^k \\left|p^{k+1}_{i,j} - p^k_{i,j}\\right|^2}\n\\end{equation}\n\n\n\n\nBut there's one flaw with this formula.  We are summing the difference between successive iterations at each point on the grid. So what happens when the grid grows? (For example, if we're refining the grid, for whatever reason.) There will be more grid points to compare and so more contributions to the sum. The norm will be a larger number just because of the grid size!\n\n\nThat doesn't seem right.  We'll fix it by normalizing the norm, dividing the above formula by the norm of the potential field at iteration \nk\n. \n\n\nFor two successive iterations, the relative L2 norm is then calculated as\n\n\n\n\n\\begin{equation}\n|\\textbf{x}| = \\frac{\\sqrt{\\sum_{i=0, j=0}^k \\left|p^{k+1}_{i,j} - p^k_{i,j}\\right|^2}}{\\sqrt{\\sum_{i=0, j=0}^k \\left|p^k_{i,j}\\right|^2}}\n\\end{equation}\n\n\n\n\nOur Python code for this calculation is a one-line function:\n\n\ndef L2_error(p, pn):\n    return numpy.sqrt(numpy.sum((p - pn)**2)/numpy.sum(pn**2))\n\n\n\n\nNow, let's define a function that will apply Jacobi's method for Laplace's equation.  Three of the boundaries are Dirichlet boundaries and so we can simply leave them alone.  Only the Neumann boundary needs to be explicitly calculated at each iteration, and we'll do it by discretizing the derivative in its first order approximation:  \n\n\ndef laplace2d(p, l2_target):\n    '''Iteratively solves the Laplace equation using the Jacobi method\n\n    Parameters:\n    ----------\n    p: 2D array of float\n        Initial potential distribution\n    l2_target: float\n        target for the difference between consecutive solutions\n\n    Returns:\n    -------\n    p: 2D array of float\n        Potential distribution after relaxation\n    '''\n\n    l2norm = 1\n    pn = numpy.empty_like(p)\n    while l2norm > l2_target:\n        pn = p.copy()\n        p[1:-1,1:-1] = .25 * (pn[1:-1,2:] + pn[1:-1, :-2] \\\n                              + pn[2:, 1:-1] + pn[:-2, 1:-1])\n\n        ##Neumann B.C. along x = L\n        p[1:-1, -1] = p[1:-1, -2]     # 1st order approx of a derivative \n        l2norm = L2_error(p, pn)\n\n    return p\n\n\n\n\nRows and columns, and index order\n\n\nThe physical problem has two dimensions, so we also store the temperatures in two dimensions: in a 2D array. \n\n\nWe chose to store it with the \ny\n coordinates corresponding to the rows of the array and \nx\n coordinates varying with the columns (this is just a code design decision!). If we are consistent with the stencil formula (with \nx\n corresponding to index \ni\n and \ny\n to index \nj\n), then \np_{i,j}\n will be stored in array format as \np[j,i]\n.\n\n\nThis might be a little confusing as most of us are used to writing coordinates in the format \n(x,y)\n, but our preference is to have the data stored so that it matches the physical orientation of the problem. Then, when we make a plot of the solution, the visualization will make sense to us, with respect to the geometry of our set-up. That's just nicer than to have the plot rotated!\n\n\n\n\nFigure 2: Row-column data storage\n\n\nAs you can see on Figure 2 above, if we want to access the value \n18\n we would write those coordinates as \n(x_2, y_3)\n.  You can also see that its location is the 3rd row, 2nd column, so its array address would be \np[3,2]\n.\n\n\nAgain, this is a design decision.  However you can choose to manipulate and store your data however you like; just remember to be consistent!\n\n\nLet's relax!\n\n\nThe initial values of the potential field are zero everywhere (initial guess), except at the boundary: \n\n\n\n\np = \\sin \\left(  \\frac{\\frac{3}{2}\\pi x}{L} \\right) \\text{ at } y=H\n\n\n\n\nTo initialize the domain, \nnumpy.zeros\n will handle everything except that one Dirichlet condition. Let's do it!\n\n\n##variable declarations\nnx = 41\nny = 41\n\n\n##initial conditions\np = numpy.zeros((ny,nx)) ##create a XxY vector of 0's\n\n\n##plotting aids\nx = numpy.linspace(0,1,nx)\ny = numpy.linspace(0,1,ny)\n\n##Dirichlet boundary conditions\np[-1,:] = numpy.sin(1.5*numpy.pi*x/x[-1])\n\n\n\n\n\nNow let's visualize the initial conditions using the \nplot_3D\n function, just to check we've got it right.\n\n\nplot_3D(x, y, p)\n\n\n\n\n\n\nThe \np\n array is equal to zero everywhere, except along the boundary \ny = 1\n.  Hopefully you can see how the relaxed solution and this initial condition are related.  \n\n\nNow, run the iterative solver with a target L2-norm difference between successive iterations of \n10^{-8}\n.\n\n\np = laplace2d(p.copy(), 1e-8)\n\n\n\n\nLet's make a gorgeous plot of the final field using the newly minted \nplot_3D\n function.\n\n\nplot_3D(x,y,p)\n\n\n\n\n\n\nAwesome!  That looks pretty good.  But we'll need more than a simple visual check, though. The \"eyeball metric\" is very forgiving!\n\n\nConvergence analysis\n\n\nConvergence, Take 1\n\n\nWe want to make sure that our Jacobi function is working properly.  Since we have an analytical solution, what better way than to do a grid-convergence analysis?  We will run our solver for several grid sizes and look at how fast the L2 norm of the difference between consecutive iterations decreases.\n\n\nLet's make our lives easier by writing a function to \"reset\" the initial guess for each grid so we don't have to keep copying and pasting them.\n\n\ndef laplace_IG(nx):\n    '''Generates initial guess for Laplace 2D problem for a \n    given number of grid points (nx) within the domain [0,1]x[0,1]\n\n    Parameters:\n    ----------\n    nx: int\n        number of grid points in x (and implicitly y) direction\n\n    Returns:\n    -------\n    p: 2D array of float\n        Pressure distribution after relaxation\n    x: array of float\n        linspace coordinates in x\n    y: array of float\n        linspace coordinates in y\n    '''\n\n    ##initial conditions\n    p = numpy.zeros((nx,nx)) ##create a XxY vector of 0's\n\n    ##plotting aids\n    x = numpy.linspace(0,1,nx)\n    y = x\n\n    ##Dirichlet boundary conditions\n    p[:,0] = 0\n    p[0,:] = 0\n    p[-1,:] = numpy.sin(1.5*numpy.pi*x/x[-1])\n\n    return p, x, y\n\n\n\n\nNow run Jacobi's method on the Laplace equation using four different grids, with the same exit criterion of \n10^{-8}\n each time. Then, we look at the error versus the grid size in a log-log plot. What do we get?\n\n\nnx_values = [11, 21, 41, 81]\nl2_target = 1e-8\n\nerror = numpy.empty_like(nx_values, dtype=numpy.float)\n\nfor i, nx in enumerate(nx_values):\n    p, x, y = laplace_IG(nx)\n\n    p = laplace2d(p.copy(), l2_target)\n\n    p_an = p_analytical(x, y)\n\n    error[i] = L2_error(p, p_an)\n\n\n\n\n\npyplot.figure(figsize=(6,6))\npyplot.grid(True)\npyplot.xlabel(r'$n_x$', fontsize=18)\npyplot.ylabel(r'$L_2$-norm of the error', fontsize=18)\n\npyplot.loglog(nx_values, error, color='k', ls='--', lw=2, marker='o')\npyplot.axis('equal');\n\n\n\n\n\n\nHmm. That doesn't look like 2nd-order convergence, but we're using second-order finite differences.  \nWhat's going on?\n  The culprit is the boundary conditions. Dirichlet conditions are order-agnostic (a  set value is a set value), but the scheme we used for the Neumann boundary condition is 1st-order.  \n\n\nRemember when we said that the boundaries drive the problem?  One boundary that's 1st-order completely tanked our spatial convergence.  Let's fix it!\n\n\n2nd-order Neumann BCs\n\n\nUp to this point, we have used the first-order approximation of a derivative to satisfy Neumann B.C.'s. For a boundary located at \nx=0\n this reads,\n\n\n\n\n\\begin{equation}\n\\frac{p^{k+1}_{1,j} - p^{k+1}_{0,j}}{\\Delta x} = 0\n\\end{equation}\n\n\n\n\nwhich, solving for \np^{k+1}_{0,j}\n gives us\n\n\n\n\n\\begin{equation}\np^{k+1}_{0,j} = p^{k+1}_{1,j}\n\\end{equation}\n\n\n\n\nUsing that Neumann condition will limit us to 1st-order convergence.  Instead, we can start with a 2nd-order approximation (the central-difference approximation):\n\n\n\n\n\\begin{equation}\n\\frac{p^{k+1}_{1,j} - p^{k+1}_{-1,j}}{2 \\Delta x} = 0\n\\end{equation}\n\n\n\n\nThat seems problematic, since there is no grid point \np^{k}_{-1,j}\n.  But no matter \u2026 let's carry on. According to the 2nd-order approximation,\n\n\n\n\n\\begin{equation}\np^{k+1}_{-1,j} = p^{k+1}_{1,j}\n\\end{equation}\n\n\n\n\nRecall the finite-difference Jacobi equation with \ni=0\n:\n\n\n\n\n\\begin{equation}\np^{k+1}_{0,j} = \\frac{1}{4} \\left(p^{k}_{0,j-1} + p^k_{0,j+1} + p^{k}_{-1,j} + p^k_{1,j} \\right)\n\\end{equation}\n\n\n\n\nNotice that the equation relies on the troublesome (nonexistent) point \np^k_{-1,j}\n, but according to the equality just above, we have a value we can substitute, namely \np^k_{1,j}\n. Ah! We've completed the 2nd-order Neumann condition:\n\n\n\n\n\\begin{equation}\np^{k+1}_{0,j} = \\frac{1}{4} \\left(p^{k}_{0,j-1} + p^k_{0,j+1} + 2p^{k}_{1,j} \\right)\n\\end{equation}\n\n\n\n\nThat's a bit more complicated than the first-order version, but it's relatively straightforward to code.\n\n\nNote\n\n\nDo not confuse \np^{k+1}_{-1,j}\n with \np[-1]\n:\n\np[-1]\n is a piece of Python code used to refer to the last element of a list or array named \np\n.  \np^{k+1}_{-1,j}\n is a 'ghost' point that describes a position that lies outside the actual domain.\n\n\nConvergence, Take 2\n\n\nWe can copy the previous Jacobi function and replace only the line implementing the Neumann boundary condition.  \n\n\nCareful!\n\n\nRemember that our problem has the Neumann boundary located at \nx = L\n and not \nx = 0\n as we assumed in the derivation above.\n\n\ndef laplace2d_neumann(p, l2_target):\n    '''Iteratively solves the Laplace equation using the Jacobi method\n    with second-order Neumann boundary conditions\n\n    Parameters:\n    ----------\n    p: 2D array of float\n        Initial potential distribution\n    l2_target: float\n        target for the difference between consecutive solutions\n\n    Returns:\n    -------\n    p: 2D array of float\n        Potential distribution after relaxation\n    '''\n\n    l2norm = 1\n    pn = numpy.empty_like(p)\n    while l2norm > l2_target:\n        pn = p.copy()\n        p[1:-1,1:-1] = .25 * (pn[1:-1,2:] + pn[1:-1, :-2] \\\n                              + pn[2:, 1:-1] + pn[:-2, 1:-1])\n\n        ##2nd-order Neumann B.C. along x = L\n        p[1:-1,-1] = .25 * (2*pn[1:-1,-2] + pn[2:, -1] + pn[:-2, -1])\n\n        l2norm = L2_error(p, pn)\n\n    return p\n\n\n\n\nAgain, this is the exact same code as before, but now we're running the Jacobi solver with a 2nd-order Neumann boundary condition.  Let's do a grid-refinement analysis, and plot the error versus the grid spacing.\n\n\nnx_values = [11, 21, 41, 81]\nl2_target = 1e-8\n\nerror = numpy.empty_like(nx_values, dtype=numpy.float)\n\n\nfor i, nx in enumerate(nx_values):\n    p, x, y = laplace_IG(nx)\n\n    p = laplace2d_neumann(p.copy(), l2_target)\n\n    p_an = p_analytical(x, y)\n\n    error[i] = L2_error(p, p_an)\n\n\n\n\npyplot.figure(figsize=(6,6))\npyplot.grid(True)\npyplot.xlabel(r'$n_x$', fontsize=18)\npyplot.ylabel(r'$L_2$-norm of the error', fontsize=18)\n\npyplot.loglog(nx_values, error, color='k', ls='--', lw=2, marker='o')\npyplot.axis('equal');\n\n\n\n\n\n\nNice!  That's much better.  It might not be \nexactly\n 2nd-order, but it's awfully close. (What is \n\"close enough\"\n in regards to observed convergence rates is a thorny question.)\n\n\nNow, notice from this plot that the error on the finest grid is around \n0.0002\n. Given this, perhaps we didn't need to continue iterating until a target difference between two solutions of \n10^{-8}\n. The spatial accuracy of the finite difference approximation is much worse than that! But we didn't know it ahead of time, did we? That's the \"catch 22\" of iterative solution of systems arising from discretization of PDEs.\n\n\nFinal word\n\n\nThe Jacobi method is the simplest relaxation scheme to explain and to apply. It is also the \nworst\n iterative solver! In practice, it is seldom used on its own as a solver, although it is useful as a smoother with multi-grid methods. There are much better iterative methods! If you are curious you can find them in this \nlesson\n. \n\n\n#Ignore this cell, It simply loads a style for the notebook.\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    try:\n        styles = open(\"../styles/custom.css\", \"r\").read()\n        return HTML(styles)\n    except:\n        pass\ncss_styling()\n\n\n\n\n\n\n\n\n\n\n</p>\n<p>@font-face {\n    font-family: \"Computer Modern\";\n    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n}</p>\n<h1 id=\"notebook_panel-main-background\">notebook_panel { /<em> main background </em>/</h1>\n<pre><code>background: rgb(245,245,245);\n</code></pre>\n<p>}</p>\n<p>div.cell { /<em> set cell width </em>/\n    width: 750px;\n}</p>\n<p>div #notebook { /<em> centre the content </em>/\n    background: #fff; /<em> white background for content </em>/\n    width: 1000px;\n    margin: auto;\n    padding-left: 0em;\n}</p>\n<h1 id=\"notebook-li-more-space-between-bullet-points\">notebook li { /<em> More space between bullet points </em>/</h1>\n<p>margin-top:0.8em;\n}</p>\n<p>/<em> draw border around running cells </em>/\ndiv.cell.border-box-sizing.code_cell.running { \n    border: 1px solid #111;\n}</p>\n<p>/<em> Put a solid color box around each cell and its output, visually linking them</em>/\ndiv.cell.code_cell {\n    background-color: rgb(256,256,256); \n    border-radius: 0px; \n    padding: 0.5em;\n    margin-left:1em;\n    margin-top: 1em;\n}</p>\n<p>div.text_cell_render{\n    font-family: 'Alegreya Sans' sans-serif;\n    line-height: 140%;\n    font-size: 125%;\n    font-weight: 400;\n    width:600px;\n    margin-left:auto;\n    margin-right:auto;\n}</p>\n<p>/<em> Formatting for header cells </em>/\n.text_cell_render h1 {\n    font-family: 'Fenix', sans-serif;\n    font-style:regular;\n    font-weight: 200;  <br />\n    font-size: 40pt;\n    line-height: 100%;\n    color: #138d75;\n    margin-bottom: 0.5em;\n    margin-top: 0.5em;\n    display: block;\n} <br />\n.text_cell_render h2 {\n    font-family: 'Fenix', serif;\n    font-size: 20pt;\n    line-height: 100%;\n    margin-bottom: 0.1em;\n    color: #1f618d;\n    margin-top: 0.3em;\n    display: block;\n}   </p>\n<p>.text_cell_render h3 {\n    font-family: 'Fenix', serif;\n    margin-top:12px;\n    font-size: 16pt;\n    margin-bottom: 3px;\n    font-style: regular;\n}</p>\n<p>.text_cell_render h4 {    /<em>Use this for captions</em>/\n    font-family: 'Fenix', serif;\n    font-size: 12pt;\n    text-align: center;\n    margin-top: 0em;\n    margin-bottom: 2em;\n    font-style: regular;\n}</p>\n<p>.text_cell_render h5 {  /<em>Use this for small titles</em>/\n    font-family: 'Alegreya Sans', sans-serif;\n    font-weight: 300;\n    font-size: 16pt;\n    color: #CD2305;\n    font-style: italic;\n    margin-bottom: .5em;\n    margin-top: 0.5em;\n    display: block;\n}</p>\n<p>.text_cell_render h6 { /<em>use this for copyright note</em>/\n    font-family: 'Source Code Pro', sans-serif;\n    font-weight: 300;\n    font-size: 9pt;\n    line-height: 100%;\n    color: grey;\n    margin-bottom: 1px;\n    margin-top: 1px;\n}</p>\n<pre><code>.CodeMirror{\n        font-family: \"Source Code Pro\";\n        font-size: 90%;\n}\n</code></pre>\n<p>/<em>    .prompt{\n        display: None;\n    }</em>/</p>\n<pre><code>.warning{\n    color: rgb( 240, 20, 20 )\n    }\n</code></pre>\n<p>\n\n\n\n    MathJax.Hub.Config({\n                        TeX: {\n                           extensions: [\"AMSmath.js\"], \n                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n                           },\n                tex2jax: {\n                    inlineMath: [ ['<script type=\"math/tex\">','\n'], [\"\\(\",\"\\)\"] ],\n                    displayMath: [ ['\n','\n'], [\"\\[\",\"\\]\"] ]\n                },\n                displayAlign: 'center', // Change this to 'center' to center equations.\n                \"HTML-CSS\": {\n                    styles: {'.MathJax_Display': {\"margin\": 4}}\n                }\n        });",
            "title": "Elliptic PDEs"
        },
        {
            "location": "/laplace/1/#relax-and-hold-steady",
            "text": "Many problems in physics have no time dependence, yet are rich with physical meaning: the gravitational field produced by a massive object, the electrostatic potential of a charge distribution, the displacement of a stretched membrane and the steady flow of fluid through a porous medium ... all these can be modeled by  Poisson's equation :   \\begin{equation}\n\\nabla^2 u = f\n\\end{equation}   where the unknown  u  and the known  f  are functions of space, in a domain  \\Omega . To find the solution, we require boundary conditions. These could be  Dirichlet boundary conditions, specifying the value of the solution on the boundary,   \\begin{equation}\nu = b_1 \\text{ on } \\partial\\Omega,\n\\end{equation}   or Neumann boundary conditions, specifying the normal derivative of the solution on the boundary,   \\begin{equation}\n\\frac{\\partial u}{\\partial n} = b_2 \\text{ on } \\partial\\Omega.\n\\end{equation}   A boundary-value problem consists of finding  u , given the above information. Numerically, we can do this using  relaxation methods , which start with an initial guess for  u  and then iterate towards the solution. Let's find out how!",
            "title": "Relax and hold steady"
        },
        {
            "location": "/laplace/1/#laplaces-equation",
            "text": "The particular case of  f=0  (homogeneous case) results in Laplace's equation:   \\begin{equation}\n\\nabla^2 u = 0\n\\end{equation}   For example, the equation for steady, two-dimensional heat conduction is:   \\begin{equation}\n\\frac{\\partial ^2 T}{\\partial x^2} + \\frac{\\partial ^2 T}{\\partial y^2} = 0\n\\end{equation}   where  T  is a temperature that has reached steady state. The Laplace equation models the equilibrium state of a system under the supplied boundary conditions.  The study of solutions to Laplace's equation is called  potential theory , and the solutions themselves are often potential fields. Let's use  p  from now on to represent our generic dependent variable, and write Laplace's equation again (in two dimensions):   \\begin{equation}\n\\frac{\\partial ^2 p}{\\partial x^2} + \\frac{\\partial ^2 p}{\\partial y^2} = 0\n\\end{equation}   Like in the diffusion equation, we discretize the second-order derivatives with  central differences . If you need to refresh your mind, check out this  lesson  and try to discretize the equation by yourself. On a two-dimensional Cartesian grid, it gives:   \\begin{equation}\n\\frac{p_{i+1, j}  - 2p_{i,j}  + p_{i-1,j} }{\\Delta x^2} + \\frac{p_{i,j+1}  - 2p_{i,j}  + p_{i, j-1} }{\\Delta y^2} = 0\n\\end{equation}   When  \\Delta x = \\Delta y , we end up with the following equation:   \\begin{equation}\np_{i+1, j}   + p_{i-1,j} + p_{i,j+1}  + p_{i, j-1}- 4 p_{i,j} = 0\n\\end{equation}   This tells us that the Laplacian differential operator at grid point  (i,j)  can be evaluated discretely using the value of  p  at that point (with a factor  -4 ) and the four neighboring points to the left and right, above and below grid point  (i,j) .  The stencil of the discrete Laplacian operator is shown in Figure 1. It is typically called the  five-point stencil , for obvious reasons.",
            "title": "Laplace's equation"
        },
        {
            "location": "/laplace/1/#figure-1-laplace-five-point-stencil",
            "text": "The discrete equation above is valid for every interior point in the domain. If we write the equations for  all  interior points, we have a linear system of algebraic equations. We  could  solve the linear system directly (e.g., with Gaussian elimination), but we can be more clever than that!  Notice that the coefficient matrix of such a linear system has mostly zeroes. For a uniform spatial grid, the matrix is  block diagonal : it has diagonal blocks that are tridiagonal with  -4  on the main diagonal and  1  on two off-center diagonals, and two more diagonals with  1 . All of the other elements are zero. Iterative methods are particularly suited for a system with this structure, and save us from storing all those zeroes.  We will start with an initial guess for the solution,  p_{i,j}^{0} , and use the discrete Laplacian to get an update,  p_{i,j}^{1} , then continue on computing  p_{i,j}^{k}  until we're happy.  Note that  k  is  not  a time index here, but an index corresponding to the number of iterations we perform in the  relaxation scheme .    At each iteration, we compute updated values  p_{i,j}^{k+1}  in a (hopefully) clever way so that they converge to a set of values satisfying Laplace's equation. The system will reach equilibrium only as the number of iterations tends to  \\infty , but we can approximate the equilibrium state by iterating until the change between one iteration and the next is  very  small.    The most intuitive method of iterative solution is known as the  Jacobi method , in which the values at the grid points are replaced by the corresponding weighted averages:   \\begin{equation}\np^{k+1}_{i,j} = \\frac{1}{4} \\left(p^{k}_{i,j-1} + p^k_{i,j+1} + p^{k}_{i-1,j} + p^k_{i+1,j} \\right)\n\\end{equation}   This method does indeed converge to the solution of Laplace's equation. Thank you Professor Jacobi!",
            "title": "Figure 1: Laplace five-point stencil."
        },
        {
            "location": "/laplace/1/#challenge-task",
            "text": "Grab a piece of paper and write out the coefficient matrix for a discretization with 7 grid points in the  x  direction (5 interior points) and 5 points in the  y  direction (3 interior). The system should have 15 unknowns, and the coefficient matrix three diagonal blocks. Assume prescribed Dirichlet boundary conditions on all sides (not necessarily zero).",
            "title": "Challenge task"
        },
        {
            "location": "/laplace/1/#boundary-conditions-and-relaxation",
            "text": "Suppose we want to model steady-state heat transfer on (say) a computer chip with one side insulated (zero Neumann BC), two sides held at a fixed temperature (Dirichlet condition) and one side touching a component that has a sinusoidal distribution of temperature.\nWe would need to solve Laplace's equation with boundary conditions like   \\begin{equation}\n  \\begin{gathered}\np=0 \\text{ at } x=0\\\\\n\\frac{\\partial p}{\\partial x} = 0 \\text{ at } x = L\\\\\np = 0 \\text{ at }y = 0 \\\\\np = \\sin \\left(  \\frac{\\frac{3}{2}\\pi x}{L} \\right) \\text{ at } y = H.\n  \\end{gathered}\n\\end{equation}   We'll take  L=1  and  H=1  for the sizes of the domain in the  x  and  y  directions.  One of the defining features of elliptic PDEs is that they are \"driven\" by the boundary conditions.  In the iterative solution of Laplace's equation, boundary conditions are set and  the solution relaxes  from an initial guess to join the boundaries together smoothly, given those conditions.  Our initial guess will be  p=0  everywhere. Now, let's relax!  First, we import our usual smattering of libraries (plus a few new ones!)  from matplotlib import pyplot\nimport numpy\n%matplotlib inline\nfrom matplotlib import rcParams\nrcParams['font.family'] = 'serif'\nrcParams['font.size'] = 16  To visualize 2D data, we can use  pyplot.imshow() , but a 3D plot can sometimes show a more intuitive view the solution. Or it's just prettier!  Be sure to enjoy the many examples of 3D plots in the  mplot3d  section of the  Matplotlib Gallery .    We'll import the  Axes3D  library from Matplotlib and also grab the  cm  package, which provides different colormaps for visualizing plots.    from mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm  Let's define a function for setting up our plotting environment, to avoid repeating this set-up over and over again. It will save us some typing.  def plot_3D(x, y, p):\n    '''Creates 3D plot with appropriate limits and viewing angle\n\n    Parameters:\n    ----------\n    x: array of float\n        nodal coordinates in x\n    y: array of float\n        nodal coordinates in y\n    p: 2D array of float\n        calculated potential field\n\n    '''\n    fig = pyplot.figure(figsize=(11,7), dpi=100)\n    ax = fig.gca(projection='3d')\n    X,Y = numpy.meshgrid(x,y)\n    surf = ax.plot_surface(X,Y,p[:], rstride=1, cstride=1, cmap=cm.viridis,\n            linewidth=0, antialiased=False)\n\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,1)\n    ax.set_xlabel('$x$')\n    ax.set_ylabel('$y$')\n    ax.set_zlabel('$z$')\n    ax.view_init(30,45)",
            "title": "Boundary conditions and relaxation"
        },
        {
            "location": "/laplace/1/#note",
            "text": "This plotting function uses  Viridis , a new (and  awesome ) colormap available in Matplotlib versions 1.5 and greater.  If you see an error when you try to plot using  cm.viridis , just update Matplotlib using  conda  or  pip .",
            "title": "Note"
        },
        {
            "location": "/laplace/1/#analytical-solution",
            "text": "The Laplace equation with the boundary conditions listed above has an analytical solution, given by   \\begin{equation}\np(x,y) = \\frac{\\sinh \\left( \\frac{\\frac{3}{2} \\pi y}{L}\\right)}{\\sinh \\left(  \\frac{\\frac{3}{2} \\pi H}{L}\\right)} \\sin \\left( \\frac{\\frac{3}{2} \\pi x}{L} \\right)\n\\end{equation}   where  L  and  H  are the length of the domain in the  x  and  y  directions, respectively.  We will use  numpy.meshgrid  to plot our 2D solutions. This is a function that takes two vectors ( x  and  y , say) and returns two 2D arrays of  x  and  y  coordinates that we then use to create the contour plot. Always useful,  linspace  creates 1-row arrays of equally spaced numbers: it helps for defining  x  and  y  axes in line plots, but now we want the analytical solution plotted for every point in our domain.  To do this, we'll use in the analytical solution the 2D arrays generated by  numpy.meshgrid .  def p_analytical(x, y):\n    X, Y = numpy.meshgrid(x,y)\n\n    p_an = numpy.sinh(1.5*numpy.pi*Y / x[-1]) /\\\n    (numpy.sinh(1.5*numpy.pi*y[-1]/x[-1]))*numpy.sin(1.5*numpy.pi*X/x[-1])\n\n    return p_an  Ok, let's try out the analytical solution and use it to test the  plot_3D  function we wrote above.    nx = 41\nny = 41\n\nx = numpy.linspace(0,1,nx)\ny = numpy.linspace(0,1,ny)\n\np_an = p_analytical(x,y)  plot_3D(x,y,p_an)   It worked!  This is what the solution  should  look like when we're 'done' relaxing. (And isn't viridis a cool colormap?)",
            "title": "Analytical solution"
        },
        {
            "location": "/laplace/1/#how-long-do-we-iterate",
            "text": "We noted above that there is no time dependence in the Laplace equation.  So it doesn't make a lot of sense to use a  for  loop with  nt  iterations.  Instead, we can use a  while  loop that continues to iteratively apply the relaxation scheme until the difference between two successive iterations is small enough.    But how small is small enough?  That's a good question.  We'll try to work that out as we go along.    To compare two successive potential fields, a good option is to use the  L2 norm  of the difference.  It's defined as   \\begin{equation}\n|\\textbf{x}| = \\sqrt{\\sum_{i=0, j=0}^k \\left|p^{k+1}_{i,j} - p^k_{i,j}\\right|^2}\n\\end{equation}   But there's one flaw with this formula.  We are summing the difference between successive iterations at each point on the grid. So what happens when the grid grows? (For example, if we're refining the grid, for whatever reason.) There will be more grid points to compare and so more contributions to the sum. The norm will be a larger number just because of the grid size!  That doesn't seem right.  We'll fix it by normalizing the norm, dividing the above formula by the norm of the potential field at iteration  k .   For two successive iterations, the relative L2 norm is then calculated as   \\begin{equation}\n|\\textbf{x}| = \\frac{\\sqrt{\\sum_{i=0, j=0}^k \\left|p^{k+1}_{i,j} - p^k_{i,j}\\right|^2}}{\\sqrt{\\sum_{i=0, j=0}^k \\left|p^k_{i,j}\\right|^2}}\n\\end{equation}   Our Python code for this calculation is a one-line function:  def L2_error(p, pn):\n    return numpy.sqrt(numpy.sum((p - pn)**2)/numpy.sum(pn**2))  Now, let's define a function that will apply Jacobi's method for Laplace's equation.  Three of the boundaries are Dirichlet boundaries and so we can simply leave them alone.  Only the Neumann boundary needs to be explicitly calculated at each iteration, and we'll do it by discretizing the derivative in its first order approximation:    def laplace2d(p, l2_target):\n    '''Iteratively solves the Laplace equation using the Jacobi method\n\n    Parameters:\n    ----------\n    p: 2D array of float\n        Initial potential distribution\n    l2_target: float\n        target for the difference between consecutive solutions\n\n    Returns:\n    -------\n    p: 2D array of float\n        Potential distribution after relaxation\n    '''\n\n    l2norm = 1\n    pn = numpy.empty_like(p)\n    while l2norm > l2_target:\n        pn = p.copy()\n        p[1:-1,1:-1] = .25 * (pn[1:-1,2:] + pn[1:-1, :-2] \\\n                              + pn[2:, 1:-1] + pn[:-2, 1:-1])\n\n        ##Neumann B.C. along x = L\n        p[1:-1, -1] = p[1:-1, -2]     # 1st order approx of a derivative \n        l2norm = L2_error(p, pn)\n\n    return p",
            "title": "How long do we iterate?"
        },
        {
            "location": "/laplace/1/#rows-and-columns-and-index-order",
            "text": "The physical problem has two dimensions, so we also store the temperatures in two dimensions: in a 2D array.   We chose to store it with the  y  coordinates corresponding to the rows of the array and  x  coordinates varying with the columns (this is just a code design decision!). If we are consistent with the stencil formula (with  x  corresponding to index  i  and  y  to index  j ), then  p_{i,j}  will be stored in array format as  p[j,i] .  This might be a little confusing as most of us are used to writing coordinates in the format  (x,y) , but our preference is to have the data stored so that it matches the physical orientation of the problem. Then, when we make a plot of the solution, the visualization will make sense to us, with respect to the geometry of our set-up. That's just nicer than to have the plot rotated!",
            "title": "Rows and columns, and index order"
        },
        {
            "location": "/laplace/1/#figure-2-row-column-data-storage",
            "text": "As you can see on Figure 2 above, if we want to access the value  18  we would write those coordinates as  (x_2, y_3) .  You can also see that its location is the 3rd row, 2nd column, so its array address would be  p[3,2] .  Again, this is a design decision.  However you can choose to manipulate and store your data however you like; just remember to be consistent!",
            "title": "Figure 2: Row-column data storage"
        },
        {
            "location": "/laplace/1/#lets-relax",
            "text": "The initial values of the potential field are zero everywhere (initial guess), except at the boundary:    p = \\sin \\left(  \\frac{\\frac{3}{2}\\pi x}{L} \\right) \\text{ at } y=H   To initialize the domain,  numpy.zeros  will handle everything except that one Dirichlet condition. Let's do it!  ##variable declarations\nnx = 41\nny = 41\n\n\n##initial conditions\np = numpy.zeros((ny,nx)) ##create a XxY vector of 0's\n\n\n##plotting aids\nx = numpy.linspace(0,1,nx)\ny = numpy.linspace(0,1,ny)\n\n##Dirichlet boundary conditions\np[-1,:] = numpy.sin(1.5*numpy.pi*x/x[-1])  Now let's visualize the initial conditions using the  plot_3D  function, just to check we've got it right.  plot_3D(x, y, p)   The  p  array is equal to zero everywhere, except along the boundary  y = 1 .  Hopefully you can see how the relaxed solution and this initial condition are related.    Now, run the iterative solver with a target L2-norm difference between successive iterations of  10^{-8} .  p = laplace2d(p.copy(), 1e-8)  Let's make a gorgeous plot of the final field using the newly minted  plot_3D  function.  plot_3D(x,y,p)   Awesome!  That looks pretty good.  But we'll need more than a simple visual check, though. The \"eyeball metric\" is very forgiving!",
            "title": "Let's relax!"
        },
        {
            "location": "/laplace/1/#convergence-analysis",
            "text": "",
            "title": "Convergence analysis"
        },
        {
            "location": "/laplace/1/#convergence-take-1",
            "text": "We want to make sure that our Jacobi function is working properly.  Since we have an analytical solution, what better way than to do a grid-convergence analysis?  We will run our solver for several grid sizes and look at how fast the L2 norm of the difference between consecutive iterations decreases.  Let's make our lives easier by writing a function to \"reset\" the initial guess for each grid so we don't have to keep copying and pasting them.  def laplace_IG(nx):\n    '''Generates initial guess for Laplace 2D problem for a \n    given number of grid points (nx) within the domain [0,1]x[0,1]\n\n    Parameters:\n    ----------\n    nx: int\n        number of grid points in x (and implicitly y) direction\n\n    Returns:\n    -------\n    p: 2D array of float\n        Pressure distribution after relaxation\n    x: array of float\n        linspace coordinates in x\n    y: array of float\n        linspace coordinates in y\n    '''\n\n    ##initial conditions\n    p = numpy.zeros((nx,nx)) ##create a XxY vector of 0's\n\n    ##plotting aids\n    x = numpy.linspace(0,1,nx)\n    y = x\n\n    ##Dirichlet boundary conditions\n    p[:,0] = 0\n    p[0,:] = 0\n    p[-1,:] = numpy.sin(1.5*numpy.pi*x/x[-1])\n\n    return p, x, y  Now run Jacobi's method on the Laplace equation using four different grids, with the same exit criterion of  10^{-8}  each time. Then, we look at the error versus the grid size in a log-log plot. What do we get?  nx_values = [11, 21, 41, 81]\nl2_target = 1e-8\n\nerror = numpy.empty_like(nx_values, dtype=numpy.float)\n\nfor i, nx in enumerate(nx_values):\n    p, x, y = laplace_IG(nx)\n\n    p = laplace2d(p.copy(), l2_target)\n\n    p_an = p_analytical(x, y)\n\n    error[i] = L2_error(p, p_an)  pyplot.figure(figsize=(6,6))\npyplot.grid(True)\npyplot.xlabel(r'$n_x$', fontsize=18)\npyplot.ylabel(r'$L_2$-norm of the error', fontsize=18)\n\npyplot.loglog(nx_values, error, color='k', ls='--', lw=2, marker='o')\npyplot.axis('equal');   Hmm. That doesn't look like 2nd-order convergence, but we're using second-order finite differences.   What's going on?   The culprit is the boundary conditions. Dirichlet conditions are order-agnostic (a  set value is a set value), but the scheme we used for the Neumann boundary condition is 1st-order.    Remember when we said that the boundaries drive the problem?  One boundary that's 1st-order completely tanked our spatial convergence.  Let's fix it!",
            "title": "Convergence, Take 1"
        },
        {
            "location": "/laplace/1/#2nd-order-neumann-bcs",
            "text": "Up to this point, we have used the first-order approximation of a derivative to satisfy Neumann B.C.'s. For a boundary located at  x=0  this reads,   \\begin{equation}\n\\frac{p^{k+1}_{1,j} - p^{k+1}_{0,j}}{\\Delta x} = 0\n\\end{equation}   which, solving for  p^{k+1}_{0,j}  gives us   \\begin{equation}\np^{k+1}_{0,j} = p^{k+1}_{1,j}\n\\end{equation}   Using that Neumann condition will limit us to 1st-order convergence.  Instead, we can start with a 2nd-order approximation (the central-difference approximation):   \\begin{equation}\n\\frac{p^{k+1}_{1,j} - p^{k+1}_{-1,j}}{2 \\Delta x} = 0\n\\end{equation}   That seems problematic, since there is no grid point  p^{k}_{-1,j} .  But no matter \u2026 let's carry on. According to the 2nd-order approximation,   \\begin{equation}\np^{k+1}_{-1,j} = p^{k+1}_{1,j}\n\\end{equation}   Recall the finite-difference Jacobi equation with  i=0 :   \\begin{equation}\np^{k+1}_{0,j} = \\frac{1}{4} \\left(p^{k}_{0,j-1} + p^k_{0,j+1} + p^{k}_{-1,j} + p^k_{1,j} \\right)\n\\end{equation}   Notice that the equation relies on the troublesome (nonexistent) point  p^k_{-1,j} , but according to the equality just above, we have a value we can substitute, namely  p^k_{1,j} . Ah! We've completed the 2nd-order Neumann condition:   \\begin{equation}\np^{k+1}_{0,j} = \\frac{1}{4} \\left(p^{k}_{0,j-1} + p^k_{0,j+1} + 2p^{k}_{1,j} \\right)\n\\end{equation}   That's a bit more complicated than the first-order version, but it's relatively straightforward to code.",
            "title": "2nd-order Neumann BCs"
        },
        {
            "location": "/laplace/1/#note_1",
            "text": "Do not confuse  p^{k+1}_{-1,j}  with  p[-1] : p[-1]  is a piece of Python code used to refer to the last element of a list or array named  p .   p^{k+1}_{-1,j}  is a 'ghost' point that describes a position that lies outside the actual domain.",
            "title": "Note"
        },
        {
            "location": "/laplace/1/#convergence-take-2",
            "text": "We can copy the previous Jacobi function and replace only the line implementing the Neumann boundary condition.",
            "title": "Convergence, Take 2"
        },
        {
            "location": "/laplace/1/#careful",
            "text": "Remember that our problem has the Neumann boundary located at  x = L  and not  x = 0  as we assumed in the derivation above.  def laplace2d_neumann(p, l2_target):\n    '''Iteratively solves the Laplace equation using the Jacobi method\n    with second-order Neumann boundary conditions\n\n    Parameters:\n    ----------\n    p: 2D array of float\n        Initial potential distribution\n    l2_target: float\n        target for the difference between consecutive solutions\n\n    Returns:\n    -------\n    p: 2D array of float\n        Potential distribution after relaxation\n    '''\n\n    l2norm = 1\n    pn = numpy.empty_like(p)\n    while l2norm > l2_target:\n        pn = p.copy()\n        p[1:-1,1:-1] = .25 * (pn[1:-1,2:] + pn[1:-1, :-2] \\\n                              + pn[2:, 1:-1] + pn[:-2, 1:-1])\n\n        ##2nd-order Neumann B.C. along x = L\n        p[1:-1,-1] = .25 * (2*pn[1:-1,-2] + pn[2:, -1] + pn[:-2, -1])\n\n        l2norm = L2_error(p, pn)\n\n    return p  Again, this is the exact same code as before, but now we're running the Jacobi solver with a 2nd-order Neumann boundary condition.  Let's do a grid-refinement analysis, and plot the error versus the grid spacing.  nx_values = [11, 21, 41, 81]\nl2_target = 1e-8\n\nerror = numpy.empty_like(nx_values, dtype=numpy.float)\n\n\nfor i, nx in enumerate(nx_values):\n    p, x, y = laplace_IG(nx)\n\n    p = laplace2d_neumann(p.copy(), l2_target)\n\n    p_an = p_analytical(x, y)\n\n    error[i] = L2_error(p, p_an)  pyplot.figure(figsize=(6,6))\npyplot.grid(True)\npyplot.xlabel(r'$n_x$', fontsize=18)\npyplot.ylabel(r'$L_2$-norm of the error', fontsize=18)\n\npyplot.loglog(nx_values, error, color='k', ls='--', lw=2, marker='o')\npyplot.axis('equal');   Nice!  That's much better.  It might not be  exactly  2nd-order, but it's awfully close. (What is  \"close enough\"  in regards to observed convergence rates is a thorny question.)  Now, notice from this plot that the error on the finest grid is around  0.0002 . Given this, perhaps we didn't need to continue iterating until a target difference between two solutions of  10^{-8} . The spatial accuracy of the finite difference approximation is much worse than that! But we didn't know it ahead of time, did we? That's the \"catch 22\" of iterative solution of systems arising from discretization of PDEs.",
            "title": "Careful!"
        },
        {
            "location": "/laplace/1/#final-word",
            "text": "The Jacobi method is the simplest relaxation scheme to explain and to apply. It is also the  worst  iterative solver! In practice, it is seldom used on its own as a solver, although it is useful as a smoother with multi-grid methods. There are much better iterative methods! If you are curious you can find them in this  lesson .   #Ignore this cell, It simply loads a style for the notebook.\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    try:\n        styles = open(\"../styles/custom.css\", \"r\").read()\n        return HTML(styles)\n    except:\n        pass\ncss_styling()     </p>\n<p>@font-face {\n    font-family: \"Computer Modern\";\n    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n}</p>\n<h1 id=\"notebook_panel-main-background\">notebook_panel { /<em> main background </em>/</h1>\n<pre><code>background: rgb(245,245,245);\n</code></pre>\n<p>}</p>\n<p>div.cell { /<em> set cell width </em>/\n    width: 750px;\n}</p>\n<p>div #notebook { /<em> centre the content </em>/\n    background: #fff; /<em> white background for content </em>/\n    width: 1000px;\n    margin: auto;\n    padding-left: 0em;\n}</p>\n<h1 id=\"notebook-li-more-space-between-bullet-points\">notebook li { /<em> More space between bullet points </em>/</h1>\n<p>margin-top:0.8em;\n}</p>\n<p>/<em> draw border around running cells </em>/\ndiv.cell.border-box-sizing.code_cell.running { \n    border: 1px solid #111;\n}</p>\n<p>/<em> Put a solid color box around each cell and its output, visually linking them</em>/\ndiv.cell.code_cell {\n    background-color: rgb(256,256,256); \n    border-radius: 0px; \n    padding: 0.5em;\n    margin-left:1em;\n    margin-top: 1em;\n}</p>\n<p>div.text_cell_render{\n    font-family: 'Alegreya Sans' sans-serif;\n    line-height: 140%;\n    font-size: 125%;\n    font-weight: 400;\n    width:600px;\n    margin-left:auto;\n    margin-right:auto;\n}</p>\n<p>/<em> Formatting for header cells </em>/\n.text_cell_render h1 {\n    font-family: 'Fenix', sans-serif;\n    font-style:regular;\n    font-weight: 200;  <br />\n    font-size: 40pt;\n    line-height: 100%;\n    color: #138d75;\n    margin-bottom: 0.5em;\n    margin-top: 0.5em;\n    display: block;\n} <br />\n.text_cell_render h2 {\n    font-family: 'Fenix', serif;\n    font-size: 20pt;\n    line-height: 100%;\n    margin-bottom: 0.1em;\n    color: #1f618d;\n    margin-top: 0.3em;\n    display: block;\n}   </p>\n<p>.text_cell_render h3 {\n    font-family: 'Fenix', serif;\n    margin-top:12px;\n    font-size: 16pt;\n    margin-bottom: 3px;\n    font-style: regular;\n}</p>\n<p>.text_cell_render h4 {    /<em>Use this for captions</em>/\n    font-family: 'Fenix', serif;\n    font-size: 12pt;\n    text-align: center;\n    margin-top: 0em;\n    margin-bottom: 2em;\n    font-style: regular;\n}</p>\n<p>.text_cell_render h5 {  /<em>Use this for small titles</em>/\n    font-family: 'Alegreya Sans', sans-serif;\n    font-weight: 300;\n    font-size: 16pt;\n    color: #CD2305;\n    font-style: italic;\n    margin-bottom: .5em;\n    margin-top: 0.5em;\n    display: block;\n}</p>\n<p>.text_cell_render h6 { /<em>use this for copyright note</em>/\n    font-family: 'Source Code Pro', sans-serif;\n    font-weight: 300;\n    font-size: 9pt;\n    line-height: 100%;\n    color: grey;\n    margin-bottom: 1px;\n    margin-top: 1px;\n}</p>\n<pre><code>.CodeMirror{\n        font-family: \"Source Code Pro\";\n        font-size: 90%;\n}\n</code></pre>\n<p>/<em>    .prompt{\n        display: None;\n    }</em>/</p>\n<pre><code>.warning{\n    color: rgb( 240, 20, 20 )\n    }\n</code></pre>\n<p>  \n    MathJax.Hub.Config({\n                        TeX: {\n                           extensions: [\"AMSmath.js\"], \n                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n                           },\n                tex2jax: {\n                    inlineMath: [ ['<script type=\"math/tex\">',' '], [\"\\(\",\"\\)\"] ],\n                    displayMath: [ [' ',' '], [\"\\[\",\"\\]\"] ]\n                },\n                displayAlign: 'center', // Change this to 'center' to center equations.\n                \"HTML-CSS\": {\n                    styles: {'.MathJax_Display': {\"margin\": 4}}\n                }\n        });",
            "title": "Final word"
        },
        {
            "location": "/numba/1/",
            "text": "Intro to profiling\n\n\nPython's dirty little secret is that it can be made to run pretty fast.  \n\n\nThe bare-metal HPC people will be angrily tweeting at me now, or rather, they would be if they could get their wireless drivers working.\n\n\nStill, there are some things you \nreally\n don't want to do in Python.  Nested loops are usually a bad idea.  But often you won't know where your code is slowing down just by looking at it and trying to accelerate everything can be a waste of time.  (Developer time, that is, both now and in the future: you incur technical debt if you unintentionally obfuscate code to make it faster when it doesn't need to be).\n\n\nThe first step is always to find the bottlenecks in your code, via \nprofiling\n: analyzing your code by measuring the execution time of its parts.\n\n\nTools\n\n\n\n\ncProfile\n\n\nline_profiler\n\n\ntimeit\n\n\n\n\nNote\n:\nIf you haven't already installed it, you can do\n\n\nconda install line_profiler\n\n\n\n\nor\n\n\npip install line_profiler\n\n\n\n\nSome bad code\n\n\nHere's a bit of code guaranteed to perform poorly: it sleeps for 1.5 seconds after doing any work! We will profile it and see where we might be able to help.\n\n\nimport numpy\nfrom time import sleep\n\ndef bad_call(dude):\n    sleep(.5)\n\ndef worse_call(dude):\n    sleep(1)\n\ndef sumulate(foo):\n    if not isinstance(foo, int):\n        return\n\n    a = numpy.random.random((1000, 1000))\n    a @ a\n\n    ans = 0\n    for i in range(foo):\n        ans += i\n\n    bad_call(ans)\n    worse_call(ans)\n\n    return ans\n\n\n\n\nsumulate(150)\n\n\n\n\n11175\n\n\n\nusing \ncProfile\n\n\ncProfile\n is the built-in profiler in Python (available since Python 2.5).  It provides a function-by-function report of execution time. First import the module, then usage is simply a call to \ncProfile.run()\n with your code as argument. It will print out a list of all the functions that were called, with the number of calls and the time spent in each.\n\n\nimport cProfile\n\n\n\n\ncProfile.run('sumulate(150)')\n\n\n\n\n         10 function calls in 1.549 seconds\n\n   Ordered by: standard name\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    0.023    0.023    1.549    1.549 <ipython-input-1-ab37b7e7eed9>:10(sumulate)\n        1    0.000    0.000    0.501    0.501 <ipython-input-1-ab37b7e7eed9>:4(bad_call)\n        1    0.000    0.000    1.001    1.001 <ipython-input-1-ab37b7e7eed9>:7(worse_call)\n        1    0.000    0.000    1.549    1.549 <string>:1(<module>)\n        1    0.000    0.000    1.549    1.549 {built-in method builtins.exec}\n        1    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n        2    1.502    0.751    1.502    0.751 {built-in method time.sleep}\n        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n        1    0.024    0.024    0.024    0.024 {method 'random_sample' of 'mtrand.RandomState' objects}\n\n\n\nYou can see here that when our code \nsumulate()\n executes, it spends almost all its time in the method \ntime.sleep\n (a bit over 1.5 seconds).\n\n\nIf your program is more complicated that this cute demo, you'll have a hard time parsing the long output of \ncProfile\n. In that case, you may want a profiling visualization tool, like \nSnakeViz\n. But that is outside the scope of this tutorial.\n\n\nusing \nline_profiler\n\n\nline_profiler\n offers more granular information thatn \ncProfile\n: it will give timing information about each line of code in a profiled function.\n\n\nLoad the \nline_profiler\n extension\n\n\n%load_ext line_profiler\n\n\n\n\nFor a pop-up window with results in notebook:\n\n\nIPython has an \n%lprun\n magic to profile specific functions within an executed statement. Usage:\n\n%lprun -f func_to_profile <statement>\n (get more help by running \n%lprun?\n in IPython).\n\n\nProfiling two functions\n\n\n%lprun -f bad_call -f worse_call sumulate(13)\n\n\n\n\nWrite results to a text file\n\n\n%lprun -T timings.txt -f sumulate sumulate(12)\n\n\n\n\n*** Profile printout saved to text file 'timings.txt'.\n\n\n\n%load timings.txt\n\n\n\n\nLet's break down what these results are telling us. \n\n\nLine #\n corresponds to the line in a given script or notebook cell. To toggle line numbers in a code cell, hit \nl\n in command mode.\n\n\nHits\n shows how many times a given line was encountered as a result of running the \nsumulate\n function. You can see that most lines in \nsumulate\n were hit only once, while the lines in the \nfor\n loop were hit several times. \n\n\nTime\n is the amount of time spent, in total, on a given line. Note at the top of the profiling report that the time unit here is microseconds. \n\n\nPer hit\n is the amount of time, on average, each \nhit\n on a given line took. This is the same as the \nTime\n column for lines that were only hit once, but you can see the \nPer hit\n time has more meaning in the \nfor\n loop lines.\n\n\n% Time\n is what we are really interested in. It tells us what percentage of the total run time of \nsumulate\n was spent on a given line. There are lots of ways to optimize all sorts of operations, but you should focus your time and energy on optimizing the code that is costing you the most time. \nYou \ncould\n try to further optimize Python's builtin matrix multiplication to get the \na @ a\n to operate a little bit faster.  You might even shave off a microsecond (although probably not). But who cares? That matrix multiply takes up 2% of runtime. Focus your efforts on the expensive parts, which here are \nbad_call\n and \nworse_call\n.\n\n\nProfiling on the command line\n\n\nOpen file, add \n@profile\n decorator to any function you want to profile, then run\n\n\nkernprof -l script_to_profile.py\n\n\n\n\nwhich will generate \nscript_to_profile.py.lprof\n (pickled result).  To view the results, run\n\n\npython -m line_profiler script_to_profile.py.lprof\n\n\n\n\nfrom IPython.display import IFrame\n\n\n\n\nIFrame('http://localhost:8888/terminals/1', width=800, height=700)\n\n\n\n\n    <iframe\n        width=\"800\"\n        height=\"700\"\n        src=\"http://localhost:8888/terminals/1\"\n        frameborder=\"0\"\n        allowfullscreen\n    ></iframe>\n\n\n\ntimeit\n\n\ntimeit\n is not perfect, but it is helpful.  \n\n\nPotential concerns re: \ntimeit\n\n\n\n\nReturns minimum time of run\n\n\nOnly runs benchmark 3 times\n\n\nIt disables garbage collection\n\n\n\n\npython -m timeit -v \"print(42)\"\n\n\n\n\npython -m timeit -r 25 \"print(42)\"\n\n\n\n\npython -m timeit -s \"gc.enable()\" \"print(42)\"\n\n\n\n\nLine magic\n\n\n%timeit x = 5\n\n\n\n\n100000000 loops, best of 3: 11.3 ns per loop\n\n\n\nCell magic\n\n\n%%timeit\nx = 5\ny = 6\nx + y\n\n\n\n\n10000000 loops, best of 3: 32.9 ns per loop\n\n\n\nThe \n-q\n flag quiets output.  The \n-o\n flag allows outputting results to a variable.  The \n-q\n flag sometimes disagrees with OSX so please remove it if you're having issues.\n\n\na = %timeit -qo x = 5\n\n\n\n\nprint(a.all_runs)\n\n\n\n\n[1.1260504741221666, 1.1415640730410814, 1.1300840568728745]\n\n\n\nprint(a.best)\n\n\n\n\n1.1260504741221666e-08\n\n\n\nprint(a.worst)\n\n\n\n\n4.5797787606716156e-07",
            "title": "Profiling"
        },
        {
            "location": "/numba/1/#intro-to-profiling",
            "text": "Python's dirty little secret is that it can be made to run pretty fast.    The bare-metal HPC people will be angrily tweeting at me now, or rather, they would be if they could get their wireless drivers working.  Still, there are some things you  really  don't want to do in Python.  Nested loops are usually a bad idea.  But often you won't know where your code is slowing down just by looking at it and trying to accelerate everything can be a waste of time.  (Developer time, that is, both now and in the future: you incur technical debt if you unintentionally obfuscate code to make it faster when it doesn't need to be).  The first step is always to find the bottlenecks in your code, via  profiling : analyzing your code by measuring the execution time of its parts.",
            "title": "Intro to profiling"
        },
        {
            "location": "/numba/1/#tools",
            "text": "cProfile  line_profiler  timeit   Note :\nIf you haven't already installed it, you can do  conda install line_profiler  or  pip install line_profiler",
            "title": "Tools"
        },
        {
            "location": "/numba/1/#some-bad-code",
            "text": "Here's a bit of code guaranteed to perform poorly: it sleeps for 1.5 seconds after doing any work! We will profile it and see where we might be able to help.  import numpy\nfrom time import sleep\n\ndef bad_call(dude):\n    sleep(.5)\n\ndef worse_call(dude):\n    sleep(1)\n\ndef sumulate(foo):\n    if not isinstance(foo, int):\n        return\n\n    a = numpy.random.random((1000, 1000))\n    a @ a\n\n    ans = 0\n    for i in range(foo):\n        ans += i\n\n    bad_call(ans)\n    worse_call(ans)\n\n    return ans  sumulate(150)  11175",
            "title": "Some bad code"
        },
        {
            "location": "/numba/1/#using-cprofile",
            "text": "cProfile  is the built-in profiler in Python (available since Python 2.5).  It provides a function-by-function report of execution time. First import the module, then usage is simply a call to  cProfile.run()  with your code as argument. It will print out a list of all the functions that were called, with the number of calls and the time spent in each.  import cProfile  cProfile.run('sumulate(150)')           10 function calls in 1.549 seconds\n\n   Ordered by: standard name\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    0.023    0.023    1.549    1.549 <ipython-input-1-ab37b7e7eed9>:10(sumulate)\n        1    0.000    0.000    0.501    0.501 <ipython-input-1-ab37b7e7eed9>:4(bad_call)\n        1    0.000    0.000    1.001    1.001 <ipython-input-1-ab37b7e7eed9>:7(worse_call)\n        1    0.000    0.000    1.549    1.549 <string>:1(<module>)\n        1    0.000    0.000    1.549    1.549 {built-in method builtins.exec}\n        1    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n        2    1.502    0.751    1.502    0.751 {built-in method time.sleep}\n        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n        1    0.024    0.024    0.024    0.024 {method 'random_sample' of 'mtrand.RandomState' objects}  You can see here that when our code  sumulate()  executes, it spends almost all its time in the method  time.sleep  (a bit over 1.5 seconds).  If your program is more complicated that this cute demo, you'll have a hard time parsing the long output of  cProfile . In that case, you may want a profiling visualization tool, like  SnakeViz . But that is outside the scope of this tutorial.",
            "title": "using cProfile"
        },
        {
            "location": "/numba/1/#using-line_profiler",
            "text": "line_profiler  offers more granular information thatn  cProfile : it will give timing information about each line of code in a profiled function.  Load the  line_profiler  extension  %load_ext line_profiler",
            "title": "using line_profiler"
        },
        {
            "location": "/numba/1/#for-a-pop-up-window-with-results-in-notebook",
            "text": "IPython has an  %lprun  magic to profile specific functions within an executed statement. Usage: %lprun -f func_to_profile <statement>  (get more help by running  %lprun?  in IPython).",
            "title": "For a pop-up window with results in notebook:"
        },
        {
            "location": "/numba/1/#profiling-two-functions",
            "text": "%lprun -f bad_call -f worse_call sumulate(13)",
            "title": "Profiling two functions"
        },
        {
            "location": "/numba/1/#write-results-to-a-text-file",
            "text": "%lprun -T timings.txt -f sumulate sumulate(12)  *** Profile printout saved to text file 'timings.txt'.  %load timings.txt  Let's break down what these results are telling us.   Line #  corresponds to the line in a given script or notebook cell. To toggle line numbers in a code cell, hit  l  in command mode.  Hits  shows how many times a given line was encountered as a result of running the  sumulate  function. You can see that most lines in  sumulate  were hit only once, while the lines in the  for  loop were hit several times.   Time  is the amount of time spent, in total, on a given line. Note at the top of the profiling report that the time unit here is microseconds.   Per hit  is the amount of time, on average, each  hit  on a given line took. This is the same as the  Time  column for lines that were only hit once, but you can see the  Per hit  time has more meaning in the  for  loop lines.  % Time  is what we are really interested in. It tells us what percentage of the total run time of  sumulate  was spent on a given line. There are lots of ways to optimize all sorts of operations, but you should focus your time and energy on optimizing the code that is costing you the most time. \nYou  could  try to further optimize Python's builtin matrix multiplication to get the  a @ a  to operate a little bit faster.  You might even shave off a microsecond (although probably not). But who cares? That matrix multiply takes up 2% of runtime. Focus your efforts on the expensive parts, which here are  bad_call  and  worse_call .",
            "title": "Write results to a text file"
        },
        {
            "location": "/numba/1/#profiling-on-the-command-line",
            "text": "Open file, add  @profile  decorator to any function you want to profile, then run  kernprof -l script_to_profile.py  which will generate  script_to_profile.py.lprof  (pickled result).  To view the results, run  python -m line_profiler script_to_profile.py.lprof  from IPython.display import IFrame  IFrame('http://localhost:8888/terminals/1', width=800, height=700)      <iframe\n        width=\"800\"\n        height=\"700\"\n        src=\"http://localhost:8888/terminals/1\"\n        frameborder=\"0\"\n        allowfullscreen\n    ></iframe>",
            "title": "Profiling on the command line"
        },
        {
            "location": "/numba/1/#timeit",
            "text": "timeit  is not perfect, but it is helpful.    Potential concerns re:  timeit   Returns minimum time of run  Only runs benchmark 3 times  It disables garbage collection   python -m timeit -v \"print(42)\"  python -m timeit -r 25 \"print(42)\"  python -m timeit -s \"gc.enable()\" \"print(42)\"",
            "title": "timeit"
        },
        {
            "location": "/numba/1/#line-magic",
            "text": "%timeit x = 5  100000000 loops, best of 3: 11.3 ns per loop",
            "title": "Line magic"
        },
        {
            "location": "/numba/1/#cell-magic",
            "text": "%%timeit\nx = 5\ny = 6\nx + y  10000000 loops, best of 3: 32.9 ns per loop  The  -q  flag quiets output.  The  -o  flag allows outputting results to a variable.  The  -q  flag sometimes disagrees with OSX so please remove it if you're having issues.  a = %timeit -qo x = 5  print(a.all_runs)  [1.1260504741221666, 1.1415640730410814, 1.1300840568728745]  print(a.best)  1.1260504741221666e-08  print(a.worst)  4.5797787606716156e-07",
            "title": "Cell magic"
        },
        {
            "location": "/numba/2/",
            "text": "Using \njit\n\n\nWe know how to find hotspots now, how do we improve their performance?\n\n\nWe \njit\n them!\n\n\nWe'll start with a trivial example but get to some more realistic applications shortly.\n\n\nArray sum\n\n\nThe function below is a naive \nsum\n function that sums all the elements of a given array.\n\n\ndef sum_array(inp):\n    J, I = inp.shape\n\n    #this is a bad idea\n    mysum = 0\n    for j in range(J):\n        for i in range(I):\n            mysum += inp[j, i]\n\n    return mysum\n\n\n\n\nimport numpy\n\n\n\n\narr = numpy.random.random((300, 300))\n\n\n\n\nFirst hand the array \narr\n off to \nsum_array\n to make sure it works (or at least doesn't error out)\n\n\nsum_array(arr)\n\n\n\n\n45102.033677230997\n\n\n\nNow run and save \ntimeit\n results of \nsum_array\n as a baseline to compare against.\n\n\nplain = %timeit -o sum_array(arr)\n\n\n\n\n100 loops, best of 3: 14.7 ms per loop\n\n\n\nLet's get started\n\n\nfrom numba import jit\n\n\n\n\nNote\n: There are two ways to \njit\n a function. These are just two ways of doing the same thing.  You can choose whichever you prefer.\n\n\nAs a function call\n\n\nsum_array_numba = jit()(sum_array)\n\n\n\n\nWhat's up with the weird double \n()\ns?  We'll cover that in a little bit.\n\n\nNow we have a new function, called \nsum_array_numba\n which is the \njit\nted version of \nsum_array\n. \nWe can again make sure that it works (and hopefully produces the same result as \nsum_array\n).\n\n\nsum_array_numba(arr)\n\n\n\n\n45102.033677231\n\n\n\nGood, that's the same result as the first version, so nothing has gone horribly wrong.  \n\n\nNow let's time and save these results.\n\n\njitted = %timeit -o sum_array_numba(arr)\n\n\n\n\n10000 loops, best of 3: 73.9 \u00b5s per loop\n\n\n\nWow. 73.7 \u00b5s is a lot faster than 15.5 ms... How much faster?  Let's see.\n\n\nplain.best / jitted.best\n\n\n\n\n198.80740381645145\n\n\n\nSo, a factor of 210x.  Not too shabby.  But we're comparing the best runs, what about the worst runs?\n\n\nplain.worst / jitted.worst\n\n\n\n\n278.7481542534447\n\n\n\nYeah, that's still an incredible speedup.\n\n\n(more commonly) As a decorator\n\n\nThe second way to \njit\n a function is to use the \njit\n decorator.  This is a very easy syntax to handle and makes applying \njit\n to a function trivial. \n\n\nNote that the only difference in terms of the outcome (compared to the other \njit\n method) is that there will be only one function, called \nsum_array\n that is a Numba \njit\nted function. The \"original\" \nsum_array\n will no longer exist, so this method, while convenient, doesn't allow you to compare results between \"vanilla\" and \njit\nted Python.\n\n\nWhen should you use one or the other? That's up to you. If I'm investigating whether Numba can help, I use \njit\n as a function call, so I can compare results. Once I've decided to use Numba, I stick with the decorator syntax since it's much prettier (and I don't care if the \"original\" function is available).\n\n\n@jit\ndef sum_array(inp):\n    I, J = inp.shape\n\n    mysum = 0\n    for i in range(I):\n        for j in range(J):\n            mysum += inp[i, j]\n\n    return mysum\n\n\n\n\nsum_array(arr)\n\n\n\n\n45102.033677231\n\n\n\nSo again, we can see that we have the same result.  That's good.  And timing?\n\n\n%timeit sum_array(arr)\n\n\n\n\n10000 loops, best of 3: 73.8 \u00b5s per loop\n\n\n\nAs expected, more or less identical to the first \njit\n example.\n\n\nHow does this compare to NumPy?\n\n\nNumPy, of course, has built in methods for summing arrays, how does Numba stack up against those?\n\n\n%timeit arr.sum()\n\n\n\n\n10000 loops, best of 3: 33.9 \u00b5s per loop\n\n\n\nRight. Remember, NumPy has been hand-tuned over many years to be very, very good at what it does. For simple operations, Numba is not going to outperform it, but when things get more complex Numba can save the day. \n\n\nAlso, take a moment to appreciate that our \njit\nted code, which was compiled on-the-fly is offering performance in the same order of magnitude as NumPy.  That's pretty incredible.\n\n\nWhen does \nnumba\n compile things?\n\n\nnumba\n is a just-in-time (hence, \njit\n) compiler.  The very first time you run a \nnumba\n compiled function, there will be a little bit of overhead for the compilation step to take place.  In practice, this is usually not noticeable. You may get a message from \ntimeit\n that one \"run\" was much slower than most; this is due to the compilation overhead. \n\n\nYour turn!",
            "title": "Intro to JIT"
        },
        {
            "location": "/numba/2/#using-jit",
            "text": "We know how to find hotspots now, how do we improve their performance?  We  jit  them!  We'll start with a trivial example but get to some more realistic applications shortly.",
            "title": "Using jit"
        },
        {
            "location": "/numba/2/#array-sum",
            "text": "The function below is a naive  sum  function that sums all the elements of a given array.  def sum_array(inp):\n    J, I = inp.shape\n\n    #this is a bad idea\n    mysum = 0\n    for j in range(J):\n        for i in range(I):\n            mysum += inp[j, i]\n\n    return mysum  import numpy  arr = numpy.random.random((300, 300))  First hand the array  arr  off to  sum_array  to make sure it works (or at least doesn't error out)  sum_array(arr)  45102.033677230997  Now run and save  timeit  results of  sum_array  as a baseline to compare against.  plain = %timeit -o sum_array(arr)  100 loops, best of 3: 14.7 ms per loop",
            "title": "Array sum"
        },
        {
            "location": "/numba/2/#lets-get-started",
            "text": "from numba import jit  Note : There are two ways to  jit  a function. These are just two ways of doing the same thing.  You can choose whichever you prefer.",
            "title": "Let's get started"
        },
        {
            "location": "/numba/2/#as-a-function-call",
            "text": "sum_array_numba = jit()(sum_array)  What's up with the weird double  () s?  We'll cover that in a little bit.  Now we have a new function, called  sum_array_numba  which is the  jit ted version of  sum_array . \nWe can again make sure that it works (and hopefully produces the same result as  sum_array ).  sum_array_numba(arr)  45102.033677231  Good, that's the same result as the first version, so nothing has gone horribly wrong.    Now let's time and save these results.  jitted = %timeit -o sum_array_numba(arr)  10000 loops, best of 3: 73.9 \u00b5s per loop  Wow. 73.7 \u00b5s is a lot faster than 15.5 ms... How much faster?  Let's see.  plain.best / jitted.best  198.80740381645145  So, a factor of 210x.  Not too shabby.  But we're comparing the best runs, what about the worst runs?  plain.worst / jitted.worst  278.7481542534447  Yeah, that's still an incredible speedup.",
            "title": "As a function call"
        },
        {
            "location": "/numba/2/#more-commonly-as-a-decorator",
            "text": "The second way to  jit  a function is to use the  jit  decorator.  This is a very easy syntax to handle and makes applying  jit  to a function trivial.   Note that the only difference in terms of the outcome (compared to the other  jit  method) is that there will be only one function, called  sum_array  that is a Numba  jit ted function. The \"original\"  sum_array  will no longer exist, so this method, while convenient, doesn't allow you to compare results between \"vanilla\" and  jit ted Python.  When should you use one or the other? That's up to you. If I'm investigating whether Numba can help, I use  jit  as a function call, so I can compare results. Once I've decided to use Numba, I stick with the decorator syntax since it's much prettier (and I don't care if the \"original\" function is available).  @jit\ndef sum_array(inp):\n    I, J = inp.shape\n\n    mysum = 0\n    for i in range(I):\n        for j in range(J):\n            mysum += inp[i, j]\n\n    return mysum  sum_array(arr)  45102.033677231  So again, we can see that we have the same result.  That's good.  And timing?  %timeit sum_array(arr)  10000 loops, best of 3: 73.8 \u00b5s per loop  As expected, more or less identical to the first  jit  example.",
            "title": "(more commonly) As a decorator"
        },
        {
            "location": "/numba/2/#how-does-this-compare-to-numpy",
            "text": "NumPy, of course, has built in methods for summing arrays, how does Numba stack up against those?  %timeit arr.sum()  10000 loops, best of 3: 33.9 \u00b5s per loop  Right. Remember, NumPy has been hand-tuned over many years to be very, very good at what it does. For simple operations, Numba is not going to outperform it, but when things get more complex Numba can save the day.   Also, take a moment to appreciate that our  jit ted code, which was compiled on-the-fly is offering performance in the same order of magnitude as NumPy.  That's pretty incredible.",
            "title": "How does this compare to NumPy?"
        },
        {
            "location": "/numba/2/#when-does-numba-compile-things",
            "text": "numba  is a just-in-time (hence,  jit ) compiler.  The very first time you run a  numba  compiled function, there will be a little bit of overhead for the compilation step to take place.  In practice, this is usually not noticeable. You may get a message from  timeit  that one \"run\" was much slower than most; this is due to the compilation overhead.",
            "title": "When does numba compile things?"
        },
        {
            "location": "/numba/2/#your-turn",
            "text": "",
            "title": "Your turn!"
        },
        {
            "location": "/numba/3/",
            "text": "Is this just magic?  What is Numba doing to make code run quickly?\n\n\nWhen you add the \njit\n decorator (or function call), Numba examines the code in the function and then tries to compile it using the LLVM compiler. LLVM takes Numba's translation of the Python code and compiles it into something like assembly code, which is a set of very low-level and very \nfast\n instructions. \n\n\nLet's create a small, simple example function to poke around in:\n\n\nfrom numba import jit\n\n\n\n\n@jit\ndef add(a, b):\n    return a + b\n\n\n\n\nadd(1, 1)\n\n\n\n\n2\n\n\n\nNow that we've run \nadd\n once, it is now \ncompiled\n and we can check out what's happened behind the scenes.  Use the \ninspect_types\n method to see how Numba translated the function.\n\n\nadd.inspect_types()\n\n\n\n\nadd (int64, int64)\n--------------------------------------------------------------------------------\n# File: <ipython-input-2-1c683d2d00ee>\n# --- LINE 1 --- \n# label 0\n#   del b\n#   del a\n#   del $0.3\n\n@jit\n\n# --- LINE 2 ---\n\ndef add(a, b):\n\n    # --- LINE 3 --- \n    #   a = arg(0, name=a)  :: int64\n    #   b = arg(1, name=b)  :: int64\n    #   $0.3 = a + b  :: int64\n    #   $0.4 = cast(value=$0.3)  :: int64\n    #   return $0.4\n\n    return a + b\n\n\n================================================================================\n\n\n\nThat is a bit more complicated than our original line -- and in fact, there's a bunch of even more complicated stuff going on behind the scenes, but we won't go into that right now. For now, just recognize that Numba is examining the code we wrote, then translating it into a more complex representation that can be efficiently compiled into a super-fast version.\n\n\nHowever...\n\n\nThis translation business is hard and Numba isn't perfect. If it encounters something that it doesn't understand, then it will still \nwork\n, but it will operate in what is called \"Object Mode\". This is fine, except \"Object Mode\" can be really, \nreally\n slow.  \n\n\nSo what can we do to avoid object mode?\n\n\nWell, first, there's a list of supported features that Numba understands that you can browse at your leisure (but do this later): http://numba.pydata.org/numba-doc/latest/reference/pysupported.html\n\n\nForcing \nnopython\n mode\n\n\nThe opposite of the slow \"object mode\" is called \nnopython\n mode.  That's a kind of confusing name, but it is what it is. The important thing to remember is that \nnopython\n mode is when Numba is \nfast\n, so that's what we want. \n\n\nBut how do we know what \"mode\" Numba is using?  \n\n\nThat's a good question. We don't always know, and we can't know ahead of time, but we do have one helper to look out for us. \n\n\nIf we specify \nnopython=True\n, then Numba will throw an exception and \nfail\n to compile when it can't make a function work in \nnopython\n mode. Then we can try to rewrite that function until it \ncan\n compile.\n\n\nHere's a quick example.  First import \nnumpy\n and the \nlinalg\n module from \nscipy\n.\n\n\nimport numpy\nfrom scipy import linalg\n\n\n\n\nDefine a random square array:\n\n\na = numpy.random.random((5, 5))\n\n\n\n\nNow write a function to pass that array to the \nlinalq\n QR decomposition method:\n\n\ndef qr_decomposition(a):\n    return linalg.decomp_qr.qr(a)\n\n\n\n\nNow let's try it out:\n\n\nqr_decomposition(a)\n\n\n\n\n(array([[-0.61251218,  0.3080602 ,  0.61705293, -0.29224924, -0.25251496],\n        [-0.51533223, -0.15379307, -0.29646637, -0.32818401,  0.71776273],\n        [-0.55845413, -0.50433313, -0.22256529,  0.49326552, -0.37540764],\n        [-0.10076405,  0.33415965, -0.67259288, -0.44155593, -0.48044886],\n        [-0.19296921,  0.71793594, -0.1715718 ,  0.60712694,  0.22201543]]),\n array([[-1.58468704, -1.26245499, -1.01284515, -1.29478127, -0.61076935],\n        [ 0.        ,  0.62312427, -0.04924057,  0.17049891,  0.09324721],\n        [ 0.        ,  0.        , -0.77693122, -0.43097318, -0.72632735],\n        [ 0.        ,  0.        ,  0.        , -0.46093787, -0.15692113],\n        [ 0.        ,  0.        ,  0.        ,  0.        , -0.05197661]]))\n\n\n\nIt works!  Ok, now let's try to \njit\n it:\n\n\nqr_jit = jit()(qr_decomposition)\n\n\n\n\nqr_jit(a)\n\n\n\n\n(array([[-0.61251218,  0.3080602 ,  0.61705293, -0.29224924, -0.25251496],\n        [-0.51533223, -0.15379307, -0.29646637, -0.32818401,  0.71776273],\n        [-0.55845413, -0.50433313, -0.22256529,  0.49326552, -0.37540764],\n        [-0.10076405,  0.33415965, -0.67259288, -0.44155593, -0.48044886],\n        [-0.19296921,  0.71793594, -0.1715718 ,  0.60712694,  0.22201543]]),\n array([[-1.58468704, -1.26245499, -1.01284515, -1.29478127, -0.61076935],\n        [ 0.        ,  0.62312427, -0.04924057,  0.17049891,  0.09324721],\n        [ 0.        ,  0.        , -0.77693122, -0.43097318, -0.72632735],\n        [ 0.        ,  0.        ,  0.        , -0.46093787, -0.15692113],\n        [ 0.        ,  0.        ,  0.        ,  0.        , -0.05197661]]))\n\n\n\nIt works! Or did it? What if try to add the \nnopython=True\n flag?  \n\n\n(Also, remember how we talked about those super weird second set of parentheses?  Here's where they come in)\n\n\nqr_jit = jit(nopython=True)(qr_decomposition)\n\n\n\n\nPrepare for a very long error message...\n\n\nqr_jit(a)\n\n\n\n\n---------------------------------------------------------------------------\n\nUntypedAttributeError                     Traceback (most recent call last)\n\n<ipython-input-12-b8e9f2ef8dc2> in <module>()\n----> 1 qr_jit(a)\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws)\n    307                                 for i, err in failed_args))\n    308                 e.patch_message(msg)\n--> 309             raise e\n    310 \n    311     def inspect_llvm(self, signature=None):\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws)\n    284                 argtypes.append(self.typeof_pyval(a))\n    285         try:\n--> 286             return self.compile(tuple(argtypes))\n    287         except errors.TypingError as e:\n    288             # Intercept typing error that may be due to an argument\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/dispatcher.py in compile(self, sig)\n    530 \n    531             self._cache_misses[sig] += 1\n--> 532             cres = self._compiler.compile(args, return_type)\n    533             self.add_overload(cres)\n    534             self._cache.save_overload(sig, cres)\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/dispatcher.py in compile(self, args, return_type)\n     79                                       impl,\n     80                                       args=args, return_type=return_type,\n---> 81                                       flags=flags, locals=self.locals)\n     82         # Check typing error if object mode is used\n     83         if cres.typing_error is not None and not flags.enable_pyobject:\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library)\n    682     pipeline = Pipeline(typingctx, targetctx, library,\n    683                         args, return_type, flags, locals)\n--> 684     return pipeline.compile_extra(func)\n    685 \n    686\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/compiler.py in compile_extra(self, func)\n    346         self.lifted = ()\n    347         self.lifted_from = None\n--> 348         return self._compile_bytecode()\n    349 \n    350     def compile_ir(self, func_ir, lifted=(), lifted_from=None):\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/compiler.py in _compile_bytecode(self)\n    647         \"\"\"\n    648         assert self.func_ir is None\n--> 649         return self._compile_core()\n    650 \n    651     def _compile_ir(self):\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/compiler.py in _compile_core(self)\n    634 \n    635         pm.finalize()\n--> 636         res = pm.run(self.status)\n    637         if res is not None:\n    638             # Early pipeline completion\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/compiler.py in run(self, status)\n    233                     # No more fallback pipelines?\n    234                     if is_final_pipeline:\n--> 235                         raise patched_exception\n    236                     # Go to next fallback pipeline\n    237                     else:\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/compiler.py in run(self, status)\n    225                 try:\n    226                     event(stage_name)\n--> 227                     stage()\n    228                 except _EarlyPipelineCompletion as e:\n    229                     return e.result\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/compiler.py in stage_nopython_frontend(self)\n    434                 self.args,\n    435                 self.return_type,\n--> 436                 self.locals)\n    437 \n    438         with self.fallback_context('Function \"%s\" has invalid return type'\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/compiler.py in type_inference_stage(typingctx, interp, args, return_type, locals)\n    783 \n    784         infer.build_constraint()\n--> 785         infer.propagate()\n    786         typemap, restype, calltypes = infer.unify()\n    787\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/typeinfer.py in propagate(self, raise_errors)\n    759         if errors:\n    760             if raise_errors:\n--> 761                 raise errors[0]\n    762             else:\n    763                 return errors\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/typeinfer.py in propagate(self, typeinfer)\n    126                                                    lineno=loc.line):\n    127                 try:\n--> 128                     constraint(typeinfer)\n    129                 except TypingError as e:\n    130                     errors.append(e)\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/typeinfer.py in __call__(self, typeinfer)\n    447                 attrty = typeinfer.context.resolve_getattr(ty, self.attr)\n    448                 if attrty is None:\n--> 449                     raise UntypedAttributeError(ty, self.attr, loc=self.inst.loc)\n    450                 else:\n    451                     typeinfer.add_type(self.target, attrty, loc=self.loc)\n\n\nUntypedAttributeError: Failed at nopython (nopython frontend)\nUnknown attribute 'qr' of type Module(<module 'scipy.linalg.decomp_qr' from '/home/gil/anaconda/lib/python3.5/site-packages/scipy/linalg/decomp_qr.py'>)\nFile \"<ipython-input-7-aa69e1f11031>\", line 2\n[1] During: typing of get attribute at <ipython-input-7-aa69e1f11031> (2)\n\n\n\nAck\n\n\nYes, that's a very long and intimidating looking error message, but just focus on the last few lines, specifically \"Failed at nopython\". That's Numba telling us that it has no idea what \nscipy.linalg.decomp_qr\n is, so it can't try to accelerate it. \n\n\nIt worked the first time because it was in \"object\" mode but we just asked Numba to \nforce\n \nnopython\n mode and it tried (and failed). \n\n\nnopython\n mode is so useful, that people got tired of typing it out all of the time, so there's a shortcut!\n\n\nfrom numba import njit\n\n\n\n\nnjit\n is exactly the same as \njit\n but it always forces \nnopython=True\n.  And like \njit\n, you can use it in a function call, or as a decorator.  Let's try it out on the simple \nadd\n function we started out with:\n\n\ndef add(a, b):\n    return a + b\n\n\n\n\nFunction call:\n\n\nadd_jit = njit(add)  # no extra parentheses needed with `njit`\n\n\n\n\nadd_jit(3, 4)\n\n\n\n\n7\n\n\n\nDecorator:\n\n\n@njit\ndef add(a, b):\n    return a + b\n\n\n\n\nadd(4, 6)\n\n\n\n\n10\n\n\n\nAnd that's it! \n\n\nUnless you have very specific requirements, we recommend always using \nnjit\n over \njit\n, so you can guarantee that you're taking advantage of all of Numba's speedup power.",
            "title": "Numba Internals"
        },
        {
            "location": "/numba/3/#is-this-just-magic-what-is-numba-doing-to-make-code-run-quickly",
            "text": "When you add the  jit  decorator (or function call), Numba examines the code in the function and then tries to compile it using the LLVM compiler. LLVM takes Numba's translation of the Python code and compiles it into something like assembly code, which is a set of very low-level and very  fast  instructions.   Let's create a small, simple example function to poke around in:  from numba import jit  @jit\ndef add(a, b):\n    return a + b  add(1, 1)  2  Now that we've run  add  once, it is now  compiled  and we can check out what's happened behind the scenes.  Use the  inspect_types  method to see how Numba translated the function.  add.inspect_types()  add (int64, int64)\n--------------------------------------------------------------------------------\n# File: <ipython-input-2-1c683d2d00ee>\n# --- LINE 1 --- \n# label 0\n#   del b\n#   del a\n#   del $0.3\n\n@jit\n\n# --- LINE 2 ---\n\ndef add(a, b):\n\n    # --- LINE 3 --- \n    #   a = arg(0, name=a)  :: int64\n    #   b = arg(1, name=b)  :: int64\n    #   $0.3 = a + b  :: int64\n    #   $0.4 = cast(value=$0.3)  :: int64\n    #   return $0.4\n\n    return a + b\n\n\n================================================================================  That is a bit more complicated than our original line -- and in fact, there's a bunch of even more complicated stuff going on behind the scenes, but we won't go into that right now. For now, just recognize that Numba is examining the code we wrote, then translating it into a more complex representation that can be efficiently compiled into a super-fast version.",
            "title": "Is this just magic?  What is Numba doing to make code run quickly?"
        },
        {
            "location": "/numba/3/#however",
            "text": "This translation business is hard and Numba isn't perfect. If it encounters something that it doesn't understand, then it will still  work , but it will operate in what is called \"Object Mode\". This is fine, except \"Object Mode\" can be really,  really  slow.    So what can we do to avoid object mode?  Well, first, there's a list of supported features that Numba understands that you can browse at your leisure (but do this later): http://numba.pydata.org/numba-doc/latest/reference/pysupported.html",
            "title": "However..."
        },
        {
            "location": "/numba/3/#forcing-nopython-mode",
            "text": "The opposite of the slow \"object mode\" is called  nopython  mode.  That's a kind of confusing name, but it is what it is. The important thing to remember is that  nopython  mode is when Numba is  fast , so that's what we want.   But how do we know what \"mode\" Numba is using?    That's a good question. We don't always know, and we can't know ahead of time, but we do have one helper to look out for us.   If we specify  nopython=True , then Numba will throw an exception and  fail  to compile when it can't make a function work in  nopython  mode. Then we can try to rewrite that function until it  can  compile.  Here's a quick example.  First import  numpy  and the  linalg  module from  scipy .  import numpy\nfrom scipy import linalg  Define a random square array:  a = numpy.random.random((5, 5))  Now write a function to pass that array to the  linalq  QR decomposition method:  def qr_decomposition(a):\n    return linalg.decomp_qr.qr(a)  Now let's try it out:  qr_decomposition(a)  (array([[-0.61251218,  0.3080602 ,  0.61705293, -0.29224924, -0.25251496],\n        [-0.51533223, -0.15379307, -0.29646637, -0.32818401,  0.71776273],\n        [-0.55845413, -0.50433313, -0.22256529,  0.49326552, -0.37540764],\n        [-0.10076405,  0.33415965, -0.67259288, -0.44155593, -0.48044886],\n        [-0.19296921,  0.71793594, -0.1715718 ,  0.60712694,  0.22201543]]),\n array([[-1.58468704, -1.26245499, -1.01284515, -1.29478127, -0.61076935],\n        [ 0.        ,  0.62312427, -0.04924057,  0.17049891,  0.09324721],\n        [ 0.        ,  0.        , -0.77693122, -0.43097318, -0.72632735],\n        [ 0.        ,  0.        ,  0.        , -0.46093787, -0.15692113],\n        [ 0.        ,  0.        ,  0.        ,  0.        , -0.05197661]]))  It works!  Ok, now let's try to  jit  it:  qr_jit = jit()(qr_decomposition)  qr_jit(a)  (array([[-0.61251218,  0.3080602 ,  0.61705293, -0.29224924, -0.25251496],\n        [-0.51533223, -0.15379307, -0.29646637, -0.32818401,  0.71776273],\n        [-0.55845413, -0.50433313, -0.22256529,  0.49326552, -0.37540764],\n        [-0.10076405,  0.33415965, -0.67259288, -0.44155593, -0.48044886],\n        [-0.19296921,  0.71793594, -0.1715718 ,  0.60712694,  0.22201543]]),\n array([[-1.58468704, -1.26245499, -1.01284515, -1.29478127, -0.61076935],\n        [ 0.        ,  0.62312427, -0.04924057,  0.17049891,  0.09324721],\n        [ 0.        ,  0.        , -0.77693122, -0.43097318, -0.72632735],\n        [ 0.        ,  0.        ,  0.        , -0.46093787, -0.15692113],\n        [ 0.        ,  0.        ,  0.        ,  0.        , -0.05197661]]))  It works! Or did it? What if try to add the  nopython=True  flag?    (Also, remember how we talked about those super weird second set of parentheses?  Here's where they come in)  qr_jit = jit(nopython=True)(qr_decomposition)  Prepare for a very long error message...  qr_jit(a)  ---------------------------------------------------------------------------\n\nUntypedAttributeError                     Traceback (most recent call last)\n\n<ipython-input-12-b8e9f2ef8dc2> in <module>()\n----> 1 qr_jit(a)\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws)\n    307                                 for i, err in failed_args))\n    308                 e.patch_message(msg)\n--> 309             raise e\n    310 \n    311     def inspect_llvm(self, signature=None):\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws)\n    284                 argtypes.append(self.typeof_pyval(a))\n    285         try:\n--> 286             return self.compile(tuple(argtypes))\n    287         except errors.TypingError as e:\n    288             # Intercept typing error that may be due to an argument\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/dispatcher.py in compile(self, sig)\n    530 \n    531             self._cache_misses[sig] += 1\n--> 532             cres = self._compiler.compile(args, return_type)\n    533             self.add_overload(cres)\n    534             self._cache.save_overload(sig, cres)\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/dispatcher.py in compile(self, args, return_type)\n     79                                       impl,\n     80                                       args=args, return_type=return_type,\n---> 81                                       flags=flags, locals=self.locals)\n     82         # Check typing error if object mode is used\n     83         if cres.typing_error is not None and not flags.enable_pyobject:\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library)\n    682     pipeline = Pipeline(typingctx, targetctx, library,\n    683                         args, return_type, flags, locals)\n--> 684     return pipeline.compile_extra(func)\n    685 \n    686\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/compiler.py in compile_extra(self, func)\n    346         self.lifted = ()\n    347         self.lifted_from = None\n--> 348         return self._compile_bytecode()\n    349 \n    350     def compile_ir(self, func_ir, lifted=(), lifted_from=None):\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/compiler.py in _compile_bytecode(self)\n    647         \"\"\"\n    648         assert self.func_ir is None\n--> 649         return self._compile_core()\n    650 \n    651     def _compile_ir(self):\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/compiler.py in _compile_core(self)\n    634 \n    635         pm.finalize()\n--> 636         res = pm.run(self.status)\n    637         if res is not None:\n    638             # Early pipeline completion\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/compiler.py in run(self, status)\n    233                     # No more fallback pipelines?\n    234                     if is_final_pipeline:\n--> 235                         raise patched_exception\n    236                     # Go to next fallback pipeline\n    237                     else:\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/compiler.py in run(self, status)\n    225                 try:\n    226                     event(stage_name)\n--> 227                     stage()\n    228                 except _EarlyPipelineCompletion as e:\n    229                     return e.result\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/compiler.py in stage_nopython_frontend(self)\n    434                 self.args,\n    435                 self.return_type,\n--> 436                 self.locals)\n    437 \n    438         with self.fallback_context('Function \"%s\" has invalid return type'\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/compiler.py in type_inference_stage(typingctx, interp, args, return_type, locals)\n    783 \n    784         infer.build_constraint()\n--> 785         infer.propagate()\n    786         typemap, restype, calltypes = infer.unify()\n    787\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/typeinfer.py in propagate(self, raise_errors)\n    759         if errors:\n    760             if raise_errors:\n--> 761                 raise errors[0]\n    762             else:\n    763                 return errors\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/typeinfer.py in propagate(self, typeinfer)\n    126                                                    lineno=loc.line):\n    127                 try:\n--> 128                     constraint(typeinfer)\n    129                 except TypingError as e:\n    130                     errors.append(e)\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/numba/typeinfer.py in __call__(self, typeinfer)\n    447                 attrty = typeinfer.context.resolve_getattr(ty, self.attr)\n    448                 if attrty is None:\n--> 449                     raise UntypedAttributeError(ty, self.attr, loc=self.inst.loc)\n    450                 else:\n    451                     typeinfer.add_type(self.target, attrty, loc=self.loc)\n\n\nUntypedAttributeError: Failed at nopython (nopython frontend)\nUnknown attribute 'qr' of type Module(<module 'scipy.linalg.decomp_qr' from '/home/gil/anaconda/lib/python3.5/site-packages/scipy/linalg/decomp_qr.py'>)\nFile \"<ipython-input-7-aa69e1f11031>\", line 2\n[1] During: typing of get attribute at <ipython-input-7-aa69e1f11031> (2)",
            "title": "Forcing nopython mode"
        },
        {
            "location": "/numba/3/#ack",
            "text": "Yes, that's a very long and intimidating looking error message, but just focus on the last few lines, specifically \"Failed at nopython\". That's Numba telling us that it has no idea what  scipy.linalg.decomp_qr  is, so it can't try to accelerate it.   It worked the first time because it was in \"object\" mode but we just asked Numba to  force   nopython  mode and it tried (and failed).   nopython  mode is so useful, that people got tired of typing it out all of the time, so there's a shortcut!  from numba import njit  njit  is exactly the same as  jit  but it always forces  nopython=True .  And like  jit , you can use it in a function call, or as a decorator.  Let's try it out on the simple  add  function we started out with:  def add(a, b):\n    return a + b",
            "title": "Ack"
        },
        {
            "location": "/numba/3/#function-call",
            "text": "add_jit = njit(add)  # no extra parentheses needed with `njit`  add_jit(3, 4)  7",
            "title": "Function call:"
        },
        {
            "location": "/numba/3/#decorator",
            "text": "@njit\ndef add(a, b):\n    return a + b  add(4, 6)  10  And that's it!   Unless you have very specific requirements, we recommend always using  njit  over  jit , so you can guarantee that you're taking advantage of all of Numba's speedup power.",
            "title": "Decorator:"
        },
        {
            "location": "/numba/4/",
            "text": "Application - Pressure Poisson Equation\n\n\nThe momentum equation for the velocity field \n\\vec{u}\n in a fluid is\n\n\n\n\n\\begin{equation}\n\\frac{\\partial \\vec{u}}{\\partial t}+(\\vec{u}\\cdot\\nabla)\\vec{u}=-\\frac{1}{\\rho}\\nabla p + \\nu \\nabla^2\\vec{u}\n\\end{equation}\n\n\n\n\nwhere \np\n is the pressure, \n\\nu\n is the fluid viscosity and \n\\rho\n is the fluid density. With three velocity components, plus the pressure, we have four unknowns but only three equations. For compressible fluids, we have an equation of state to complete the system. In the incompressible case, we don't have an equation of state and we need an additional constraint from somewhere else. \n\n\nThis is what we do: take the divergence of the momentum equation, apply the incompressibility constraint to cancel some terms, and get an equation for the pressure. It's a pretty cool trick.\n\n\nConservation of mass for an incompressible fluid requires that the divergence of \n\\vec{u}\n must be zero:\n\n\n\n\n\\nabla \\cdot \\vec{u} = 0\n\n\n\n\nWriting out the momentum equation in \nx\n and \ny\n components (for two-dimensional flow), we get\n\n\n\n\n\\frac{\\partial u}{\\partial t}+u\\frac{\\partial u}{\\partial x}+v\\frac{\\partial u}{\\partial y} = -\\frac{1}{\\rho}\\frac{\\partial p}{\\partial x}+\\nu \\left(\\frac{\\partial^2 u}{\\partial x^2}+\\frac{\\partial^2 u}{\\partial y^2} \\right) \n\n\n\n\n\n\n\\frac{\\partial v}{\\partial t}+u\\frac{\\partial v}{\\partial x}+v\\frac{\\partial v}{\\partial y} = -\\frac{1}{\\rho}\\frac{\\partial p}{\\partial y}+\\nu\\left(\\frac{\\partial^2 v}{\\partial x^2}+\\frac{\\partial^2 v}{\\partial y^2}\\right) \n\n\n\n\nWe take the divergence of the momentum equation and then apply the incompressibility constraint.  After some wrangling and cancellations, this leaves us with the pressure Poisson equation:\n\n\n\n\n\\frac{\\partial^2 p}{\\partial x^2}+\\frac{\\partial^2 p}{\\partial y^2} = -\\rho\\left(\\frac{\\partial u}{\\partial x}\\frac{\\partial u}{\\partial x}+2\\frac{\\partial u}{\\partial y}\\frac{\\partial v}{\\partial x}+\\frac{\\partial v}{\\partial y}\\frac{\\partial v}{\\partial y} \\right)\n\n\n\n\nWhich is an equation of the form\n\n\n\n\n\\frac{\\partial ^2 p}{\\partial x^2} + \\frac{\\partial ^2 p}{\\partial y^2} = b\n\n\n\n\nImagine we discretize a domain using a uniform mesh of points in each spatial direction, as in the figure below:\n\n\n\n\nThen the left-hand side of the Poisson equation, i.e., the Laplacian differential operator applied to \np\n, is discretized using 2nd-order central differences as follows\n\n\n\n\n\\frac{p^n_{i+1, j} - 2p^n_{i,j} + p^n_{i - 1, j}}{\\Delta x ^2} + \\frac{p^n_{i, j+1} - 2p^n_{i,j} + p^n_{i, j-1}}{\\Delta y ^2}\n\n\n\n\nwhere subscripts \ni,j\n denote the spatial location on a Cartesian coordinate system and superscripts \nn\n denote a point in time.\n\n\nWe apply an appropriate finite-difference discretization to the momentum equation (forward-time, backward-space for the 1st-order terms) and also assume a uniform mesh, so \n\\Delta x = \\Delta y\n.\n\n\nUsing this discretized form in the Poisson equation, we will leave only the \np_{i,j}\n terms in the left-hand side, and move the other terms to the right. Then we say that we can update all the values of \np_{i,j}\n using the values at the neighboring points for both \np\n and \nu, v\n. This update, repeated many times, happens to converge to the solution of Poisson's equation.\n\n\nSolution procedure\n\n\nInitial velocity field\n\n\nWe start with a velocity field in \nu\n and \nv\n at some timestep \nn\n. \n\n\nCalculate pressure\n\n\nThen, we iteratively solve the Poisson equation for pressure, as described above. Starting with an initial guess, the values \np_{i,j}\n are updated using the neighboring values of \np\n, \nu\n and \nv\n at \n(i+1,j)\n and \n(i,j+1)\n. The updates can be written as follows, where the \nk\n superscript denotes an iteration in 'pseudo-time':\n\n\n\n\n\\begin{align}\np_{i,j}^{k+1} &= \\frac{1}{4}\\left(p_{i+1,j}^{k}+p_{i-1,j}^{k}+p_{i,j+1}^{k}+p_{i,j-1}^{k}\\right) \\\\\n&-\\frac{\\rho \\Delta x}{16} \\left( \\frac{2}{\\Delta t} \\left(u_{i+1,j} - u_{i-1,j} + v_{i,j+1} - v_{i,j-1}\\right) \\right . \\\\\n&-\\frac{2}{\\Delta x}\\left(u_{i,j+1} - u_{i,j-1} \\right) \\left(v_{i+1,j} - v_{i-1,j} \\right) \\\\\n&- \\left . \\frac{\\left(u_{i+1,j} - u_{i-1,j} \\right)^2}{\\Delta x} \n- \\frac{ \\left(v_{i,j+1} - v_{i,j-1} \\right)^2 }{\\Delta x} \\right) \\\\\n\\end{align}\n\n\n\n\nIn other words, we repeatedly apply the Poisson equation until the pressure reaches a quasi-steady state.\n\n\nUpdate the velocity\n\n\nOnce the pressure field reaches its quasi-steady state via the Poisson equation, we use that field for the current time step, \np^n\n, to solve for the velocity components \nu\n and \nv\n at the next timestep, \nn+1\n.\n\n\nThe momentum equation in the \nu\n direction:\n\n\n\n\n\\begin{align}\nu_{i,j}^{n+1} = u_{i,j}^{n} &- \\frac{\\Delta t}{\\Delta x} \\left( u_{i,j}^{n}(u_{i,j}^{n}-u_{i-1,j}^{n})\n+ v_{i,j}^{n} (u_{i,j}^{n}-u_{i,j-1}^{n}) + \\frac{1}{2 \\rho}(p_{i+1,j}^{n}-p_{i-1,j}^{n}) \\right) \\\\\n&+\\frac{\\nu \\Delta t}{\\Delta x^2}\\left(u_{i+1,j}^{n} + u_{i-1,j}^{n} + u_{i,j+1}^{n} + u_{i,j-1}^{n} -4u_{i,j}^{n}\\right)\n\\end{align}\n\n\n\n\nThe momentum equation in the \nv\n direction:\n\n\n\n\n\\begin{align}\nv_{i,j}^{n+1} = v_{i,j}^{n} &- \\frac{\\Delta t}{\\Delta x} \\left( u_{i,j}^{n}(v_{i,j}^{n}-v_{i-1,j}^{n})\n+ v_{i,j}^{n} (v_{i,j}^{n}-v_{i,j-1}^{n}) + \\frac{1}{2 \\rho}(p_{i,j+1}^{n}-p_{i,j-1}^{n}) \\right) \\\\\n&+\\frac{\\nu \\Delta t}{\\Delta x^2}\\left(v_{i+1,j}^{n} + v_{i-1,j}^{n} + v_{i,j+1}^{n} + v_{i,j-1}^{n} -4v_{i,j}^{n}\\right)\n\\end{align}\n\n\n\n\nThen, rinse and repeat.\n\n\nWhat we left out\n\n\nThere are various subtleties that we left out here, to get quickly to the equations we need in the code. First, there are some variations on the form of the pressure Poisson equation, depending on what terms involving \n\\nabla\\cdot\\vec{u}\n one chooses to cancel (this has caused long arguments in the literature!). Second, we say nothing about the boundary conditions, which can cause some trouble (and more arguments!). And third, we show only the simplest iterative method for solving the Poisson equation, which also happens to be the slowest to converge. This is just meant to be a pedagogical example and discussion of these subtleties would be part of a full-fledged CFD course.",
            "title": "CFD Intro"
        },
        {
            "location": "/numba/4/#application-pressure-poisson-equation",
            "text": "The momentum equation for the velocity field  \\vec{u}  in a fluid is   \\begin{equation}\n\\frac{\\partial \\vec{u}}{\\partial t}+(\\vec{u}\\cdot\\nabla)\\vec{u}=-\\frac{1}{\\rho}\\nabla p + \\nu \\nabla^2\\vec{u}\n\\end{equation}   where  p  is the pressure,  \\nu  is the fluid viscosity and  \\rho  is the fluid density. With three velocity components, plus the pressure, we have four unknowns but only three equations. For compressible fluids, we have an equation of state to complete the system. In the incompressible case, we don't have an equation of state and we need an additional constraint from somewhere else.   This is what we do: take the divergence of the momentum equation, apply the incompressibility constraint to cancel some terms, and get an equation for the pressure. It's a pretty cool trick.  Conservation of mass for an incompressible fluid requires that the divergence of  \\vec{u}  must be zero:   \\nabla \\cdot \\vec{u} = 0   Writing out the momentum equation in  x  and  y  components (for two-dimensional flow), we get   \\frac{\\partial u}{\\partial t}+u\\frac{\\partial u}{\\partial x}+v\\frac{\\partial u}{\\partial y} = -\\frac{1}{\\rho}\\frac{\\partial p}{\\partial x}+\\nu \\left(\\frac{\\partial^2 u}{\\partial x^2}+\\frac{\\partial^2 u}{\\partial y^2} \\right)     \\frac{\\partial v}{\\partial t}+u\\frac{\\partial v}{\\partial x}+v\\frac{\\partial v}{\\partial y} = -\\frac{1}{\\rho}\\frac{\\partial p}{\\partial y}+\\nu\\left(\\frac{\\partial^2 v}{\\partial x^2}+\\frac{\\partial^2 v}{\\partial y^2}\\right)    We take the divergence of the momentum equation and then apply the incompressibility constraint.  After some wrangling and cancellations, this leaves us with the pressure Poisson equation:   \\frac{\\partial^2 p}{\\partial x^2}+\\frac{\\partial^2 p}{\\partial y^2} = -\\rho\\left(\\frac{\\partial u}{\\partial x}\\frac{\\partial u}{\\partial x}+2\\frac{\\partial u}{\\partial y}\\frac{\\partial v}{\\partial x}+\\frac{\\partial v}{\\partial y}\\frac{\\partial v}{\\partial y} \\right)   Which is an equation of the form   \\frac{\\partial ^2 p}{\\partial x^2} + \\frac{\\partial ^2 p}{\\partial y^2} = b   Imagine we discretize a domain using a uniform mesh of points in each spatial direction, as in the figure below:   Then the left-hand side of the Poisson equation, i.e., the Laplacian differential operator applied to  p , is discretized using 2nd-order central differences as follows   \\frac{p^n_{i+1, j} - 2p^n_{i,j} + p^n_{i - 1, j}}{\\Delta x ^2} + \\frac{p^n_{i, j+1} - 2p^n_{i,j} + p^n_{i, j-1}}{\\Delta y ^2}   where subscripts  i,j  denote the spatial location on a Cartesian coordinate system and superscripts  n  denote a point in time.  We apply an appropriate finite-difference discretization to the momentum equation (forward-time, backward-space for the 1st-order terms) and also assume a uniform mesh, so  \\Delta x = \\Delta y .  Using this discretized form in the Poisson equation, we will leave only the  p_{i,j}  terms in the left-hand side, and move the other terms to the right. Then we say that we can update all the values of  p_{i,j}  using the values at the neighboring points for both  p  and  u, v . This update, repeated many times, happens to converge to the solution of Poisson's equation.",
            "title": "Application - Pressure Poisson Equation"
        },
        {
            "location": "/numba/4/#solution-procedure",
            "text": "",
            "title": "Solution procedure"
        },
        {
            "location": "/numba/4/#initial-velocity-field",
            "text": "We start with a velocity field in  u  and  v  at some timestep  n .",
            "title": "Initial velocity field"
        },
        {
            "location": "/numba/4/#calculate-pressure",
            "text": "Then, we iteratively solve the Poisson equation for pressure, as described above. Starting with an initial guess, the values  p_{i,j}  are updated using the neighboring values of  p ,  u  and  v  at  (i+1,j)  and  (i,j+1) . The updates can be written as follows, where the  k  superscript denotes an iteration in 'pseudo-time':   \\begin{align}\np_{i,j}^{k+1} &= \\frac{1}{4}\\left(p_{i+1,j}^{k}+p_{i-1,j}^{k}+p_{i,j+1}^{k}+p_{i,j-1}^{k}\\right) \\\\\n&-\\frac{\\rho \\Delta x}{16} \\left( \\frac{2}{\\Delta t} \\left(u_{i+1,j} - u_{i-1,j} + v_{i,j+1} - v_{i,j-1}\\right) \\right . \\\\\n&-\\frac{2}{\\Delta x}\\left(u_{i,j+1} - u_{i,j-1} \\right) \\left(v_{i+1,j} - v_{i-1,j} \\right) \\\\\n&- \\left . \\frac{\\left(u_{i+1,j} - u_{i-1,j} \\right)^2}{\\Delta x} \n- \\frac{ \\left(v_{i,j+1} - v_{i,j-1} \\right)^2 }{\\Delta x} \\right) \\\\\n\\end{align}   In other words, we repeatedly apply the Poisson equation until the pressure reaches a quasi-steady state.",
            "title": "Calculate pressure"
        },
        {
            "location": "/numba/4/#update-the-velocity",
            "text": "Once the pressure field reaches its quasi-steady state via the Poisson equation, we use that field for the current time step,  p^n , to solve for the velocity components  u  and  v  at the next timestep,  n+1 .  The momentum equation in the  u  direction:   \\begin{align}\nu_{i,j}^{n+1} = u_{i,j}^{n} &- \\frac{\\Delta t}{\\Delta x} \\left( u_{i,j}^{n}(u_{i,j}^{n}-u_{i-1,j}^{n})\n+ v_{i,j}^{n} (u_{i,j}^{n}-u_{i,j-1}^{n}) + \\frac{1}{2 \\rho}(p_{i+1,j}^{n}-p_{i-1,j}^{n}) \\right) \\\\\n&+\\frac{\\nu \\Delta t}{\\Delta x^2}\\left(u_{i+1,j}^{n} + u_{i-1,j}^{n} + u_{i,j+1}^{n} + u_{i,j-1}^{n} -4u_{i,j}^{n}\\right)\n\\end{align}   The momentum equation in the  v  direction:   \\begin{align}\nv_{i,j}^{n+1} = v_{i,j}^{n} &- \\frac{\\Delta t}{\\Delta x} \\left( u_{i,j}^{n}(v_{i,j}^{n}-v_{i-1,j}^{n})\n+ v_{i,j}^{n} (v_{i,j}^{n}-v_{i,j-1}^{n}) + \\frac{1}{2 \\rho}(p_{i,j+1}^{n}-p_{i,j-1}^{n}) \\right) \\\\\n&+\\frac{\\nu \\Delta t}{\\Delta x^2}\\left(v_{i+1,j}^{n} + v_{i-1,j}^{n} + v_{i,j+1}^{n} + v_{i,j-1}^{n} -4v_{i,j}^{n}\\right)\n\\end{align}   Then, rinse and repeat.",
            "title": "Update the velocity"
        },
        {
            "location": "/numba/4/#what-we-left-out",
            "text": "There are various subtleties that we left out here, to get quickly to the equations we need in the code. First, there are some variations on the form of the pressure Poisson equation, depending on what terms involving  \\nabla\\cdot\\vec{u}  one chooses to cancel (this has caused long arguments in the literature!). Second, we say nothing about the boundary conditions, which can cause some trouble (and more arguments!). And third, we show only the simplest iterative method for solving the Poisson equation, which also happens to be the slowest to converge. This is just meant to be a pedagogical example and discussion of these subtleties would be part of a full-fledged CFD course.",
            "title": "What we left out"
        },
        {
            "location": "/numba/5/",
            "text": "Application: Cavity Flow\n\n\nOne of the most common validation cases in CFD is the lid-driven cavity flow.  We take a square cavity filled with a fluid and set the velocity of the lid to some constant value.  The flow within the cavity is driven by the lid, a spiral flow pattern develops and two distinctive pressure zones are visible in the upper corners against the lid.\n\n\n\n\nimport numpy\n\n\n\n\nThe Poisson equation is an elliptic PDE which almost always means using an iterative solver.  We're going to use the Jacobi method.  There are better ways, but that's beside the point.  \n\n\nHere's the pressure Poisson equation:\n\n\n\n\n\\begin{align}\np_{i,j}^{n+1} &= \\frac{1}{4}\\left(p_{i+1,j}^{n}+p_{i-1,j}^{n}+p_{i,j+1}^{n}+p_{i,j-1}^{n}\\right) \\\\\n&-\\frac{\\rho \\Delta x}{16} \\left( \\frac{2}{\\Delta t} \\left(u_{i+1,j} - u_{i-1,j} + v_{i,j+1} - v_{i,j-1}\\right) \\right . \\\\\n&-\\frac{2}{\\Delta x}\\left(u_{i,j+1} - u_{i,j-1} \\right) \\left(v_{i+1,j} - v_{i-1,j} \\right) \\\\\n&- \\left . \\frac{\\left(u_{i+1,j} - u_{i-1,j} \\right)^2}{\\Delta x} \n- \\frac{ \\left(v_{i,j+1} - v_{i,j-1} \\right)^2 }{\\Delta x} \\right) \\\\\n\\end{align}\n\n\n\n\nThat looks a little nasty, but we only care about the top line when we iterate, since the bottom three lines depend only on values that don't change when we're correcting the pressure field.  Because it doesn't change, we break it out into a separate function.\n\n\ndef velocity_term(b, rho, dt, u, v, dx):\n    b[1:-1, 1:-1] = (\n        rho * dx / 16 *\n        (2 / dt * (u[1:-1, 2:] -\n                    u[1:-1, :-2] +\n                    v[2:, 1:-1] -\n                    v[:-2, 1:-1]) -\n        2 / dx * (u[2:, 1:-1] - u[:-2, 1:-1]) *\n                 (v[1:-1, 2:] - v[1:-1, :-2]) -\n        (u[1:-1, 2:] - u[1:-1, :-2])**2 / dx -\n        (v[2:, 1:-1] - v[:-2, 1:-1])**2 / dx)\n                     )\n\n    return b\n\n\n\n\nNow, to calculate the pressure field, we pass in the original pressure field, the value \nb\n (which is the result of the \nvelocity_term\n function above) and a target value for difference between two iterates.  We repeatedly update the pressure field until the difference of the L2 norm between two successive iterations is less than that target value.\n\n\ndef pressure_poisson(p, b, l2_target):\n    iter_diff = l2_target + 1\n    n = 0\n    while iter_diff > l2_target and n <= 500:\n\n        pn = p.copy()\n        p[1:-1,1:-1] = (.25 * (pn[1:-1, 2:] +\n                               pn[1:-1, :-2] +\n                               pn[2:, 1:-1] +\n                               pn[:-2, 1:-1]) -\n                               b[1:-1, 1:-1])\n\n        p[:, 0] = p[:, 1]   #dp/dx = 0 at x = 0\n        p[:, -1] = p[:, -2] #dp/dx = 0 at x = 2\n        p[0, :] = p[1, :]   #dp/dy = 0 at y = 0\n        p[-1, :] = 0        #p = 0 at y = 2\n\n        if n % 10 == 0:\n            iter_diff = numpy.sqrt(numpy.sum((p - pn)**2)/numpy.sum(pn**2))\n\n        n += 1\n\n    return p\n\n\n\n\nIn the interests of brevity, we're only going to worry about the pressure Poisson solver.  The rest of the 2D Navier-Stokes solution is encapsulated in the function \ncavity_flow\n, which we've prepared ahead of time and saved in a helper file. If you want to dig deeper into how the \ncavity_flow\n function works, check out \"The 12 Steps to Navier-Stokes\" at \nCFD Python\n.\n\n\nFor now, though, we just need to import the function:\n\n\nfrom snippets.ns_helper import cavity_flow\n\n\n\n\nWe'll also load up \npickled\n initial conditions, so we can reliably compare final solutions.\n\n\nimport pickle\n\n\n\n\ndef run_cavity():\n    nx = 41\n    ny = 41\n    with open('IC.pickle', 'rb') as f:\n        u, v, p, b = pickle.load(f)\n\n    dx = 2 / (nx - 1)\n    dt = .005\n    nt = 1000\n\n    u, v, p = cavity_flow(u, v, p, nt, dt, dx, \n                         velocity_term, \n                         pressure_poisson, \n                         rtol=1e-4)\n\n    return u, v, p\n\n\n\n\nSo what does this all do?  Let's check it out.\n\n\nu, v, p = run_cavity()\n\n\n\n\n%matplotlib inline\nfrom snippets.ns_helper import quiver_plot\n\n\n\n\nquiver_plot(u, v, p)\n\n\n\n\n\n\nSave NumPy answers for comparison\n\n\nThis will create a binary file, saved to disk as \nnumpy_ans.pickle\n that contains the values of \nu\n, \nv\n, and \np\n. \n\n\nwith open('numpy_ans.pickle', 'wb') as f:\n    pickle.dump((u, v, p), f)\n\n\n\n\nLet's profile the \ncavity_flow\n function and see if there's a specific place that's really hurting our performance.\n\n\n%timeit run_cavity()\n\n\n\n\n1 loop, best of 3: 462 ms per loop\n\n\n\nRemember that we need to load the \nline_profiler\n extension before we can use the \nlprun\n magic.\n\n\n%load_ext line_profiler\n\n\n\n\n%lprun -f cavity_flow run_cavity()\n\n\n\n\nWhere is the bottleneck?\n\n\nClearly the PPE is the problem here, so let's use \nnumba\n to rewrite it.  \n\n\nExercise: Speed up the PPE\n\n\nfrom numba import jit\n\n\n\n\n%load snippets/ppe_numba.py\n\n\n\n\nSince we have redefined the \npressure_poisson\n function, we can run \nrun_cavity\n again and we'll be using the new and improved \njit\nted PPE. \n\n\nWe don't want to overwrite our results, so we can choose different variable names to store the final velocity and pressure fields.\n\n\nu_numba, v_numba, p_numba = run_cavity()\n\n\n\n\nWe use \nnumpy.allclose\n to check that each value of the Python and Numba fields match to within a specified tolerance (the default tolerance is 1\n\\tt{E}^-\n08).\n\n\nassert numpy.allclose(p, p_numba)\nassert numpy.allclose(u, u_numba)\nassert numpy.allclose(v, v_numba)\n\n\n\n\nIf there were no errors raised by the previous cell, then the two answers match.  Hooray! \nIf the answers match, we should see the same plot as above.  If they don't match, we can also check the plot to see if we're just a little bit off or completely wrong (although that can be hard to judge sometimes).\n\n\nquiver_plot(u_numba, v_numba, p_numba)\n\n\n\n\n\n\nAnd now we can check to see how much the performance has been improved:\n\n\n%timeit run_cavity()\n\n\n\n\n1 loop, best of 3: 460 ms per loop\n\n\n\nNot as spectacular as some of the other examples, but remember, we started off using NumPy arrays, not regular nested Python loops, so this is still a very respectable speedup.  \n\n\nNow if we re-profile the code, we can see how the runtime percentages have shifted around the improved PPE.  It's almost certainly going to remain the most expensive part of the solve, but it should be a smaller percentage overall.\n\n\n%lprun -f cavity_flow run_cavity()\n\n\n\n\nOne more bit of optimization?\n\n\nAs we improve the performance of the Pressure Poisson function, the profiling results will change and indicate other possible hotspots that we want to improve. It can be hard to know when to \nstop\n improving, to recognize that the gains are not worth what you are investing in rewriting code. \n\n\nIn the profile above (after rewriting the PPE) you'll notice that the \nvelocity_term\n now occupies a larger percentage of the total run time. Here's a rewritten version of \nvelocity_term\n that makes use of Numba so you can compare how much more performance we might be able to eke out of this simple cavity flow solver.\n\n\n@jit(nopython=True)\ndef velocity_term(b, rho, dt, u, v, dx):\n    J, I = b.shape\n\n    for i in range(1, I):\n        for j in range(1, J):\n            b[j, i] = (\n            rho * dx / 16 * \n            (2 / dt * (u[j, i + 1] - \n                      u[j, i - 1] + \n                      v[j + 1, i] - \n                      v[j - 1, i]) - \n            2 / dx * (u[j + 1, i] - u[j - 1, i]) * \n                     (v[j, i + 1] - v[j, i - 1]) - \n            (u[j, i + 1] - u[j, i - 1])**2 / dx - \n            (v[j + 1, i] - v[j - 1, i])**2 / dx)\n            )\n    return b\n\n\n\n\n%timeit run_cavity()\n\n\n\n\n1 loop, best of 3: 415 ms per loop",
            "title": "Cavity Flow"
        },
        {
            "location": "/numba/5/#application-cavity-flow",
            "text": "One of the most common validation cases in CFD is the lid-driven cavity flow.  We take a square cavity filled with a fluid and set the velocity of the lid to some constant value.  The flow within the cavity is driven by the lid, a spiral flow pattern develops and two distinctive pressure zones are visible in the upper corners against the lid.   import numpy  The Poisson equation is an elliptic PDE which almost always means using an iterative solver.  We're going to use the Jacobi method.  There are better ways, but that's beside the point.    Here's the pressure Poisson equation:   \\begin{align}\np_{i,j}^{n+1} &= \\frac{1}{4}\\left(p_{i+1,j}^{n}+p_{i-1,j}^{n}+p_{i,j+1}^{n}+p_{i,j-1}^{n}\\right) \\\\\n&-\\frac{\\rho \\Delta x}{16} \\left( \\frac{2}{\\Delta t} \\left(u_{i+1,j} - u_{i-1,j} + v_{i,j+1} - v_{i,j-1}\\right) \\right . \\\\\n&-\\frac{2}{\\Delta x}\\left(u_{i,j+1} - u_{i,j-1} \\right) \\left(v_{i+1,j} - v_{i-1,j} \\right) \\\\\n&- \\left . \\frac{\\left(u_{i+1,j} - u_{i-1,j} \\right)^2}{\\Delta x} \n- \\frac{ \\left(v_{i,j+1} - v_{i,j-1} \\right)^2 }{\\Delta x} \\right) \\\\\n\\end{align}   That looks a little nasty, but we only care about the top line when we iterate, since the bottom three lines depend only on values that don't change when we're correcting the pressure field.  Because it doesn't change, we break it out into a separate function.  def velocity_term(b, rho, dt, u, v, dx):\n    b[1:-1, 1:-1] = (\n        rho * dx / 16 *\n        (2 / dt * (u[1:-1, 2:] -\n                    u[1:-1, :-2] +\n                    v[2:, 1:-1] -\n                    v[:-2, 1:-1]) -\n        2 / dx * (u[2:, 1:-1] - u[:-2, 1:-1]) *\n                 (v[1:-1, 2:] - v[1:-1, :-2]) -\n        (u[1:-1, 2:] - u[1:-1, :-2])**2 / dx -\n        (v[2:, 1:-1] - v[:-2, 1:-1])**2 / dx)\n                     )\n\n    return b  Now, to calculate the pressure field, we pass in the original pressure field, the value  b  (which is the result of the  velocity_term  function above) and a target value for difference between two iterates.  We repeatedly update the pressure field until the difference of the L2 norm between two successive iterations is less than that target value.  def pressure_poisson(p, b, l2_target):\n    iter_diff = l2_target + 1\n    n = 0\n    while iter_diff > l2_target and n <= 500:\n\n        pn = p.copy()\n        p[1:-1,1:-1] = (.25 * (pn[1:-1, 2:] +\n                               pn[1:-1, :-2] +\n                               pn[2:, 1:-1] +\n                               pn[:-2, 1:-1]) -\n                               b[1:-1, 1:-1])\n\n        p[:, 0] = p[:, 1]   #dp/dx = 0 at x = 0\n        p[:, -1] = p[:, -2] #dp/dx = 0 at x = 2\n        p[0, :] = p[1, :]   #dp/dy = 0 at y = 0\n        p[-1, :] = 0        #p = 0 at y = 2\n\n        if n % 10 == 0:\n            iter_diff = numpy.sqrt(numpy.sum((p - pn)**2)/numpy.sum(pn**2))\n\n        n += 1\n\n    return p  In the interests of brevity, we're only going to worry about the pressure Poisson solver.  The rest of the 2D Navier-Stokes solution is encapsulated in the function  cavity_flow , which we've prepared ahead of time and saved in a helper file. If you want to dig deeper into how the  cavity_flow  function works, check out \"The 12 Steps to Navier-Stokes\" at  CFD Python .  For now, though, we just need to import the function:  from snippets.ns_helper import cavity_flow  We'll also load up  pickled  initial conditions, so we can reliably compare final solutions.  import pickle  def run_cavity():\n    nx = 41\n    ny = 41\n    with open('IC.pickle', 'rb') as f:\n        u, v, p, b = pickle.load(f)\n\n    dx = 2 / (nx - 1)\n    dt = .005\n    nt = 1000\n\n    u, v, p = cavity_flow(u, v, p, nt, dt, dx, \n                         velocity_term, \n                         pressure_poisson, \n                         rtol=1e-4)\n\n    return u, v, p  So what does this all do?  Let's check it out.  u, v, p = run_cavity()  %matplotlib inline\nfrom snippets.ns_helper import quiver_plot  quiver_plot(u, v, p)",
            "title": "Application: Cavity Flow"
        },
        {
            "location": "/numba/5/#save-numpy-answers-for-comparison",
            "text": "This will create a binary file, saved to disk as  numpy_ans.pickle  that contains the values of  u ,  v , and  p .   with open('numpy_ans.pickle', 'wb') as f:\n    pickle.dump((u, v, p), f)  Let's profile the  cavity_flow  function and see if there's a specific place that's really hurting our performance.  %timeit run_cavity()  1 loop, best of 3: 462 ms per loop  Remember that we need to load the  line_profiler  extension before we can use the  lprun  magic.  %load_ext line_profiler  %lprun -f cavity_flow run_cavity()",
            "title": "Save NumPy answers for comparison"
        },
        {
            "location": "/numba/5/#where-is-the-bottleneck",
            "text": "Clearly the PPE is the problem here, so let's use  numba  to rewrite it.",
            "title": "Where is the bottleneck?"
        },
        {
            "location": "/numba/5/#exercise-speed-up-the-ppe",
            "text": "from numba import jit  %load snippets/ppe_numba.py  Since we have redefined the  pressure_poisson  function, we can run  run_cavity  again and we'll be using the new and improved  jit ted PPE.   We don't want to overwrite our results, so we can choose different variable names to store the final velocity and pressure fields.  u_numba, v_numba, p_numba = run_cavity()  We use  numpy.allclose  to check that each value of the Python and Numba fields match to within a specified tolerance (the default tolerance is 1 \\tt{E}^- 08).  assert numpy.allclose(p, p_numba)\nassert numpy.allclose(u, u_numba)\nassert numpy.allclose(v, v_numba)  If there were no errors raised by the previous cell, then the two answers match.  Hooray! \nIf the answers match, we should see the same plot as above.  If they don't match, we can also check the plot to see if we're just a little bit off or completely wrong (although that can be hard to judge sometimes).  quiver_plot(u_numba, v_numba, p_numba)   And now we can check to see how much the performance has been improved:  %timeit run_cavity()  1 loop, best of 3: 460 ms per loop  Not as spectacular as some of the other examples, but remember, we started off using NumPy arrays, not regular nested Python loops, so this is still a very respectable speedup.    Now if we re-profile the code, we can see how the runtime percentages have shifted around the improved PPE.  It's almost certainly going to remain the most expensive part of the solve, but it should be a smaller percentage overall.  %lprun -f cavity_flow run_cavity()",
            "title": "Exercise: Speed up the PPE"
        },
        {
            "location": "/numba/5/#one-more-bit-of-optimization",
            "text": "As we improve the performance of the Pressure Poisson function, the profiling results will change and indicate other possible hotspots that we want to improve. It can be hard to know when to  stop  improving, to recognize that the gains are not worth what you are investing in rewriting code.   In the profile above (after rewriting the PPE) you'll notice that the  velocity_term  now occupies a larger percentage of the total run time. Here's a rewritten version of  velocity_term  that makes use of Numba so you can compare how much more performance we might be able to eke out of this simple cavity flow solver.  @jit(nopython=True)\ndef velocity_term(b, rho, dt, u, v, dx):\n    J, I = b.shape\n\n    for i in range(1, I):\n        for j in range(1, J):\n            b[j, i] = (\n            rho * dx / 16 * \n            (2 / dt * (u[j, i + 1] - \n                      u[j, i - 1] + \n                      v[j + 1, i] - \n                      v[j - 1, i]) - \n            2 / dx * (u[j + 1, i] - u[j - 1, i]) * \n                     (v[j, i + 1] - v[j, i - 1]) - \n            (u[j, i + 1] - u[j, i - 1])**2 / dx - \n            (v[j + 1, i] - v[j - 1, i])**2 / dx)\n            )\n    return b  %timeit run_cavity()  1 loop, best of 3: 415 ms per loop",
            "title": "One more bit of optimization?"
        },
        {
            "location": "/numba/6/",
            "text": "Defining \nufuncs\n using \nvectorize\n\n\nYou have been able to define your own NumPy \nufuncs\n for quite some time, but it's a little involved.  \n\n\nYou can read through the \ndocumentation\n, the example they post there is a ufunc to perform \n\n\n\n\nf(a) = \\log \\left(\\frac{a}{1-a}\\right)\n\n\n\n\nIt looks like this:\n\n\nstatic void double_logit(char **args, npy_intp *dimensions,\n                            npy_intp* steps, void* data)\n{\n    npy_intp i;\n    npy_intp n = dimensions[0];\n    char *in = args[0], *out = args[1];\n    npy_intp in_step = steps[0], out_step = steps[1];\n\n    double tmp;\n\n    for (i = 0; i < n; i++) {\n        /*BEGIN main ufunc computation*/\n        tmp = *(double *)in;\n        tmp /= 1-tmp;\n        *((double *)out) = log(tmp);\n        /*END main ufunc computation*/\n\n        in += in_step;\n        out += out_step;\n    }\n}\n\n\n\n\nAnd \nnote\n, that's just for a \ndouble\n.  If you want \nfloats\n, \nlong doubles\n, etc... you have to write all of those, too.  And then create a \nsetup.py\n file to install it.  And I left out a bunch of boilerplate stuff to set up the import hooks, etc...\n\n\nSay \"thank you\" to the NumPy devs\n\n\nWe can use Numba to define ufuncs without all of the pain.\n\n\nimport numpy\nimport math\n\n\n\n\nLet's define a function that operates on two inputs\n\n\ndef trig(a, b):\n    return math.sin(a**2) * math.exp(b)\n\n\n\n\ntrig(1, 1)\n\n\n\n\n2.2873552871788423\n\n\n\nSeems reasonable.  However, the \nmath\n library only works on scalars.  If we try to pass in arrays, we'll get an error.\n\n\na = numpy.ones((5,5))\nb = numpy.ones((5,5))\n\n\n\n\ntrig(a, b)\n\n\n\n\n---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n<ipython-input-5-6bc62cd8d328> in <module>()\n----> 1 trig(a, b)\n\n\n<ipython-input-2-27083e35e9e9> in trig(a, b)\n      1 def trig(a, b):\n----> 2     return math.sin(a**2) * math.exp(b)\n\n\nTypeError: only length-1 arrays can be converted to Python scalars\n\n\n\nfrom numba import vectorize\n\n\n\n\nvec_trig = vectorize()(trig)\n\n\n\n\nvec_trig(a, b)\n\n\n\n\narray([[ 2.28735529,  2.28735529,  2.28735529,  2.28735529,  2.28735529],\n       [ 2.28735529,  2.28735529,  2.28735529,  2.28735529,  2.28735529],\n       [ 2.28735529,  2.28735529,  2.28735529,  2.28735529,  2.28735529],\n       [ 2.28735529,  2.28735529,  2.28735529,  2.28735529,  2.28735529],\n       [ 2.28735529,  2.28735529,  2.28735529,  2.28735529,  2.28735529]])\n\n\n\nAnd just like that, the scalar function \ntrig\n is now a NumPy \nufunc\n called \nvec_trig\n\n\nNote that this is a \"Dynamic UFunc\" with no signature given.  \n\n\nHow does it compare to just using NumPy?  Let's check\n\n\ndef numpy_trig(a, b):\n    return numpy.sin(a**2) * numpy.exp(b)\n\n\n\n\na = numpy.random.random((1000, 1000))\nb = numpy.random.random((1000, 1000))\n\n\n\n\n%timeit vec_trig(a, b)\n\n\n\n\n10 loops, best of 3: 29.4 ms per loop\n\n\n\n%timeit numpy_trig(a, b)\n\n\n\n\n10 loops, best of 3: 32.5 ms per loop\n\n\n\nWhat happens if we do specify a signature?  Is there a speed boost?\n\n\nvec_trig = vectorize('float64(float64, float64)')(trig)\n\n\n\n\n%timeit vec_trig(a, b)\n\n\n\n\n10 loops, best of 3: 29.5 ms per loop\n\n\n\nNo, not really.  But(!), if we have a signature, then we can add the target \nkwarg\n.\n\n\nvec_trig = vectorize('float64(float64, float64)', target='parallel')(trig)\n\n\n\n\n%timeit vec_trig(a, b)\n\n\n\n\n100 loops, best of 3: 6.24 ms per loop\n\n\n\nAutomatic multicore operations!\n\n\nNote\n: \ntarget='parallel'\n is not always the best option.  There is overhead in setting up the threading, so if the individual scalar operations that make up a \nufunc\n are simple you'll probably get better performance in serial.  If the individual operations are more expensive (like trig!) then parallel is (usually) a good option.\n\n\nPassing multiple signatures\n\n\nIf you use multiple signatures, they have to be listed in order of most specific -> least specific\n\n\n@vectorize(['int32(int32, int32)',\n            'int64(int64, int64)',\n            'float32(float32, float32)',\n            'float64(float64, float64)'])\ndef trig(a, b):\n    return math.sin(a**2) * math.exp(b)\n\n\n\n\ntrig(1, 1)\n\n\n\n\n2\n\n\n\ntrig(1., 1.)\n\n\n\n\n2.2873552871788423\n\n\n\ntrig.ntypes\n\n\n\n\n4\n\n\n\nExercise: Clipping an array\n\n\nYes, NumPy has a \nclip\n ufunc already, but let's pretend it doesn't.  \n\n\nCreate a Numba vectorized ufunc that takes a vector \na\n, a lower limit \namin\n and an upper limit \namax\n.  It should return the vector \na\n with all values clipped such that \na_{min} < a < a_{max}\n:\n\n\n# %load snippets/clip.py\n\n\n\n\na = numpy.random.random((5000))\n\n\n\n\namin = .2\namax = .6\n\n\n\n\n%timeit vec_truncate_serial(a, amin, amax)\n\n\n\n\n---------------------------------------------------------------------------\n\nNameError                                 Traceback (most recent call last)\n\n<ipython-input-24-1221b70b3424> in <module>()\n----> 1 get_ipython().magic('timeit vec_truncate_serial(a, amin, amax)')\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py in magic(self, arg_s)\n   2156         magic_name, _, magic_arg_s = arg_s.partition(' ')\n   2157         magic_name = magic_name.lstrip(prefilter.ESC_MAGIC)\n-> 2158         return self.run_line_magic(magic_name, magic_arg_s)\n   2159 \n   2160     #-------------------------------------------------------------------------\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_line_magic(self, magic_name, line)\n   2077                 kwargs['local_ns'] = sys._getframe(stack_depth).f_locals\n   2078             with self.builtin_trap:\n-> 2079                 result = fn(*args,**kwargs)\n   2080             return result\n   2081\n\n\n<decorator-gen-58> in timeit(self, line, cell)\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k)\n    186     # but it's overkill for just that one bit of state.\n    187     def magic_deco(arg):\n--> 188         call = lambda f, *a, **k: f(*a, **k)\n    189 \n    190         if callable(arg):\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py in timeit(self, line, cell)\n   1042             number = 1\n   1043             for _ in range(1, 10):\n-> 1044                 time_number = timer.timeit(number)\n   1045                 worst_tuning = max(worst_tuning, time_number / number)\n   1046                 if time_number >= 0.2:\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py in timeit(self, number)\n    137         gc.disable()\n    138         try:\n--> 139             timing = self.inner(it, self.timer)\n    140         finally:\n    141             if gcold:\n\n\n<magic-timeit> in inner(_it, _timer)\n\n\nNameError: name 'vec_truncate_serial' is not defined\n\n\n\n%timeit vec_truncate_par(a, amin, amax)\n\n\n\n\n---------------------------------------------------------------------------\n\nNameError                                 Traceback (most recent call last)\n\n<ipython-input-25-a1fc14977f98> in <module>()\n----> 1 get_ipython().magic('timeit vec_truncate_par(a, amin, amax)')\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py in magic(self, arg_s)\n   2156         magic_name, _, magic_arg_s = arg_s.partition(' ')\n   2157         magic_name = magic_name.lstrip(prefilter.ESC_MAGIC)\n-> 2158         return self.run_line_magic(magic_name, magic_arg_s)\n   2159 \n   2160     #-------------------------------------------------------------------------\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_line_magic(self, magic_name, line)\n   2077                 kwargs['local_ns'] = sys._getframe(stack_depth).f_locals\n   2078             with self.builtin_trap:\n-> 2079                 result = fn(*args,**kwargs)\n   2080             return result\n   2081\n\n\n<decorator-gen-58> in timeit(self, line, cell)\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k)\n    186     # but it's overkill for just that one bit of state.\n    187     def magic_deco(arg):\n--> 188         call = lambda f, *a, **k: f(*a, **k)\n    189 \n    190         if callable(arg):\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py in timeit(self, line, cell)\n   1042             number = 1\n   1043             for _ in range(1, 10):\n-> 1044                 time_number = timer.timeit(number)\n   1045                 worst_tuning = max(worst_tuning, time_number / number)\n   1046                 if time_number >= 0.2:\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py in timeit(self, number)\n    137         gc.disable()\n    138         try:\n--> 139             timing = self.inner(it, self.timer)\n    140         finally:\n    141             if gcold:\n\n\n<magic-timeit> in inner(_it, _timer)\n\n\nNameError: name 'vec_truncate_par' is not defined\n\n\n\n%timeit numpy.clip(a, amin, amax)\n\n\n\n\n100000 loops, best of 3: 4.78 \u00b5s per loop\n\n\n\na = numpy.random.random((100000))\n\n\n\n\n%timeit vec_truncate_serial(a, amin, amax)\n\n\n\n\n---------------------------------------------------------------------------\n\nNameError                                 Traceback (most recent call last)\n\n<ipython-input-28-1221b70b3424> in <module>()\n----> 1 get_ipython().magic('timeit vec_truncate_serial(a, amin, amax)')\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py in magic(self, arg_s)\n   2156         magic_name, _, magic_arg_s = arg_s.partition(' ')\n   2157         magic_name = magic_name.lstrip(prefilter.ESC_MAGIC)\n-> 2158         return self.run_line_magic(magic_name, magic_arg_s)\n   2159 \n   2160     #-------------------------------------------------------------------------\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_line_magic(self, magic_name, line)\n   2077                 kwargs['local_ns'] = sys._getframe(stack_depth).f_locals\n   2078             with self.builtin_trap:\n-> 2079                 result = fn(*args,**kwargs)\n   2080             return result\n   2081\n\n\n<decorator-gen-58> in timeit(self, line, cell)\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k)\n    186     # but it's overkill for just that one bit of state.\n    187     def magic_deco(arg):\n--> 188         call = lambda f, *a, **k: f(*a, **k)\n    189 \n    190         if callable(arg):\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py in timeit(self, line, cell)\n   1042             number = 1\n   1043             for _ in range(1, 10):\n-> 1044                 time_number = timer.timeit(number)\n   1045                 worst_tuning = max(worst_tuning, time_number / number)\n   1046                 if time_number >= 0.2:\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py in timeit(self, number)\n    137         gc.disable()\n    138         try:\n--> 139             timing = self.inner(it, self.timer)\n    140         finally:\n    141             if gcold:\n\n\n<magic-timeit> in inner(_it, _timer)\n\n\nNameError: name 'vec_truncate_serial' is not defined\n\n\n\n%timeit vec_truncate_par(a, amin, amax)\n\n\n\n\n---------------------------------------------------------------------------\n\nNameError                                 Traceback (most recent call last)\n\n<ipython-input-29-a1fc14977f98> in <module>()\n----> 1 get_ipython().magic('timeit vec_truncate_par(a, amin, amax)')\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py in magic(self, arg_s)\n   2156         magic_name, _, magic_arg_s = arg_s.partition(' ')\n   2157         magic_name = magic_name.lstrip(prefilter.ESC_MAGIC)\n-> 2158         return self.run_line_magic(magic_name, magic_arg_s)\n   2159 \n   2160     #-------------------------------------------------------------------------\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_line_magic(self, magic_name, line)\n   2077                 kwargs['local_ns'] = sys._getframe(stack_depth).f_locals\n   2078             with self.builtin_trap:\n-> 2079                 result = fn(*args,**kwargs)\n   2080             return result\n   2081\n\n\n<decorator-gen-58> in timeit(self, line, cell)\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k)\n    186     # but it's overkill for just that one bit of state.\n    187     def magic_deco(arg):\n--> 188         call = lambda f, *a, **k: f(*a, **k)\n    189 \n    190         if callable(arg):\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py in timeit(self, line, cell)\n   1042             number = 1\n   1043             for _ in range(1, 10):\n-> 1044                 time_number = timer.timeit(number)\n   1045                 worst_tuning = max(worst_tuning, time_number / number)\n   1046                 if time_number >= 0.2:\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py in timeit(self, number)\n    137         gc.disable()\n    138         try:\n--> 139             timing = self.inner(it, self.timer)\n    140         finally:\n    141             if gcold:\n\n\n<magic-timeit> in inner(_it, _timer)\n\n\nNameError: name 'vec_truncate_par' is not defined\n\n\n\n%timeit numpy.clip(a, amin, amax)\n\n\n\n\n1000 loops, best of 3: 212 \u00b5s per loop\n\n\n\nExercise: Create \nlogit\n ufunc\n\n\nRecall from above that this is a ufunc which performs this operation:\n\n\n\n\nf(a) = \\log \\left(\\frac{a}{1-a}\\right)\n\n\n\n\n# %load snippets/logit.py\n\n\n\n\nlogit(a)\n\n\n\n\n---------------------------------------------------------------------------\n\nNameError                                 Traceback (most recent call last)\n\n<ipython-input-32-43208ff16126> in <module>()\n----> 1 logit(a)\n\n\nNameError: name 'logit' is not defined\n\n\n\nPerformance of \nvectorize\n vs. regular array-wide operations\n\n\n@vectorize\ndef discriminant(a, b, c):\n    return b**2 - 4 * a * c\n\n\n\n\na = numpy.arange(10000)\nb = numpy.arange(10000)\nc = numpy.arange(10000)\n\n\n\n\n%timeit discriminant(a, b, c)\n\n\n\n\nThe slowest run took 3182.90 times longer than the fastest. This could mean that an intermediate result is being cached.\n100000 loops, best of 3: 11.7 \u00b5s per loop\n\n\n\n%timeit b**2 - 4 * a * c\n\n\n\n\nThe slowest run took 5.46 times longer than the fastest. This could mean that an intermediate result is being cached.\n10000 loops, best of 3: 39.4 \u00b5s per loop\n\n\n\nWhat's going on?\n\n\n\n\nEach array operation creates a temporary copy\n\n\nEach of these arrays are loaded into and out of cache a whole bunch\n\n\n\n\ndel a, b, c",
            "title": "vectorize"
        },
        {
            "location": "/numba/6/#defining-ufuncs-using-vectorize",
            "text": "You have been able to define your own NumPy  ufuncs  for quite some time, but it's a little involved.    You can read through the  documentation , the example they post there is a ufunc to perform    f(a) = \\log \\left(\\frac{a}{1-a}\\right)   It looks like this:  static void double_logit(char **args, npy_intp *dimensions,\n                            npy_intp* steps, void* data)\n{\n    npy_intp i;\n    npy_intp n = dimensions[0];\n    char *in = args[0], *out = args[1];\n    npy_intp in_step = steps[0], out_step = steps[1];\n\n    double tmp;\n\n    for (i = 0; i < n; i++) {\n        /*BEGIN main ufunc computation*/\n        tmp = *(double *)in;\n        tmp /= 1-tmp;\n        *((double *)out) = log(tmp);\n        /*END main ufunc computation*/\n\n        in += in_step;\n        out += out_step;\n    }\n}  And  note , that's just for a  double .  If you want  floats ,  long doubles , etc... you have to write all of those, too.  And then create a  setup.py  file to install it.  And I left out a bunch of boilerplate stuff to set up the import hooks, etc...",
            "title": "Defining ufuncs using vectorize"
        },
        {
            "location": "/numba/6/#say-thank-you-to-the-numpy-devs",
            "text": "We can use Numba to define ufuncs without all of the pain.  import numpy\nimport math  Let's define a function that operates on two inputs  def trig(a, b):\n    return math.sin(a**2) * math.exp(b)  trig(1, 1)  2.2873552871788423  Seems reasonable.  However, the  math  library only works on scalars.  If we try to pass in arrays, we'll get an error.  a = numpy.ones((5,5))\nb = numpy.ones((5,5))  trig(a, b)  ---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n<ipython-input-5-6bc62cd8d328> in <module>()\n----> 1 trig(a, b)\n\n\n<ipython-input-2-27083e35e9e9> in trig(a, b)\n      1 def trig(a, b):\n----> 2     return math.sin(a**2) * math.exp(b)\n\n\nTypeError: only length-1 arrays can be converted to Python scalars  from numba import vectorize  vec_trig = vectorize()(trig)  vec_trig(a, b)  array([[ 2.28735529,  2.28735529,  2.28735529,  2.28735529,  2.28735529],\n       [ 2.28735529,  2.28735529,  2.28735529,  2.28735529,  2.28735529],\n       [ 2.28735529,  2.28735529,  2.28735529,  2.28735529,  2.28735529],\n       [ 2.28735529,  2.28735529,  2.28735529,  2.28735529,  2.28735529],\n       [ 2.28735529,  2.28735529,  2.28735529,  2.28735529,  2.28735529]])  And just like that, the scalar function  trig  is now a NumPy  ufunc  called  vec_trig  Note that this is a \"Dynamic UFunc\" with no signature given.    How does it compare to just using NumPy?  Let's check  def numpy_trig(a, b):\n    return numpy.sin(a**2) * numpy.exp(b)  a = numpy.random.random((1000, 1000))\nb = numpy.random.random((1000, 1000))  %timeit vec_trig(a, b)  10 loops, best of 3: 29.4 ms per loop  %timeit numpy_trig(a, b)  10 loops, best of 3: 32.5 ms per loop  What happens if we do specify a signature?  Is there a speed boost?  vec_trig = vectorize('float64(float64, float64)')(trig)  %timeit vec_trig(a, b)  10 loops, best of 3: 29.5 ms per loop  No, not really.  But(!), if we have a signature, then we can add the target  kwarg .  vec_trig = vectorize('float64(float64, float64)', target='parallel')(trig)  %timeit vec_trig(a, b)  100 loops, best of 3: 6.24 ms per loop  Automatic multicore operations!  Note :  target='parallel'  is not always the best option.  There is overhead in setting up the threading, so if the individual scalar operations that make up a  ufunc  are simple you'll probably get better performance in serial.  If the individual operations are more expensive (like trig!) then parallel is (usually) a good option.",
            "title": "Say \"thank you\" to the NumPy devs"
        },
        {
            "location": "/numba/6/#passing-multiple-signatures",
            "text": "If you use multiple signatures, they have to be listed in order of most specific -> least specific  @vectorize(['int32(int32, int32)',\n            'int64(int64, int64)',\n            'float32(float32, float32)',\n            'float64(float64, float64)'])\ndef trig(a, b):\n    return math.sin(a**2) * math.exp(b)  trig(1, 1)  2  trig(1., 1.)  2.2873552871788423  trig.ntypes  4",
            "title": "Passing multiple signatures"
        },
        {
            "location": "/numba/6/#exercise-clipping-an-array",
            "text": "Yes, NumPy has a  clip  ufunc already, but let's pretend it doesn't.    Create a Numba vectorized ufunc that takes a vector  a , a lower limit  amin  and an upper limit  amax .  It should return the vector  a  with all values clipped such that  a_{min} < a < a_{max} :  # %load snippets/clip.py  a = numpy.random.random((5000))  amin = .2\namax = .6  %timeit vec_truncate_serial(a, amin, amax)  ---------------------------------------------------------------------------\n\nNameError                                 Traceback (most recent call last)\n\n<ipython-input-24-1221b70b3424> in <module>()\n----> 1 get_ipython().magic('timeit vec_truncate_serial(a, amin, amax)')\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py in magic(self, arg_s)\n   2156         magic_name, _, magic_arg_s = arg_s.partition(' ')\n   2157         magic_name = magic_name.lstrip(prefilter.ESC_MAGIC)\n-> 2158         return self.run_line_magic(magic_name, magic_arg_s)\n   2159 \n   2160     #-------------------------------------------------------------------------\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_line_magic(self, magic_name, line)\n   2077                 kwargs['local_ns'] = sys._getframe(stack_depth).f_locals\n   2078             with self.builtin_trap:\n-> 2079                 result = fn(*args,**kwargs)\n   2080             return result\n   2081\n\n\n<decorator-gen-58> in timeit(self, line, cell)\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k)\n    186     # but it's overkill for just that one bit of state.\n    187     def magic_deco(arg):\n--> 188         call = lambda f, *a, **k: f(*a, **k)\n    189 \n    190         if callable(arg):\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py in timeit(self, line, cell)\n   1042             number = 1\n   1043             for _ in range(1, 10):\n-> 1044                 time_number = timer.timeit(number)\n   1045                 worst_tuning = max(worst_tuning, time_number / number)\n   1046                 if time_number >= 0.2:\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py in timeit(self, number)\n    137         gc.disable()\n    138         try:\n--> 139             timing = self.inner(it, self.timer)\n    140         finally:\n    141             if gcold:\n\n\n<magic-timeit> in inner(_it, _timer)\n\n\nNameError: name 'vec_truncate_serial' is not defined  %timeit vec_truncate_par(a, amin, amax)  ---------------------------------------------------------------------------\n\nNameError                                 Traceback (most recent call last)\n\n<ipython-input-25-a1fc14977f98> in <module>()\n----> 1 get_ipython().magic('timeit vec_truncate_par(a, amin, amax)')\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py in magic(self, arg_s)\n   2156         magic_name, _, magic_arg_s = arg_s.partition(' ')\n   2157         magic_name = magic_name.lstrip(prefilter.ESC_MAGIC)\n-> 2158         return self.run_line_magic(magic_name, magic_arg_s)\n   2159 \n   2160     #-------------------------------------------------------------------------\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_line_magic(self, magic_name, line)\n   2077                 kwargs['local_ns'] = sys._getframe(stack_depth).f_locals\n   2078             with self.builtin_trap:\n-> 2079                 result = fn(*args,**kwargs)\n   2080             return result\n   2081\n\n\n<decorator-gen-58> in timeit(self, line, cell)\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k)\n    186     # but it's overkill for just that one bit of state.\n    187     def magic_deco(arg):\n--> 188         call = lambda f, *a, **k: f(*a, **k)\n    189 \n    190         if callable(arg):\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py in timeit(self, line, cell)\n   1042             number = 1\n   1043             for _ in range(1, 10):\n-> 1044                 time_number = timer.timeit(number)\n   1045                 worst_tuning = max(worst_tuning, time_number / number)\n   1046                 if time_number >= 0.2:\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py in timeit(self, number)\n    137         gc.disable()\n    138         try:\n--> 139             timing = self.inner(it, self.timer)\n    140         finally:\n    141             if gcold:\n\n\n<magic-timeit> in inner(_it, _timer)\n\n\nNameError: name 'vec_truncate_par' is not defined  %timeit numpy.clip(a, amin, amax)  100000 loops, best of 3: 4.78 \u00b5s per loop  a = numpy.random.random((100000))  %timeit vec_truncate_serial(a, amin, amax)  ---------------------------------------------------------------------------\n\nNameError                                 Traceback (most recent call last)\n\n<ipython-input-28-1221b70b3424> in <module>()\n----> 1 get_ipython().magic('timeit vec_truncate_serial(a, amin, amax)')\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py in magic(self, arg_s)\n   2156         magic_name, _, magic_arg_s = arg_s.partition(' ')\n   2157         magic_name = magic_name.lstrip(prefilter.ESC_MAGIC)\n-> 2158         return self.run_line_magic(magic_name, magic_arg_s)\n   2159 \n   2160     #-------------------------------------------------------------------------\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_line_magic(self, magic_name, line)\n   2077                 kwargs['local_ns'] = sys._getframe(stack_depth).f_locals\n   2078             with self.builtin_trap:\n-> 2079                 result = fn(*args,**kwargs)\n   2080             return result\n   2081\n\n\n<decorator-gen-58> in timeit(self, line, cell)\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k)\n    186     # but it's overkill for just that one bit of state.\n    187     def magic_deco(arg):\n--> 188         call = lambda f, *a, **k: f(*a, **k)\n    189 \n    190         if callable(arg):\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py in timeit(self, line, cell)\n   1042             number = 1\n   1043             for _ in range(1, 10):\n-> 1044                 time_number = timer.timeit(number)\n   1045                 worst_tuning = max(worst_tuning, time_number / number)\n   1046                 if time_number >= 0.2:\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py in timeit(self, number)\n    137         gc.disable()\n    138         try:\n--> 139             timing = self.inner(it, self.timer)\n    140         finally:\n    141             if gcold:\n\n\n<magic-timeit> in inner(_it, _timer)\n\n\nNameError: name 'vec_truncate_serial' is not defined  %timeit vec_truncate_par(a, amin, amax)  ---------------------------------------------------------------------------\n\nNameError                                 Traceback (most recent call last)\n\n<ipython-input-29-a1fc14977f98> in <module>()\n----> 1 get_ipython().magic('timeit vec_truncate_par(a, amin, amax)')\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py in magic(self, arg_s)\n   2156         magic_name, _, magic_arg_s = arg_s.partition(' ')\n   2157         magic_name = magic_name.lstrip(prefilter.ESC_MAGIC)\n-> 2158         return self.run_line_magic(magic_name, magic_arg_s)\n   2159 \n   2160     #-------------------------------------------------------------------------\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_line_magic(self, magic_name, line)\n   2077                 kwargs['local_ns'] = sys._getframe(stack_depth).f_locals\n   2078             with self.builtin_trap:\n-> 2079                 result = fn(*args,**kwargs)\n   2080             return result\n   2081\n\n\n<decorator-gen-58> in timeit(self, line, cell)\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k)\n    186     # but it's overkill for just that one bit of state.\n    187     def magic_deco(arg):\n--> 188         call = lambda f, *a, **k: f(*a, **k)\n    189 \n    190         if callable(arg):\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py in timeit(self, line, cell)\n   1042             number = 1\n   1043             for _ in range(1, 10):\n-> 1044                 time_number = timer.timeit(number)\n   1045                 worst_tuning = max(worst_tuning, time_number / number)\n   1046                 if time_number >= 0.2:\n\n\n/home/gil/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py in timeit(self, number)\n    137         gc.disable()\n    138         try:\n--> 139             timing = self.inner(it, self.timer)\n    140         finally:\n    141             if gcold:\n\n\n<magic-timeit> in inner(_it, _timer)\n\n\nNameError: name 'vec_truncate_par' is not defined  %timeit numpy.clip(a, amin, amax)  1000 loops, best of 3: 212 \u00b5s per loop",
            "title": "Exercise: Clipping an array"
        },
        {
            "location": "/numba/6/#exercise-create-logit-ufunc",
            "text": "Recall from above that this is a ufunc which performs this operation:   f(a) = \\log \\left(\\frac{a}{1-a}\\right)   # %load snippets/logit.py  logit(a)  ---------------------------------------------------------------------------\n\nNameError                                 Traceback (most recent call last)\n\n<ipython-input-32-43208ff16126> in <module>()\n----> 1 logit(a)\n\n\nNameError: name 'logit' is not defined",
            "title": "Exercise: Create logit ufunc"
        },
        {
            "location": "/numba/6/#performance-of-vectorize-vs-regular-array-wide-operations",
            "text": "@vectorize\ndef discriminant(a, b, c):\n    return b**2 - 4 * a * c  a = numpy.arange(10000)\nb = numpy.arange(10000)\nc = numpy.arange(10000)  %timeit discriminant(a, b, c)  The slowest run took 3182.90 times longer than the fastest. This could mean that an intermediate result is being cached.\n100000 loops, best of 3: 11.7 \u00b5s per loop  %timeit b**2 - 4 * a * c  The slowest run took 5.46 times longer than the fastest. This could mean that an intermediate result is being cached.\n10000 loops, best of 3: 39.4 \u00b5s per loop  What's going on?   Each array operation creates a temporary copy  Each of these arrays are loaded into and out of cache a whole bunch   del a, b, c",
            "title": "Performance of vectorize vs. regular array-wide operations"
        },
        {
            "location": "/references/",
            "text": "References\n\n\n\n\n\n\nScopatz, Anthony, and Kathryn D. Huff\n. 2015. \nEffective Computation in Physics. 1 edition\n. S.l.: O\u2019Reilly Media.\n\n\n\n\n\n\nCharles Severance\n. 2016 \nPython for Everybody: Exploring Data In Python 3\n.\n\n\n\n\n\n\nSoftware Carpentry\n.",
            "title": "References"
        },
        {
            "location": "/references/#references",
            "text": "Scopatz, Anthony, and Kathryn D. Huff . 2015.  Effective Computation in Physics. 1 edition . S.l.: O\u2019Reilly Media.    Charles Severance . 2016  Python for Everybody: Exploring Data In Python 3 .    Software Carpentry .",
            "title": "References"
        },
        {
            "location": "/resources/",
            "text": "Resources and Tips\n\n\nHere are some useful links to different tools that will help you to make your \nlife easier if you are doing reproducible research computing. \n\n\n\n\n\n\nFor publishing datasets publicly and getting an associated DOI, use \nZenodo\n.\n\n\n\n\n\n\nFor sharing figures, presentations, posters, short reports and more: \nFigshare\n\n\n\n\n\n\nFor publishing research software: submit to the \nJournal of Open Source Software\n\n\n\n\n\n\nFor replication studies: submit to \nThe ReScience Journal\n\n\n\n\n\n\nSubmit preprints! \narXiv\n for physics, computer science, mathematics, quantitative biology, quantitative finance, statistics; \nbioRxiv\n for all subject areas of biology; \nsocArXiv\n for social sciences; \nengrXiv\n for engineering.\n\n\n\n\n\n\nFor collaborative writing  \nAuthorea\n  and \nShareLaTeX",
            "title": "Resources"
        },
        {
            "location": "/resources/#resources-and-tips",
            "text": "Here are some useful links to different tools that will help you to make your \nlife easier if you are doing reproducible research computing.     For publishing datasets publicly and getting an associated DOI, use  Zenodo .    For sharing figures, presentations, posters, short reports and more:  Figshare    For publishing research software: submit to the  Journal of Open Source Software    For replication studies: submit to  The ReScience Journal    Submit preprints!  arXiv  for physics, computer science, mathematics, quantitative biology, quantitative finance, statistics;  bioRxiv  for all subject areas of biology;  socArXiv  for social sciences;  engrXiv  for engineering.    For collaborative writing   Authorea   and  ShareLaTeX",
            "title": "Resources and Tips"
        }
    ]
}